public class DatasetVolumeChecker {  public static final Logger LOG =      LoggerFactory.getLogger(DatasetVolumeChecker.class);  private AsyncChecker<VolumeCheckContext, VolumeCheckResult> delegateChecker;  private final AtomicLong numVolumeChecks = new AtomicLong(0);  private final AtomicLong numSyncDatasetChecks = new AtomicLong(0);  private final AtomicLong numAsyncDatasetChecks = new AtomicLong(0);  private final AtomicLong numSkippedChecks = new AtomicLong(0);    private final long maxAllowedTimeForCheckMs;    private final int maxVolumeFailuresTolerated;    private final long minDiskCheckGapMs;  private final long diskCheckTimeout;    private long lastAllVolumesCheck;  private final Timer timer;  private static final VolumeCheckContext IGNORED_CONTEXT =      new VolumeCheckContext();  private final ExecutorService checkVolumeResultHandlerExecutorService;    public DatasetVolumeChecker(Configuration conf, Timer timer)      throws DiskErrorException {    maxAllowedTimeForCheckMs = conf.getTimeDuration(        DFS_DATANODE_DISK_CHECK_TIMEOUT_KEY,        DFS_DATANODE_DISK_CHECK_TIMEOUT_DEFAULT,        TimeUnit.MILLISECONDS);    if (maxAllowedTimeForCheckMs <= 0) {      throw new HadoopIllegalArgumentException("Invalid value configured for "          + DFS_DATANODE_DISK_CHECK_TIMEOUT_KEY + " - "          + maxAllowedTimeForCheckMs + " (should be > 0)");    }    this.timer = timer;    maxVolumeFailuresTolerated = conf.getInt(        DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY,        DFS_DATANODE_FAILED_VOLUMES_TOLERATED_DEFAULT);    minDiskCheckGapMs = conf.getTimeDuration(        DFSConfigKeys.DFS_DATANODE_DISK_CHECK_MIN_GAP_KEY,        DFSConfigKeys.DFS_DATANODE_DISK_CHECK_MIN_GAP_DEFAULT,        TimeUnit.MILLISECONDS);    if (minDiskCheckGapMs < 0) {      throw new HadoopIllegalArgumentException("Invalid value configured for "          + DFS_DATANODE_DISK_CHECK_MIN_GAP_KEY + " - "          + minDiskCheckGapMs + " (should be >= 0)");    }    diskCheckTimeout = conf.getTimeDuration(        DFSConfigKeys.DFS_DATANODE_DISK_CHECK_TIMEOUT_KEY,        DFSConfigKeys.DFS_DATANODE_DISK_CHECK_TIMEOUT_DEFAULT,        TimeUnit.MILLISECONDS);    if (diskCheckTimeout < 0) {      throw new HadoopIllegalArgumentException("Invalid value configured for "          + DFS_DATANODE_DISK_CHECK_TIMEOUT_KEY + " - "          + diskCheckTimeout + " (should be >= 0)");    }    lastAllVolumesCheck = timer.monotonicNow() - minDiskCheckGapMs;    if (maxVolumeFailuresTolerated < DataNode.MAX_VOLUME_FAILURE_TOLERATED_LIMIT) {      throw new HadoopIllegalArgumentException("Invalid value configured for "          + DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY + " - "          + maxVolumeFailuresTolerated + " "          + DataNode.MAX_VOLUME_FAILURES_TOLERATED_MSG);    }    delegateChecker = new ThrottledAsyncChecker<>(        timer, minDiskCheckGapMs, diskCheckTimeout,        Executors.newCachedThreadPool(            new ThreadFactoryBuilder()                .setNameFormat("DataNode DiskChecker thread %d")                .setDaemon(true)                .build()));    checkVolumeResultHandlerExecutorService = Executors.newCachedThreadPool(        new ThreadFactoryBuilder()            .setNameFormat("VolumeCheck ResultHandler thread %d")            .setDaemon(true)            .build());  }    public Set<FsVolumeSpi> checkAllVolumes(      final FsDatasetSpi<? extends FsVolumeSpi> dataset)      throws InterruptedException {    final long gap = timer.monotonicNow() - lastAllVolumesCheck;    if (gap < minDiskCheckGapMs) {      numSkippedChecks.incrementAndGet();      LOG.trace(          "Skipped checking all volumes, time since last check {} is less " +          "than the minimum gap between checks ({} ms).",          gap, minDiskCheckGapMs);      return Collections.emptySet();    }    final FsDatasetSpi.FsVolumeReferences references =        dataset.getFsVolumeReferences();    if (references.size() == 0) {      LOG.warn("checkAllVolumesAsync - no volumes can be referenced");      return Collections.emptySet();    }    lastAllVolumesCheck = timer.monotonicNow();    final Set<FsVolumeSpi> healthyVolumes = new HashSet<>();    final Set<FsVolumeSpi> failedVolumes = new HashSet<>();    final Set<FsVolumeSpi> allVolumes = new HashSet<>();    final AtomicLong numVolumes = new AtomicLong(references.size());    final CountDownLatch latch = new CountDownLatch(1);    for (int i = 0; i < references.size(); ++i) {      final FsVolumeReference reference = references.getReference(i);      Optional<ListenableFuture<VolumeCheckResult>> olf =          delegateChecker.schedule(reference.getVolume(), IGNORED_CONTEXT);      LOG.info("Scheduled health check for volume {}", reference.getVolume());      if (olf.isPresent()) {        allVolumes.add(reference.getVolume());        Futures.addCallback(olf.get(),            new ResultHandler(reference, healthyVolumes, failedVolumes,                numVolumes, new Callback() {                  @Override                  public void call(Set<FsVolumeSpi> ignored1,                                   Set<FsVolumeSpi> ignored2) {                    latch.countDown();                  }                }), MoreExecutors.directExecutor());      } else {        IOUtils.cleanup(null, reference);        if (numVolumes.decrementAndGet() == 0) {          latch.countDown();        }      }    }            if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {      LOG.warn("checkAllVolumes timed out after {} ms",          maxAllowedTimeForCheckMs);    }    numSyncDatasetChecks.incrementAndGet();    synchronized (this) {                                    return new HashSet<>(Sets.difference(allVolumes, healthyVolumes));    }  }    public interface Callback {        void call(Set<FsVolumeSpi> healthyVolumes,              Set<FsVolumeSpi> failedVolumes);  }    public boolean checkVolume(      final FsVolumeSpi volume,      Callback callback) {    if (volume == null) {      LOG.debug("Cannot schedule check on null volume");      return false;    }    FsVolumeReference volumeReference;    try {      volumeReference = volume.obtainReference();    } catch (ClosedChannelException e) {            return false;    }    Optional<ListenableFuture<VolumeCheckResult>> olf =        delegateChecker.schedule(volume, IGNORED_CONTEXT);    if (olf.isPresent()) {      numVolumeChecks.incrementAndGet();      Futures.addCallback(olf.get(),          new ResultHandler(volumeReference, new HashSet<>(), new HashSet<>(),              new AtomicLong(1), callback),          checkVolumeResultHandlerExecutorService      );      return true;    } else {      IOUtils.cleanup(null, volumeReference);    }    return false;  }    private class ResultHandler      implements FutureCallback<VolumeCheckResult> {    private final FsVolumeReference reference;    private final Set<FsVolumeSpi> failedVolumes;    private final Set<FsVolumeSpi> healthyVolumes;    private final AtomicLong volumeCounter;    @Nullable    private final Callback callback;        ResultHandler(FsVolumeReference reference,                  Set<FsVolumeSpi> healthyVolumes,                  Set<FsVolumeSpi> failedVolumes,                  AtomicLong volumeCounter,                  @Nullable Callback callback) {      Preconditions.checkState(reference != null);      this.reference = reference;      this.healthyVolumes = healthyVolumes;      this.failedVolumes = failedVolumes;      this.volumeCounter = volumeCounter;      this.callback = callback;    }    @Override    public void onSuccess(VolumeCheckResult result) {      if (result == null) {        LOG.error("Unexpected health check result null for volume {}",            reference.getVolume());        markHealthy();      } else {        switch(result) {        case HEALTHY:        case DEGRADED:          LOG.debug("Volume {} is {}.", reference.getVolume(), result);          markHealthy();          break;        case FAILED:          LOG.warn("Volume {} detected as being unhealthy",              reference.getVolume());          markFailed();          break;        default:          LOG.error("Unexpected health check result {} for volume {}",              result, reference.getVolume());          markHealthy();          break;        }      }      cleanup();    }    @Override    public void onFailure(@Nonnull Throwable t) {      Throwable exception = (t instanceof ExecutionException) ?          t.getCause() : t;      LOG.warn("Exception running disk checks against volume " +          reference.getVolume(), exception);      markFailed();      cleanup();    }    private void markHealthy() {      synchronized (DatasetVolumeChecker.this) {        healthyVolumes.add(reference.getVolume());      }    }    private void markFailed() {      synchronized (DatasetVolumeChecker.this) {        failedVolumes.add(reference.getVolume());      }    }    private void cleanup() {      IOUtils.cleanup(null, reference);      invokeCallback();    }    private void invokeCallback() {      try {        final long remaining = volumeCounter.decrementAndGet();        if (callback != null && remaining == 0) {          callback.call(healthyVolumes, failedVolumes);        }      } catch(Exception e) {                LOG.warn("Unexpected exception", e);      }    }  }    public void shutdownAndWait(int gracePeriod, TimeUnit timeUnit) {    try {      delegateChecker.shutdownAndWait(gracePeriod, timeUnit);    } catch (InterruptedException e) {      LOG.warn("DatasetVolumeChecker interrupted during shutdown.");      Thread.currentThread().interrupt();    }  }    @VisibleForTesting  void setDelegateChecker(      AsyncChecker<VolumeCheckContext, VolumeCheckResult> testDelegate) {    delegateChecker = testDelegate;  }    public long getNumVolumeChecks() {    return numVolumeChecks.get();  }    public long getNumSyncDatasetChecks() {    return numSyncDatasetChecks.get();  }    public long getNumSkippedChecks() {    return numSkippedChecks.get();  }}