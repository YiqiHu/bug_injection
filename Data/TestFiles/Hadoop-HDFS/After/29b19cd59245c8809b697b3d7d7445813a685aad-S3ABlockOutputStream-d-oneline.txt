class S3ABlockOutputStream extends OutputStream implements    StreamCapabilities {  private static final Logger LOG =      LoggerFactory.getLogger(S3ABlockOutputStream.class);    private final S3AFileSystem fs;    private final String key;    private final int blockSize;    private long bytesSubmitted;    private final ProgressListener progressListener;  private final ListeningExecutorService executorService;    private final S3ADataBlocks.BlockFactory blockFactory;    private final byte[] singleCharWrite = new byte[1];    private MultiPartUpload multiPartUpload;    private final AtomicBoolean closed = new AtomicBoolean(false);    private S3ADataBlocks.DataBlock activeBlock;    private long blockCount = 0;    private final S3AInstrumentation.OutputStreamStatistics statistics;    private final WriteOperationHelper writeOperationHelper;    private final PutTracker putTracker;    S3ABlockOutputStream(S3AFileSystem fs,      String key,      ExecutorService executorService,      Progressable progress,      long blockSize,      S3ADataBlocks.BlockFactory blockFactory,      S3AInstrumentation.OutputStreamStatistics statistics,      WriteOperationHelper writeOperationHelper,      PutTracker putTracker)      throws IOException {    this.fs = fs;    this.key = key;    this.blockFactory = blockFactory;    this.blockSize = (int) blockSize;    this.statistics = statistics;    this.writeOperationHelper = writeOperationHelper;    this.putTracker = putTracker;    Preconditions.checkArgument(blockSize >= Constants.MULTIPART_MIN_SIZE,        "Block size is too small: %d", blockSize);    this.executorService = MoreExecutors.listeningDecorator(executorService);    this.multiPartUpload = null;    this.progressListener = (progress instanceof ProgressListener) ?        (ProgressListener) progress        : new ProgressableListener(progress);            createBlockIfNeeded();    LOG.debug("Initialized S3ABlockOutputStream for {}" +        " output to {}", key, activeBlock);    if (putTracker.initialize()) {      LOG.debug("Put tracker requests multipart upload");      initMultipartUpload();    }  }    private synchronized S3ADataBlocks.DataBlock createBlockIfNeeded()      throws IOException {    if (activeBlock == null) {      blockCount++;      if (blockCount>= Constants.MAX_MULTIPART_COUNT) {        LOG.error("Number of partitions in stream exceeds limit for S3: "             + Constants.MAX_MULTIPART_COUNT +  " write may fail.");      }      activeBlock = blockFactory.create(blockCount, this.blockSize, statistics);    }    return activeBlock;  }    private synchronized S3ADataBlocks.DataBlock getActiveBlock() {    return activeBlock;  }    private synchronized boolean hasActiveBlock() {    return activeBlock != null;  }    private void clearActiveBlock() {    if (activeBlock != null) {      LOG.debug("Clearing active block");    }    synchronized (this) {      activeBlock = null;    }  }    void checkOpen() throws IOException {    if (closed.get()) {      throw new IOException("Filesystem " + writeOperationHelper + " closed");    }  }    @Override  public synchronized void flush() throws IOException {    try {      checkOpen();    } catch (IOException e) {      LOG.warn("Stream closed: " + e.getMessage());      return;    }    S3ADataBlocks.DataBlock dataBlock = getActiveBlock();    if (dataBlock != null) {      dataBlock.flush();    }  }    @Override  public synchronized void write(int b) throws IOException {    singleCharWrite[0] = (byte)b;    write(singleCharWrite, 0, 1);  }    @Override  public synchronized void write(byte[] source, int offset, int len)      throws IOException {    S3ADataBlocks.validateWriteArgs(source, offset, len);    checkOpen();    if (len == 0) {      return;    }    S3ADataBlocks.DataBlock block = createBlockIfNeeded();    int written = block.write(source, offset, len);    int remainingCapacity = block.remainingCapacity();    if (written < len) {                        LOG.debug("writing more data than block has capacity -triggering upload");      uploadCurrentBlock();                  this.write(source, offset + written, len - written);    } else {      if (remainingCapacity == 0) {                uploadCurrentBlock();      }    }  }    private synchronized void uploadCurrentBlock() throws IOException {    Preconditions.checkState(hasActiveBlock(), "No active block");    LOG.debug("Writing block # {}", blockCount);    initMultipartUpload();    try {      multiPartUpload.uploadBlockAsync(getActiveBlock());      bytesSubmitted += getActiveBlock().dataSize();    } finally {            clearActiveBlock();    }  }    private void initMultipartUpload() throws IOException {    if (multiPartUpload == null) {      LOG.debug("Initiating Multipart upload");      multiPartUpload = new MultiPartUpload(key);    }  }    @Override  public void close() throws IOException {    if (closed.getAndSet(true)) {            LOG.debug("Ignoring close() as stream is already closed");      return;    }    S3ADataBlocks.DataBlock block = getActiveBlock();    boolean hasBlock = hasActiveBlock();    LOG.debug("{}: Closing block #{}: current block= {}",        this,        blockCount,        hasBlock ? block : "(none)");    long bytes = 0;    try {      if (multiPartUpload == null) {        if (hasBlock) {                                        bytes = putObject();          bytesSubmitted = bytes;        }      } else {                                if (hasBlock &&            (block.hasData() || multiPartUpload.getPartsSubmitted() == 0)) {                    uploadCurrentBlock();        }                final List<PartETag> partETags =            multiPartUpload.waitForAllPartUploads();        bytes = bytesSubmitted;                if (putTracker.aboutToComplete(multiPartUpload.getUploadId(),            partETags,            bytes)) {          multiPartUpload.complete(partETags);        } else {          LOG.info("File {} will be visible when the job is committed", key);        }      }      if (!putTracker.outputImmediatelyVisible()) {                statistics.commitUploaded(bytes);      }      LOG.debug("Upload complete to {} by {}", key, writeOperationHelper);    } catch (IOException ioe) {                              if (multiPartUpload != null) {        multiPartUpload.abort();      }      writeOperationHelper.writeFailed(ioe);      throw ioe;    } finally {      cleanupWithLogger(LOG, block, blockFactory);      LOG.debug("Statistics: {}", statistics);      cleanupWithLogger(LOG, statistics);      clearActiveBlock();    }        writeOperationHelper.writeSuccessful(bytes);  }    private int putObject() throws IOException {    LOG.debug("Executing regular upload for {}", writeOperationHelper);    final S3ADataBlocks.DataBlock block = getActiveBlock();    int size = block.dataSize();    final S3ADataBlocks.BlockUploadData uploadData = block.startUpload();    final PutObjectRequest putObjectRequest = uploadData.hasFile() ?        writeOperationHelper.createPutObjectRequest(key, uploadData.getFile())        : writeOperationHelper.createPutObjectRequest(key,            uploadData.getUploadStream(), size);    long transferQueueTime = now();    BlockUploadProgress callback =        new BlockUploadProgress(            block, progressListener, transferQueueTime);    putObjectRequest.setGeneralProgressListener(callback);    statistics.blockUploadQueued(size);    ListenableFuture<PutObjectResult> putObjectResult =        executorService.submit(() -> {          try {                                    return writeOperationHelper.putObject(putObjectRequest);          } finally {            cleanupWithLogger(LOG, uploadData, block);          }        });    clearActiveBlock();        try {      putObjectResult.get();      return size;    } catch (InterruptedException ie) {      LOG.warn("Interrupted object upload", ie);      Thread.currentThread().interrupt();      return 0;    } catch (ExecutionException ee) {      throw extractException("regular upload", key, ee);    }  }  @Override  public String toString() {    final StringBuilder sb = new StringBuilder(        "S3ABlockOutputStream{");    sb.append(writeOperationHelper.toString());    sb.append(", blockSize=").append(blockSize);        S3ADataBlocks.DataBlock block = activeBlock;    if (block != null) {      sb.append(", activeBlock=").append(block);    }    sb.append('}');    return sb.toString();  }  private void incrementWriteOperations() {    fs.incrementWriteOperations();  }    private long now() {    return System.currentTimeMillis();  }    S3AInstrumentation.OutputStreamStatistics getStatistics() {    return statistics;  }    @SuppressWarnings("deprecation")  @Override  public boolean hasCapability(String capability) {    switch (capability.toLowerCase(Locale.ENGLISH)) {          case CommitConstants.STREAM_CAPABILITY_MAGIC_OUTPUT:    case CommitConstants.STREAM_CAPABILITY_MAGIC_OUTPUT_OLD:      return !putTracker.outputImmediatelyVisible();          case StreamCapabilities.HFLUSH:    case StreamCapabilities.HSYNC:      return false;    default:      return false;    }  }    private class MultiPartUpload {    private final String uploadId;    private final List<ListenableFuture<PartETag>> partETagsFutures;    private int partsSubmitted;    private int partsUploaded;    private long bytesSubmitted;        private IOException blockUploadFailure;    MultiPartUpload(String key) throws IOException {      this.uploadId = writeOperationHelper.initiateMultiPartUpload(key);      this.partETagsFutures = new ArrayList<>(2);      LOG.debug("Initiated multi-part upload for {} with " +          "id '{}'", writeOperationHelper, uploadId);    }        public int getPartsSubmitted() {      return partsSubmitted;    }        public int getPartsUploaded() {      return partsUploaded;    }        public String getUploadId() {      return uploadId;    }        public long getBytesSubmitted() {      return bytesSubmitted;    }        public void noteUploadFailure(final IOException e) {      if (blockUploadFailure == null) {        blockUploadFailure = e;      }    }        public void maybeRethrowUploadFailure() throws IOException {      if (blockUploadFailure != null) {        throw blockUploadFailure;      }    }        private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)        throws IOException {      LOG.debug("Queueing upload of {} for upload {}", block, uploadId);      Preconditions.checkNotNull(uploadId, "Null uploadId");      maybeRethrowUploadFailure();      partsSubmitted++;      final int size = block.dataSize();      bytesSubmitted += size;      final int currentPartNumber = partETagsFutures.size() + 1;      final UploadPartRequest request;      final S3ADataBlocks.BlockUploadData uploadData;      try {        uploadData = block.startUpload();        request = writeOperationHelper.newUploadPartRequest(            key,            uploadId,            currentPartNumber,            size,            uploadData.getUploadStream(),            uploadData.getFile(),            0L);      } catch (IOException e) {                noteUploadFailure(e);        throw e;      }      long transferQueueTime = now();      BlockUploadProgress callback =          new BlockUploadProgress(              block, progressListener, transferQueueTime);      request.setGeneralProgressListener(callback);      statistics.blockUploadQueued(block.dataSize());      ListenableFuture<PartETag> partETagFuture =          executorService.submit(() -> {                                    try {              LOG.debug("Uploading part {} for id '{}'",                  currentPartNumber, uploadId);              PartETag partETag = writeOperationHelper.uploadPart(request)                  .getPartETag();              LOG.debug("Completed upload of {} to part {}",                  block, partETag.getETag());              LOG.debug("Stream statistics of {}", statistics);              partsUploaded++;              return partETag;            } catch (IOException e) {                            noteUploadFailure(e);              throw e;            } finally {                            cleanupWithLogger(LOG, uploadData, block);            }          });      partETagsFutures.add(partETagFuture);    }        private List<PartETag> waitForAllPartUploads() throws IOException {      LOG.debug("Waiting for {} uploads to complete", partETagsFutures.size());      try {        return Futures.allAsList(partETagsFutures).get();      } catch (InterruptedException ie) {        LOG.warn("Interrupted partUpload", ie);        Thread.currentThread().interrupt();        return null;      } catch (ExecutionException ee) {                        LOG.debug("While waiting for upload completion", ee);                this.abort();        throw extractException("Multi-part upload with id '" + uploadId                + "' to " + key, key, ee);      }    }        private void cancelAllActiveFutures() {      LOG.debug("Cancelling futures");      for (ListenableFuture<PartETag> future : partETagsFutures) {        future.cancel(true);      }    }        private void complete(List<PartETag> partETags)        throws IOException {      maybeRethrowUploadFailure();      AtomicInteger errorCount = new AtomicInteger(0);      try {        writeOperationHelper.completeMPUwithRetries(key,            uploadId,            partETags,            bytesSubmitted,            errorCount);      } finally {        statistics.exceptionInMultipartComplete(errorCount.get());      }    }        public void abort() {      LOG.debug("Aborting upload");      fs.incrementStatistic(OBJECT_MULTIPART_UPLOAD_ABORTED);      cancelAllActiveFutures();      try {        writeOperationHelper.abortMultipartUpload(key, uploadId,            (text, e, r, i) -> statistics.exceptionInMultipartAbort());      } catch (IOException e) {                        LOG.warn("Unable to abort multipart upload,"            + " you may need to purge uploaded parts", e);      }    }  }    private final class BlockUploadProgress implements ProgressListener {    private final S3ADataBlocks.DataBlock block;    private final ProgressListener nextListener;    private final long transferQueueTime;    private long transferStartTime;        private BlockUploadProgress(S3ADataBlocks.DataBlock block,        ProgressListener nextListener,        long transferQueueTime) {      this.block = block;      this.transferQueueTime = transferQueueTime;      this.nextListener = nextListener;    }    @Override    public void progressChanged(ProgressEvent progressEvent) {      ProgressEventType eventType = progressEvent.getEventType();      long bytesTransferred = progressEvent.getBytesTransferred();      int size = block.dataSize();      switch (eventType) {      case REQUEST_BYTE_TRANSFER_EVENT:                statistics.bytesTransferred(bytesTransferred);        break;      case TRANSFER_PART_STARTED_EVENT:        transferStartTime = now();        statistics.blockUploadStarted(transferStartTime - transferQueueTime,            size);        incrementWriteOperations();        break;      case TRANSFER_PART_COMPLETED_EVENT:        statistics.blockUploadCompleted(now() - transferStartTime, size);        break;      case TRANSFER_PART_FAILED_EVENT:        statistics.blockUploadFailed(now() - transferStartTime, size);        LOG.warn("Transfer failure of block {}", block);        break;      default:              }      if (nextListener != null) {        nextListener.progressChanged(progressEvent);      }    }  }    private static class ProgressableListener implements ProgressListener {    private final Progressable progress;    ProgressableListener(Progressable progress) {      this.progress = progress;    }    public void progressChanged(ProgressEvent progressEvent) {      if (progress != null) {        progress.progress();      }    }  }}