diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java
index f39b1b721ffc..5ca7f3c84cab 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java
@@ -34,7 +34,6 @@
 import java.util.Arrays;
 
 import org.apache.avro.reflect.Stringable;
-
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.classification.InterfaceStability;
 
@@ -73,6 +72,10 @@ protected CharsetDecoder initialValue() {
     }
   };
 
+  // max size of the byte array, seems to be a safe choice for multiple JVMs
+  // (see ArrayList.MAX_ARRAY_SIZE)
+  private static final int ARRAY_MAX_SIZE = Integer.MAX_VALUE - 8;
+
   private static final byte[] EMPTY_BYTES = new byte[0];
 
   private byte[] bytes = EMPTY_BYTES;
@@ -302,8 +305,15 @@ public void clear() {
   private boolean ensureCapacity(final int capacity) {
     if (bytes.length < capacity) {
       // Try to expand the backing array by the factor of 1.5x
-      // (by taking the current size + diving it by half)
-      int targetSize = Math.max(capacity, bytes.length + (bytes.length >> 1));
+      // (by taking the current size + diving it by half).
+      //
+      // If the calculated value is beyond the size
+      // limit, we cap it to ARRAY_MAX_SIZE
+
+      long targetSizeLong = bytes.length + (bytes.length >> 1);
+      int targetSize = (int)Math.min(targetSizeLong, ARRAY_MAX_SIZE);
+      targetSize = Math.max(capacity, targetSize);
+
       bytes = new byte[targetSize];
       return true;
     }
