diff --git a/hadoop-assemblies/src/main/resources/assemblies/hadoop-src-submarine.xml b/hadoop-assemblies/src/main/resources/assemblies/hadoop-src-submarine.xml
deleted file mode 100644
index b1e039fd501b..000000000000
--- a/hadoop-assemblies/src/main/resources/assemblies/hadoop-src-submarine.xml
+++ /dev/null
@@ -1,56 +0,0 @@
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-<assembly xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.3"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.3 http://maven.apache.org/xsd/assembly-1.1.3.xsd">
-  <id>hadoop-src</id>
-  <formats>
-    <format>tar.gz</format>
-  </formats>
-  <includeBaseDirectory>true</includeBaseDirectory>
-  <fileSets>
-    <fileSet>
-      <directory>.</directory>
-      <includes>
-        <include>LICENCE.txt</include>
-        <include>README.txt</include>
-        <include>NOTICE.txt</include>
-      </includes>
-    </fileSet>
-    <fileSet>
-      <directory>.</directory>
-      <useDefaultExcludes>true</useDefaultExcludes>
-      <excludes>
-        <exclude>.git/**</exclude>
-        <exclude>**/.gitignore</exclude>
-        <exclude>**/.svn</exclude>
-        <exclude>**/*.iws</exclude>
-        <exclude>**/*.ipr</exclude>
-        <exclude>**/*.iml</exclude>
-        <exclude>**/.classpath</exclude>
-        <exclude>**/.project</exclude>
-        <exclude>**/.settings</exclude>
-        <exclude>**/target/**</exclude>
-        <!-- until the code that does this is fixed -->
-        <exclude>**/*.log</exclude>
-        <exclude>**/build/**</exclude>
-        <exclude>**/file:/**</exclude>
-        <exclude>**/SecurityAuth.audit*</exclude>
-      </excludes>
-    </fileSet>
-  </fileSets>
-</assembly>
diff --git a/hadoop-assemblies/src/main/resources/assemblies/hadoop-src.xml b/hadoop-assemblies/src/main/resources/assemblies/hadoop-src.xml
index 7da999c001af..7c725d73e041 100644
--- a/hadoop-assemblies/src/main/resources/assemblies/hadoop-src.xml
+++ b/hadoop-assemblies/src/main/resources/assemblies/hadoop-src.xml
@@ -56,7 +56,6 @@
         <exclude>**/build/**</exclude>
         <exclude>**/file:/**</exclude>
         <exclude>**/SecurityAuth.audit*</exclude>
-        <exclude>hadoop-submarine/**</exclude>
       </excludes>
     </fileSet>
   </fileSets>
diff --git a/hadoop-common-project/hadoop-common/src/site/markdown/DownstreamDev.md b/hadoop-common-project/hadoop-common/src/site/markdown/DownstreamDev.md
index b47f83bbadf5..b04bc2488f8a 100644
--- a/hadoop-common-project/hadoop-common/src/site/markdown/DownstreamDev.md
+++ b/hadoop-common-project/hadoop-common/src/site/markdown/DownstreamDev.md
@@ -430,4 +430,3 @@ please contact the developer mailing list for the relevant component(s):
 * [hdfs-dev](mailto:hdfs-dev@hadoop.apache.org)
 * [mapreduce-dev](mailto:mapreduce-dev@hadoop.apache.org)
 * [yarn-dev](mailto:yarn-dev@hadoop.apache.org)
-* [submarine-dev](mailto:submarine-dev@hadoop.apache.org)
diff --git a/hadoop-project/src/site/site.xml b/hadoop-project/src/site/site.xml
index 5a7c94c8db77..b87ffea2b81a 100644
--- a/hadoop-project/src/site/site.xml
+++ b/hadoop-project/src/site/site.xml
@@ -175,12 +175,6 @@
       <item name="System Services" href="hadoop-yarn/hadoop-yarn-site/yarn-service/SystemServices.html"/>
     </menu>
 
-    <menu name="Submarine" inherit="top">
-      <item name="Index" href="hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-submarine/Index.html"/>
-      <item name="QuickStart" href="hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-submarine/QuickStart.html"/>
-      <item name="Examples" href="hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-submarine/Examples.html"/>
-    </menu>
-
     <menu name="Hadoop Compatible File Systems" inherit="top">
       <item name="Aliyun OSS" href="hadoop-aliyun/tools/hadoop-aliyun/index.html"/>
       <item name="Amazon S3" href="hadoop-aws/tools/hadoop-aws/index.html"/>
diff --git a/hadoop-submarine/dev-support/checks/rat.sh b/hadoop-submarine/dev-support/checks/rat.sh
deleted file mode 100755
index 0e4a034338dc..000000000000
--- a/hadoop-submarine/dev-support/checks/rat.sh
+++ /dev/null
@@ -1,24 +0,0 @@
-#!/usr/bin/env bash
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-mkdir -p target
-rm target/rat-aggregated.txt
-mvn apache-rat:check
-grep -r --include=rat.txt -E "\!\?\?\?\?" ./* | tee ./target/rat-aggregated.txt
-if [ "$(cat target/rat-aggregated.txt)" ]; then
-   echo "Failed to pass apache rat check!"
-   exit -1
-fi
diff --git a/hadoop-submarine/hadoop-submarine-all/pom.xml b/hadoop-submarine/hadoop-submarine-all/pom.xml
deleted file mode 100644
index c26e0d57df50..000000000000
--- a/hadoop-submarine/hadoop-submarine-all/pom.xml
+++ /dev/null
@@ -1,183 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
-                      https://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <artifactId>hadoop-submarine</artifactId>
-    <groupId>org.apache.hadoop</groupId>
-    <version>0.3.0-SNAPSHOT</version>
-  </parent>
-  <artifactId>${project.artifactId}</artifactId>
-  <version>${project.version}</version>
-  <name>Hadoop Submarine All</name>
-
-  <properties>
-    <!-- Needed for generating FindBugs warnings using parent pom -->
-    <yarn.basedir>${project.parent.parent.basedir}</yarn.basedir>
-    <project.artifactId>hadoop-submarine-all</project.artifactId>
-    <project.version>0.3.0-SNAPSHOT</project.version>
-  </properties>
-
-  <dependencies>
-
-    <!-- Dependencies for Hadoop commons -->
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs</artifactId>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-common</artifactId>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-submarine-core</artifactId>
-      <version>${project.version}</version>
-    </dependency>
-  </dependencies>
-
-  <profiles>
-
-    <profile>
-      <id>hadoop-3.2</id>
-      <dependencies>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-yarnservice-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-tony-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-hdfs-client</artifactId>
-          <version>${hadoop.version}</version>
-        </dependency>
-      </dependencies>
-    </profile>
-
-    <!-- Default profile-->
-    <profile>
-      <id>hadoop-3.1</id>
-      <activation>
-        <activeByDefault>true</activeByDefault>
-      </activation>
-      <dependencies>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-yarnservice-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-tony-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-hdfs-client</artifactId>
-          <version>${hadoop.version}</version>
-        </dependency>
-      </dependencies>
-    </profile>
-
-    <profile>
-      <id>hadoop-2.9</id>
-      <dependencies>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-tony-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-hdfs-client</artifactId>
-          <version>${hadoop.version}</version>
-        </dependency>
-      </dependencies>
-    </profile>
-
-    <profile>
-      <id>hadoop-2.7</id>
-      <dependencies>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-tony-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-      </dependencies>
-    </profile>
-  </profiles>
-
-  <build>
-    <plugins>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-shade-plugin</artifactId>
-        <version>3.2.1</version>
-        <executions>
-          <execution>
-            <phase>package</phase>
-            <goals>
-              <goal>shade</goal>
-            </goals>
-            <configuration>
-              <!--
-              <shadedArtifactAttached>true</shadedArtifactAttached>
-              <shadedClassifierName>with-all-dependencies</shadedClassifierName>
-              -->
-              <outputFile>target/${project.artifactId}-${project.version}-${project.activeProfiles[0].id}.jar</outputFile>
-              <artifactSet>
-                <excludes>
-                  <exclude>classworlds:classworlds</exclude>
-                  <exclude>junit:junit</exclude>
-                  <exclude>jmock:*</exclude>
-                  <exclude>*:xml-apis</exclude>
-                  <exclude>org.apache.maven:lib:tests</exclude>
-                </excludes>
-              </artifactSet>
-              <filters>
-                <filter>
-                  <artifact>*:*</artifact>
-                  <excludes>
-                    <exclude>META-INF/*.SF</exclude>
-                    <exclude>META-INF/*.DSA</exclude>
-                    <exclude>META-INF/*.RSA</exclude>
-                  </excludes>
-                </filter>
-              </filters>
-              <transformers>
-                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
-                  <mainClass>org.apache.hadoop.yarn.submarine.client.cli.Cli</mainClass>
-                </transformer>
-                <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
-              </transformers>
-            </configuration>
-          </execution>
-        </executions>
-      </plugin>
-    </plugins>
-  </build>
-
-</project>
diff --git a/hadoop-submarine/hadoop-submarine-core/README.md b/hadoop-submarine/hadoop-submarine-core/README.md
deleted file mode 100644
index cc137ea5db32..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/README.md
+++ /dev/null
@@ -1,54 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-# Overview
-
-```$xslt
-              _                              _
-             | |                            (_)
-  ___  _   _ | |__   _ __ ___    __ _  _ __  _  _ __    ___
- / __|| | | || '_ \ | '_ ` _ \  / _` || '__|| || '_ \  / _ \
- \__ \| |_| || |_) || | | | | || (_| || |   | || | | ||  __/
- |___/ \__,_||_.__/ |_| |_| |_| \__,_||_|   |_||_| |_| \___|
-
-                             ?
- ~~~~~~~~~~~~~~~~~~~~~~~~~~~|^"~~~~~~~~~~~~~~~~~~~~~~~~~o~~~~~~~~~~~
-        o                   |                  o      __o
-         o                  |                 o     |X__>
-       ___o                 |                __o
-     (X___>--             __|__            |X__>     o
-                         |     \                   __o
-                         |      \                |X__>
-  _______________________|_______\________________
- <                                                \____________   _
-  \                                                            \ (_)
-   \    O       O       O                                       >=)
-    \__________________________________________________________/ (_)
-```
-
-Submarine is a project which allows infra engineer / data scientist to run
-*unmodified* Tensorflow or PyTorch programs on YARN or Kubernetes.
-
-Goals of Submarine:
-- It allows jobs easy access data/models in HDFS and other storages.
-- Can launch services to serve Tensorflow/PyTorch models.
-- Support run distributed Tensorflow jobs with simple configs.
-- Support run user-specified Docker images.
-- Support specify GPU and other resources.
-- Support launch tensorboard for training jobs if user specified.
-- Support customized DNS name for roles (like tensorboard.$user.$domain:6006)
-
-Please jump to [QuickStart](src/site/markdown/QuickStart.md) guide to quickly understand how to use this framework.
-
-Please jump to [Examples](src/site/markdown/Examples.md) to try other examples like running Distributed Tensorflow Training for CIFAR 10.
diff --git a/hadoop-submarine/hadoop-submarine-core/pom.xml b/hadoop-submarine/hadoop-submarine-core/pom.xml
deleted file mode 100644
index 0ef6f71a3c37..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/pom.xml
+++ /dev/null
@@ -1,158 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
-                      https://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <artifactId>hadoop-submarine</artifactId>
-    <groupId>org.apache.hadoop</groupId>
-    <version>0.3.0-SNAPSHOT</version>
-  </parent>
-  <artifactId>hadoop-submarine-core</artifactId>
-  <version>0.3.0-SNAPSHOT</version>
-  <name>Hadoop Submarine Core</name>
-
-  <properties>
-    <!-- Needed for generating FindBugs warnings using parent pom -->
-    <yarn.basedir>${project.parent.parent.basedir}</yarn.basedir>
-  </properties>
-
-  <dependencies>
-
-    <dependency>
-      <groupId>junit</groupId>
-      <artifactId>junit</artifactId>
-      <scope>test</scope>
-    </dependency>
-
-    <dependency>
-      <groupId>log4j</groupId>
-      <artifactId>log4j</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>com.google.guava</groupId>
-      <artifactId>guava</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>commons-logging</groupId>
-      <artifactId>commons-logging</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>commons-cli</groupId>
-      <artifactId>commons-cli</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>commons-io</groupId>
-      <artifactId>commons-io</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.slf4j</groupId>
-      <artifactId>slf4j-api</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.yaml</groupId>
-      <artifactId>snakeyaml</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.commons</groupId>
-      <artifactId>commons-lang3</artifactId>
-    </dependency>
-
-    <!-- Dependencies for Hadoop commons -->
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-common</artifactId>
-      <scope>provided</scope>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-api</artifactId>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-common</artifactId>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-client</artifactId>
-    </dependency>
-
-    <dependency>
-      <groupId>org.mockito</groupId>
-      <artifactId>mockito-core</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-common</artifactId>
-      <type>test-jar</type>
-      <scope>test</scope>
-    </dependency>
-  </dependencies>
-
-  <build>
-    <plugins>
-      <plugin>
-        <artifactId>maven-jar-plugin</artifactId>
-        <executions>
-          <execution>
-            <goals>
-              <goal>jar</goal>
-            </goals>
-            <!-- strictly speaking, the unit test is really a regression test. It
-                 needs the main jar to be available to be able to run. -->
-            <phase>test-compile</phase>
-          </execution>
-        </executions>
-        <configuration>
-           <archive>
-             <manifest>
-               <mainClass>org.apache.hadoop.yarn.submarine.client.cli.Cli</mainClass>
-             </manifest>
-           </archive>
-        </configuration>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-surefire-plugin</artifactId>
-        <configuration>
-          <environmentVariables>
-            <JAVA_HOME>${java.home}</JAVA_HOME>
-          </environmentVariables>
-       </configuration>
-      </plugin>
-
-      <plugin>
-        <artifactId>maven-jar-plugin</artifactId>
-        <executions>
-          <execution>
-            <goals>
-              <goal>test-jar</goal>
-            </goals>
-            <phase>test-compile</phase>
-          </execution>
-        </executions>
-      </plugin>
-    </plugins>
-  </build>
-
-
-</project>
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/base/ubuntu-16.04/Dockerfile.gpu.pytorch_latest b/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/base/ubuntu-16.04/Dockerfile.gpu.pytorch_latest
deleted file mode 100644
index 955f2c90c6ac..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/base/ubuntu-16.04/Dockerfile.gpu.pytorch_latest
+++ /dev/null
@@ -1,77 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04
-ARG PYTHON_VERSION=3.6
-RUN apt-get update && apt-get install -y --no-install-recommends \
-         build-essential \
-         cmake \
-         git \
-         curl \
-         vim \
-         ca-certificates \
-         libjpeg-dev \
-         libpng-dev \
-         wget &&\
-     rm -rf /var/lib/apt/lists/*
-
-
-RUN curl -o ~/miniconda.sh -O  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh  && \
-     chmod +x ~/miniconda.sh && \
-     ~/miniconda.sh -b -p /opt/conda && \
-     rm ~/miniconda.sh && \
-     /opt/conda/bin/conda install -y python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl mkl-include cython typing && \
-     /opt/conda/bin/conda install -y -c pytorch magma-cuda100 && \
-     /opt/conda/bin/conda clean -ya
-ENV PATH /opt/conda/bin:$PATH
-RUN pip install ninja
-# This must be done before pip so that requirements.txt is available
-WORKDIR /opt/pytorch
-RUN git clone https://github.com/pytorch/pytorch.git
-WORKDIR pytorch
-RUN git submodule update --init
-RUN TORCH_CUDA_ARCH_LIST="3.5 5.2 6.0 6.1 7.0+PTX" TORCH_NVCC_FLAGS="-Xfatbin -compress-all" \
-    CMAKE_PREFIX_PATH="$(dirname $(which conda))/../" \
-    pip install -v .
-
-WORKDIR /opt/pytorch
-RUN git clone https://github.com/pytorch/vision.git && cd vision && pip install -v .
-
-WORKDIR /
-# Install Hadoop
-ENV HADOOP_VERSION="3.1.2"
-RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
-RUN tar zxf hadoop-${HADOOP_VERSION}.tar.gz
-RUN ln -s hadoop-${HADOOP_VERSION} hadoop-current
-RUN rm hadoop-${HADOOP_VERSION}.tar.gz
-
-ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
-RUN echo "$LOG_TAG Install java8" && \
-    apt-get update && \
-    apt-get install -y --no-install-recommends openjdk-8-jdk && \
-    apt-get clean && rm -rf /var/lib/apt/lists/*
-
-RUN echo "Install python related packages" && \
-    pip --no-cache-dir install Pillow h5py ipykernel jupyter matplotlib numpy pandas scipy sklearn && \
-    python -m ipykernel.kernelspec
-
-# Set the locale to fix bash warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
-RUN apt-get update && apt-get install -y --no-install-recommends locales && \
-    apt-get clean && rm -rf /var/lib/apt/lists/*
-RUN locale-gen en_US.UTF-8
-
-
-WORKDIR /workspace
-RUN chmod -R a+w /workspace
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/build-all.sh b/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/build-all.sh
deleted file mode 100755
index 30e8c0fd2a6e..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/build-all.sh
+++ /dev/null
@@ -1,30 +0,0 @@
-#!/usr/bin/env bash
-
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-echo "Building base images"
-
-set -e
-
-cd base/ubuntu-16.04
-
-docker build . -f Dockerfile.gpu.pytorch_latest -t pytorch-latest-gpu-base:0.0.1
-
-echo "Finished building base images"
-
-cd ../../with-cifar10-models/ubuntu-16.04
-
-docker build . -f Dockerfile.gpu.pytorch_latest -t pytorch-latest-gpu:0.0.1
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/with-cifar10-models/cifar10_tutorial.py b/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/with-cifar10-models/cifar10_tutorial.py
deleted file mode 100644
index 02824eca5691..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/with-cifar10-models/cifar10_tutorial.py
+++ /dev/null
@@ -1,354 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#    http://www.apache.org/licenses/LICENSE-2.0
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-# -*- coding: utf-8 -*-
-"""
-Training a Classifier
-=====================
-
-This is it. You have seen how to define neural networks, compute loss and make
-updates to the weights of the network.
-
-Now you might be thinking,
-
-What about data?
-----------------
-
-Generally, when you have to deal with image, text, audio or video data,
-you can use standard python packages that load data into a numpy array.
-Then you can convert this array into a ``torch.*Tensor``.
-
--  For images, packages such as Pillow, OpenCV are useful
--  For audio, packages such as scipy and librosa
--  For text, either raw Python or Cython based loading, or NLTK and
-   SpaCy are useful
-
-Specifically for vision, we have created a package called
-``torchvision``, that has data loaders for common datasets such as
-Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,
-``torchvision.datasets`` and ``torch.utils.data.DataLoader``.
-
-This provides a huge convenience and avoids writing boilerplate code.
-
-For this tutorial, we will use the CIFAR10 dataset.
-It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,
-‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of
-size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.
-
-.. figure:: /_static/img/cifar10.png
-   :alt: cifar10
-
-   cifar10
-
-
-Training an image classifier
-----------------------------
-
-We will do the following steps in order:
-
-1. Load and normalizing the CIFAR10 training and test datasets using
-   ``torchvision``
-2. Define a Convolutional Neural Network
-3. Define a loss function
-4. Train the network on the training data
-5. Test the network on the test data
-
-1. Loading and normalizing CIFAR10
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-Using ``torchvision``, it’s extremely easy to load CIFAR10.
-"""
-import torch
-import torchvision
-import torchvision.transforms as transforms
-
-########################################################################
-# The output of torchvision datasets are PILImage images of range [0, 1].
-# We transform them to Tensors of normalized range [-1, 1].
-
-transform = transforms.Compose(
-  [transforms.ToTensor(),
-   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
-
-trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
-                                        download=True, transform=transform)
-trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
-                                          shuffle=True, num_workers=2)
-
-testset = torchvision.datasets.CIFAR10(root='./data', train=False,
-                                       download=True, transform=transform)
-testloader = torch.utils.data.DataLoader(testset, batch_size=4,
-                                         shuffle=False, num_workers=2)
-
-classes = ('plane', 'car', 'bird', 'cat',
-           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
-
-########################################################################
-# Let us show some of the training images, for fun.
-
-import matplotlib.pyplot as plt
-import numpy as np
-
-
-# functions to show an image
-
-
-def imshow(img):
-  img = img / 2 + 0.5  # unnormalize
-  npimg = img.numpy()
-  plt.imshow(np.transpose(npimg, (1, 2, 0)))
-  plt.show()
-
-
-# get some random training images
-dataiter = iter(trainloader)
-images, labels = dataiter.next()
-
-# show images
-imshow(torchvision.utils.make_grid(images))
-# print labels
-print(' '.join('%5s' % classes[labels[j]] for j in range(4)))
-
-########################################################################
-# 2. Define a Convolutional Neural Network
-# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-# Copy the neural network from the Neural Networks section before and modify it to
-# take 3-channel images (instead of 1-channel images as it was defined).
-
-import torch.nn as nn
-import torch.nn.functional as F
-
-
-class Net(nn.Module):
-  def __init__(self):
-    super(Net, self).__init__()
-    self.conv1 = nn.Conv2d(3, 6, 5)
-    self.pool = nn.MaxPool2d(2, 2)
-    self.conv2 = nn.Conv2d(6, 16, 5)
-    self.fc1 = nn.Linear(16 * 5 * 5, 120)
-    self.fc2 = nn.Linear(120, 84)
-    self.fc3 = nn.Linear(84, 10)
-
-  def forward(self, x):
-    x = self.pool(F.relu(self.conv1(x)))
-    x = self.pool(F.relu(self.conv2(x)))
-    x = x.view(-1, 16 * 5 * 5)
-    x = F.relu(self.fc1(x))
-    x = F.relu(self.fc2(x))
-    x = self.fc3(x)
-    return x
-
-
-net = Net()
-
-########################################################################
-# 3. Define a Loss function and optimizer
-# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-# Let's use a Classification Cross-Entropy loss and SGD with momentum.
-
-import torch.optim as optim
-
-criterion = nn.CrossEntropyLoss()
-optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
-
-########################################################################
-# 4. Train the network
-# ^^^^^^^^^^^^^^^^^^^^
-#
-# This is when things start to get interesting.
-# We simply have to loop over our data iterator, and feed the inputs to the
-# network and optimize.
-
-for epoch in range(2):  # loop over the dataset multiple times
-
-  running_loss = 0.0
-  for i, data in enumerate(trainloader, 0):
-    # get the inputs
-    inputs, labels = data
-
-    # zero the parameter gradients
-    optimizer.zero_grad()
-
-    # forward + backward + optimize
-    outputs = net(inputs)
-    loss = criterion(outputs, labels)
-    loss.backward()
-    optimizer.step()
-
-    # print statistics
-    running_loss += loss.item()
-    if i % 2000 == 1999:  # print every 2000 mini-batches
-      print('[%d, %5d] loss: %.3f' %
-            (epoch + 1, i + 1, running_loss / 2000))
-      running_loss = 0.0
-
-print('Finished Training')
-
-########################################################################
-# 5. Test the network on the test data
-# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-#
-# We have trained the network for 2 passes over the training dataset.
-# But we need to check if the network has learnt anything at all.
-#
-# We will check this by predicting the class label that the neural network
-# outputs, and checking it against the ground-truth. If the prediction is
-# correct, we add the sample to the list of correct predictions.
-#
-# Okay, first step. Let us display an image from the test set to get familiar.
-
-dataiter = iter(testloader)
-images, labels = dataiter.next()
-
-# print images
-imshow(torchvision.utils.make_grid(images))
-print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))
-
-########################################################################
-# Okay, now let us see what the neural network thinks these examples above are:
-
-outputs = net(images)
-
-########################################################################
-# The outputs are energies for the 10 classes.
-# The higher the energy for a class, the more the network
-# thinks that the image is of the particular class.
-# So, let's get the index of the highest energy:
-_, predicted = torch.max(outputs, 1)
-
-print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]
-                              for j in range(4)))
-
-########################################################################
-# The results seem pretty good.
-#
-# Let us look at how the network performs on the whole dataset.
-
-correct = 0
-total = 0
-with torch.no_grad():
-  for data in testloader:
-    images, labels = data
-    outputs = net(images)
-    _, predicted = torch.max(outputs.data, 1)
-    total += labels.size(0)
-    correct += (predicted == labels).sum().item()
-
-print('Accuracy of the network on the 10000 test images: %d %%' % (
-        100 * correct / total))
-
-########################################################################
-# That looks waaay better than chance, which is 10% accuracy (randomly picking
-# a class out of 10 classes).
-# Seems like the network learnt something.
-#
-# Hmmm, what are the classes that performed well, and the classes that did
-# not perform well:
-
-class_correct = list(0. for i in range(10))
-class_total = list(0. for i in range(10))
-with torch.no_grad():
-  for data in testloader:
-    images, labels = data
-    outputs = net(images)
-    _, predicted = torch.max(outputs, 1)
-    c = (predicted == labels).squeeze()
-    for i in range(4):
-      label = labels[i]
-      class_correct[label] += c[i].item()
-      class_total[label] += 1
-
-for i in range(10):
-  print('Accuracy of %5s : %2d %%' % (
-    classes[i], 100 * class_correct[i] / class_total[i]))
-
-########################################################################
-# Okay, so what next?
-#
-# How do we run these neural networks on the GPU?
-#
-# Training on GPU
-# ----------------
-# Just like how you transfer a Tensor onto the GPU, you transfer the neural
-# net onto the GPU.
-#
-# Let's first define our device as the first visible cuda device if we have
-# CUDA available:
-
-device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
-
-# Assuming that we are on a CUDA machine, this should print a CUDA device:
-
-print(device)
-
-########################################################################
-# The rest of this section assumes that ``device`` is a CUDA device.
-#
-# Then these methods will recursively go over all modules and convert their
-# parameters and buffers to CUDA tensors:
-#
-# .. code:: python
-#
-#     net.to(device)
-#
-#
-# Remember that you will have to send the inputs and targets at every step
-# to the GPU too:
-#
-# .. code:: python
-#
-#         inputs, labels = inputs.to(device), labels.to(device)
-#
-# Why dont I notice MASSIVE speedup compared to CPU? Because your network
-# is realllly small.
-#
-# **Exercise:** Try increasing the width of your network (argument 2 of
-# the first ``nn.Conv2d``, and argument 1 of the second ``nn.Conv2d`` –
-# they need to be the same number), see what kind of speedup you get.
-#
-# **Goals achieved**:
-#
-# - Understanding PyTorch's Tensor library and neural networks at a high level.
-# - Train a small neural network to classify images
-#
-# Training on multiple GPUs
-# -------------------------
-# If you want to see even more MASSIVE speedup using all of your GPUs,
-# please check out :doc:`data_parallel_tutorial`.
-#
-# Where do I go next?
-# -------------------
-#
-# -  :doc:`Train neural nets to play video games </intermediate/reinforcement_q_learning>`
-# -  `Train a state-of-the-art ResNet network on imagenet`_
-# -  `Train a face generator using Generative Adversarial Networks`_
-# -  `Train a word-level language model using Recurrent LSTM networks`_
-# -  `More examples`_
-# -  `More tutorials`_
-# -  `Discuss PyTorch on the Forums`_
-# -  `Chat with other users on Slack`_
-#
-# .. _Train a state-of-the-art ResNet network on imagenet: https://github.com/pytorch/examples/tree/master/imagenet
-# .. _Train a face generator using Generative Adversarial Networks: https://github.com/pytorch/examples/tree/master/dcgan
-# .. _Train a word-level language model using Recurrent LSTM networks: https://github.com/pytorch/examples/tree/master/word_language_model
-# .. _More examples: https://github.com/pytorch/examples
-# .. _More tutorials: https://github.com/pytorch/tutorials
-# .. _Discuss PyTorch on the Forums: https://discuss.pytorch.org/
-# .. _Chat with other users on Slack: https://pytorch.slack.com/messages/beginner/
-
-# %%%%%%INVISIBLE_CODE_BLOCK%%%%%%
-del dataiter
-# %%%%%%INVISIBLE_CODE_BLOCK%%%%%%
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/with-cifar10-models/ubuntu-16.04/Dockerfile.gpu.pytorch_latest b/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/with-cifar10-models/ubuntu-16.04/Dockerfile.gpu.pytorch_latest
deleted file mode 100644
index 83e8fdecdd90..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/pytorch/with-cifar10-models/ubuntu-16.04/Dockerfile.gpu.pytorch_latest
+++ /dev/null
@@ -1,21 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-FROM pytorch-latest-gpu-base:0.0.1
-
-RUN mkdir -p /test/data
-RUN chmod -R 777 /test
-ADD cifar10_tutorial.py /test/cifar10_tutorial.py
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/base/ubuntu-16.04/Dockerfile.cpu.tf_1.13.1 b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/base/ubuntu-16.04/Dockerfile.cpu.tf_1.13.1
deleted file mode 100644
index b32cb4161bd7..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/base/ubuntu-16.04/Dockerfile.cpu.tf_1.13.1
+++ /dev/null
@@ -1,71 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-FROM ubuntu:16.04
-
-# Pick up some TF dependencies
-RUN apt-get update && apt-get install -y --allow-downgrades --no-install-recommends \
-        --allow-change-held-packages --allow-unauthenticated \
-        build-essential libfreetype6-dev libpng12-dev \
-        libzmq3-dev pkg-config python python-dev \
-        rsync software-properties-common curl unzip wget grep sed vim iputils-ping net-tools gdb python2.7-dbg tzdata && \
-        apt-get clean && rm -rf /var/lib/apt/lists/*
-
-RUN export DEBIAN_FRONTEND=noninteractive && apt-get update && apt-get install -yq --no-install-recommends \
-        krb5-user libpam-krb5 && \
-        apt-get clean && rm -rf /var/lib/apt/lists/*
-
-RUN wget https://bootstrap.pypa.io/get-pip.py && \
-    python get-pip.py && \
-    rm get-pip.py
-
-RUN echo "Install python related packages" && \
-    apt-get update && \
-    apt-get install -y --no-install-recommends gfortran \
-    # numerical/algebra packages
-    libblas-dev libatlas-dev liblapack-dev \
-    # font, image for matplotlib
-    libpng-dev libxft-dev \
-    # for tkinter
-    python-tk libxml2-dev libxslt-dev zlib1g-dev && \
-    apt-get clean && rm -rf /var/lib/apt/lists/*
-
-RUN pip --no-cache-dir install Pillow h5py ipykernel jupyter matplotlib numpy pandas scipy sklearn && \
-    python -m ipykernel.kernelspec
-
-# Install TensorFlow CPU version.
-ENV TENSORFLOW_VERSION="1.13.1"
-RUN pip --no-cache-dir install \
-    http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-${TENSORFLOW_VERSION}-cp27-none-linux_x86_64.whl
-RUN apt-get update && apt-get install -y --no-install-recommends git && \
-    apt-get clean && rm -rf /var/lib/apt/lists/*
-
-# Install hadoop
-ENV HADOOP_VERSION="3.1.2"
-RUN wget http://mirrors.shu.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
-RUN tar zxf hadoop-${HADOOP_VERSION}.tar.gz
-RUN ln -s hadoop-${HADOOP_VERSION} hadoop-current
-RUN rm hadoop-${HADOOP_VERSION}.tar.gz
-
-ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
-RUN echo "$LOG_TAG Install java8" && \
-    apt-get update && \
-    apt-get install -y --no-install-recommends openjdk-8-jdk && \
-    apt-get clean && rm -rf /var/lib/apt/lists/*
-
-# Set the locale to fix bash warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
-RUN apt-get update && apt-get install -y --no-install-recommends locales && \
-    apt-get clean && rm -rf /var/lib/apt/lists/*
-RUN locale-gen en_US.UTF-8
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/base/ubuntu-16.04/Dockerfile.gpu.tf_1.13.1 b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/base/ubuntu-16.04/Dockerfile.gpu.tf_1.13.1
deleted file mode 100644
index 85f5ea1e55d0..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/base/ubuntu-16.04/Dockerfile.gpu.tf_1.13.1
+++ /dev/null
@@ -1,85 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04
-
-# Pick up some TF dependencies
-RUN apt-get update && apt-get install -y --allow-downgrades --no-install-recommends \
-        --allow-change-held-packages --allow-unauthenticated \
-        build-essential libfreetype6-dev libpng12-dev \
-        libzmq3-dev pkg-config python python-dev \
-        rsync software-properties-common curl unzip wget grep sed vim \
-        iputils-ping net-tools gdb python2.7-dbg tzdata \
-        cuda-command-line-tools-10-0 cuda-cublas-10-0 \
-        cuda-cufft-10-0 cuda-curand-10-0 cuda-cusolver-10-0 \
-        cuda-cusparse-10-0 libcudnn7=7.4.1.5-1+cuda10.0 && \
-        apt-get clean && rm -rf /var/lib/apt/lists/*
-
-# Install TensorRT
-RUN apt-get update && \
-        apt-get install -y --allow-unauthenticated --no-install-recommends \
-        nvinfer-runtime-trt-repo-ubuntu1604-5.0.2-ga-cuda10.0 && \
-        apt-get update && \
-        apt-get install -y --no-install-recommends \
-        libnvinfer5=5.0.2-1+cuda10.0 && \
-        apt-get clean && rm -rf /var/lib/apt/lists/*
-
-
-RUN export DEBIAN_FRONTEND=noninteractive && apt-get update && \
-        apt-get install -yq --no-install-recommends krb5-user libpam-krb5 \
-        && apt-get clean && rm -rf /var/lib/apt/lists/*
-
-RUN wget https://bootstrap.pypa.io/get-pip.py && \
-    python get-pip.py && \
-    rm get-pip.py
-
-RUN echo "Install python related packages" && \
-    apt-get -y update && \
-    apt-get install -y --no-install-recommends gfortran \
-    # numerical/algebra packages
-    libblas-dev libatlas-dev liblapack-dev \
-    # font, image for matplotlib
-    libpng-dev libxft-dev \
-    # for tkinter
-    python-tk libxml2-dev libxslt-dev zlib1g-dev && \
-    apt-get clean && rm -rf /var/lib/apt/lists/*
-
-RUN pip --no-cache-dir install Pillow h5py ipykernel jupyter matplotlib numpy pandas scipy sklearn && \
-    python -m ipykernel.kernelspec
-
-# Install TensorFlow GPU version.
-ENV TENSORFLOW_VERSION="1.13.1"
-RUN pip --no-cache-dir install \
-    http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-${TENSORFLOW_VERSION}-cp27-none-linux_x86_64.whl
-RUN apt-get update && apt-get install -y --no-install-recommends git && \
-    apt-get clean && rm -rf /var/lib/apt/lists/*
-
-# Install hadoop
-ENV HADOOP_VERSION="3.1.2"
-RUN wget http://mirrors.shu.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
-RUN tar zxf hadoop-${HADOOP_VERSION}.tar.gz
-RUN ln -s hadoop-${HADOOP_VERSION} hadoop-current
-RUN rm hadoop-${HADOOP_VERSION}.tar.gz
-
-ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
-RUN echo "$LOG_TAG Install java8" && \
-    apt-get -y update && \
-    apt-get install -y --no-install-recommends openjdk-8-jdk && \
-    rm -rf /var/lib/apt/lists/*
-
-# Set the locale to fix bash warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
-RUN apt-get update && apt-get install -y --no-install-recommends locales && \
-    apt-get clean && rm -rf /var/lib/apt/lists/*
-RUN locale-gen en_US.UTF-8
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/build-all.sh b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/build-all.sh
deleted file mode 100755
index 1e9848fc4c14..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/build-all.sh
+++ /dev/null
@@ -1,32 +0,0 @@
-#!/usr/bin/env bash
-
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-echo "Building base images"
-
-set -e
-
-cd base/ubuntu-16.04
-
-docker build . -f Dockerfile.cpu.tf_1.13.1 -t tf-1.13.1-cpu-base:0.0.1
-docker build . -f Dockerfile.gpu.tf_1.13.1 -t tf-1.13.1-gpu-base:0.0.1
-
-echo "Finished building base images"
-
-cd ../../with-cifar10-models/ubuntu-16.04
-
-docker build . -f Dockerfile.cpu.tf_1.13.1 -t tf-1.13.1-cpu:0.0.1
-docker build . -f Dockerfile.gpu.tf_1.13.1 -t tf-1.13.1-gpu:0.0.1
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/Dockerfile.cpu.tf_1.13.1 b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/Dockerfile.cpu.tf_1.13.1
deleted file mode 100644
index 188e4878b9b9..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/Dockerfile.cpu.tf_1.13.1
+++ /dev/null
@@ -1,22 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-FROM tf-1.13.1-cpu-base:0.0.1
-
-# Include models
-RUN mkdir /test
-ADD cifar10_estimator_tf_1.13.1 /test/cifar10_estimator
-RUN chown -R nobody /test
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/Dockerfile.gpu.tf_1.13.1 b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/Dockerfile.gpu.tf_1.13.1
deleted file mode 100644
index 8819fa619f09..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/Dockerfile.gpu.tf_1.13.1
+++ /dev/null
@@ -1,22 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-FROM tf-1.13.1-gpu-base:0.0.1
-
-# Include models
-RUN mkdir /test
-ADD cifar10_estimator_tf_1.13.1 /test/cifar10_estimator
-RUN chown -R nobody /test
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/README.md b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/README.md
deleted file mode 100644
index 5b4ae34e3d1d..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/README.md
+++ /dev/null
@@ -1,542 +0,0 @@
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-(Copied from https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator)
-
-CIFAR-10 is a common benchmark in machine learning for image recognition.
-
-http://www.cs.toronto.edu/~kriz/cifar.html
-
-Code in this directory focuses on how to use TensorFlow Estimators to train and
-evaluate a CIFAR-10 ResNet model on:
-
-* A single host with one CPU;
-* A single host with multiple GPUs;
-* Multiple hosts with CPU or multiple GPUs;
-
-Before trying to run the model we highly encourage you to read all the README.
-
-## Prerequisite
-
-1. [Install](https://www.tensorflow.org/install/) TensorFlow version 1.2.1 or
-later.
-
-2. Download the CIFAR-10 dataset and generate TFRecord files using the provided
-script.  The script and associated command below will download the CIFAR-10
-dataset and then generate a TFRecord for the training, validation, and
-evaluation datasets.
-
-```shell
-python generate_cifar10_tfrecords.py --data-dir=${PWD}/cifar-10-data
-```
-
-After running the command above, you should see the following files in the
---data-dir (```ls -R cifar-10-data```):
-
-* train.tfrecords
-* validation.tfrecords
-* eval.tfrecords
-
-
-## Training on a single machine with GPUs or CPU
-
-Run the training on CPU only. After training, it runs the evaluation.
-
-```
-python cifar10_main.py --data-dir=${PWD}/cifar-10-data \
-                       --job-dir=/tmp/cifar10 \
-                       --num-gpus=0 \
-                       --train-steps=1000
-```
-
-Run the model on 2 GPUs using CPU as parameter server. After training, it runs
-the evaluation.
-```
-python cifar10_main.py --data-dir=${PWD}/cifar-10-data \
-                       --job-dir=/tmp/cifar10 \
-                       --num-gpus=2 \
-                       --train-steps=1000
-```
-
-Run the model on 2 GPUs using GPU as parameter server.
-It will run an experiment, which for local setting basically means it will run
-stop training
-a couple of times to perform evaluation.
-
-```
-python cifar10_main.py --data-dir=${PWD}/cifar-10-data \
-                       --job-dir=/tmp/cifar10 \
-                       --variable-strategy GPU \
-                       --num-gpus=2 \
-```
-
-There are more command line flags to play with; run
-`python cifar10_main.py --help` for details.
-
-## Run distributed training
-
-### (Optional) Running on Google Cloud Machine Learning Engine
-
-This example can be run on Google Cloud Machine Learning Engine (ML Engine),
-which will configure the environment and take care of running workers,
-parameters servers, and masters in a fault tolerant way.
-
-To install the command line tool, and set up a project and billing, see the
-quickstart [here](https://cloud.google.com/ml-engine/docs/quickstarts/command-line).
-
-You'll also need a Google Cloud Storage bucket for the data. If you followed the
-instructions above, you can just run:
-
-```
-MY_BUCKET=gs://<my-bucket-name>
-gsutil cp -r ${PWD}/cifar-10-data $MY_BUCKET/
-```
-
-Then run the following command from the `tutorials/image` directory of this
-repository (the parent directory of this README):
-
-```
-gcloud ml-engine jobs submit training cifarmultigpu \
-    --runtime-version 1.2 \
-    --job-dir=$MY_BUCKET/model_dirs/cifarmultigpu \
-    --config cifar10_estimator/cmle_config.yaml \
-    --package-path cifar10_estimator/ \
-    --module-name cifar10_estimator.cifar10_main \
-    -- \
-    --data-dir=$MY_BUCKET/cifar-10-data \
-    --num-gpus=4 \
-    --train-steps=1000
-```
-
-
-### Set TF_CONFIG
-
-Considering that you already have multiple hosts configured, all you need is a
-`TF_CONFIG` environment variable on each host. You can set up the hosts manually
-or check [tensorflow/ecosystem](https://github.com/tensorflow/ecosystem) for
-instructions about how to set up a Cluster.
-
-The `TF_CONFIG` will be used by the `RunConfig` to know the existing hosts and
-their task: `master`, `ps` or `worker`.
-
-Here's an example of `TF_CONFIG`.
-
-```python
-cluster = {'master': ['master-ip:8000'],
-           'ps': ['ps-ip:8000'],
-           'worker': ['worker-ip:8000']}
-
-TF_CONFIG = json.dumps(
-  {'cluster': cluster,
-   'task': {'type': master, 'index': 0},
-   'model_dir': 'gs://<bucket_path>/<dir_path>',
-   'environment': 'cloud'
-  })
-```
-
-*Cluster*
-
-A cluster spec, which is basically a dictionary that describes all of the tasks
-in the cluster. More about it [here](https://www.tensorflow.org/deploy/distributed).
-
-In this cluster spec we are defining a cluster with 1 master, 1 ps and 1 worker.
-
-* `ps`: saves the parameters among all workers. All workers can
-   read/write/update the parameters for model via ps. As some models are
-   extremely large the parameters are shared among the ps (each ps stores a
-   subset).
-
-* `worker`: does the training.
-
-* `master`: basically a special worker, it does training, but also restores and
-   saves checkpoints and do evaluation.
-
-*Task*
-
-The Task defines what is the role of the current node, for this example the node
-is the master on index 0 on the cluster spec, the task will be different for
-each node. An example of the `TF_CONFIG` for a worker would be:
-
-```python
-cluster = {'master': ['master-ip:8000'],
-           'ps': ['ps-ip:8000'],
-           'worker': ['worker-ip:8000']}
-
-TF_CONFIG = json.dumps(
-  {'cluster': cluster,
-   'task': {'type': worker, 'index': 0},
-   'model_dir': 'gs://<bucket_path>/<dir_path>',
-   'environment': 'cloud'
-  })
-```
-
-*Model_dir*
-
-This is the path where the master will save the checkpoints, graph and
-TensorBoard files. For a multi host environment you may want to use a
-Distributed File System, Google Storage and DFS are supported.
-
-*Environment*
-
-By the default environment is *local*, for a distributed setting we need to
-change it to *cloud*.
-
-### Running script
-
-Once you have a `TF_CONFIG` configured properly on each host you're ready to run
-on distributed settings.
-
-#### Master
-Run this on master:
-Runs an Experiment in sync mode on 4 GPUs using CPU as parameter server for
-40000 steps. It will run evaluation a couple of times during training. The
-num_workers arugument is used only to update the learning rate correctly. Make
-sure the model_dir is the same as defined on the TF_CONFIG.
-
-```shell
-python cifar10_main.py --data-dir=gs://path/cifar-10-data \
-                       --job-dir=gs://path/model_dir/ \
-                       --num-gpus=4 \
-                       --train-steps=40000 \
-                       --sync \
-                       --num-workers=2
-```
-
-*Output:*
-
-```shell
-INFO:tensorflow:Using model_dir in TF_CONFIG: gs://path/model_dir/
-INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 1, '_keep_checkpoint_max': 5, '_task_type': u'master', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd16fb2be10>, '_model_dir': 'gs://path/model_dir/', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': intra_op_parallelism_threads: 1
-gpu_options {
-}
-allow_soft_placement: true
-, '_tf_random_seed': None, '_environment': u'cloud', '_num_worker_replicas': 1, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {
-  per_process_gpu_memory_fraction: 1.0
-}
-, '_evaluation_master': '', '_master': u'grpc://master-ip:8000'}
-...
-2017-08-01 19:59:26.496208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:
-name: Tesla K80
-major: 3 minor: 7 memoryClockRate (GHz) 0.8235
-pciBusID 0000:00:04.0
-Total memory: 11.17GiB
-Free memory: 11.09GiB
-2017-08-01 19:59:26.775660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 1 with properties:
-name: Tesla K80
-major: 3 minor: 7 memoryClockRate (GHz) 0.8235
-pciBusID 0000:00:05.0
-Total memory: 11.17GiB
-Free memory: 11.10GiB
-...
-2017-08-01 19:59:29.675171: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:8000
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 32, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)
-INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)
-INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=1; total_num_replicas=1
-INFO:tensorflow:Create CheckpointSaverHook.
-INFO:tensorflow:Restoring parameters from gs://path/model_dir/model.ckpt-0
-2017-08-01 19:59:37.560775: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 156fcb55fe6648d6 with config:
-intra_op_parallelism_threads: 1
-gpu_options {
-  per_process_gpu_memory_fraction: 1
-}
-allow_soft_placement: true
-
-INFO:tensorflow:Saving checkpoints for 1 into gs://path/model_dir/model.ckpt.
-INFO:tensorflow:loss = 1.20682, step = 1
-INFO:tensorflow:loss = 1.20682, learning_rate = 0.1
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 32, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)
-INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)
-INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=2; total_num_replicas=2
-INFO:tensorflow:Starting evaluation at 2017-08-01-20:00:14
-2017-08-01 20:00:15.745881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0)
-2017-08-01 20:00:15.745949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0)
-2017-08-01 20:00:15.745958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:00:06.0)
-2017-08-01 20:00:15.745964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:00:07.0)
-2017-08-01 20:00:15.745969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:4) -> (device: 4, name: Tesla K80, pci bus id: 0000:00:08.0)
-2017-08-01 20:00:15.745975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:5) -> (device: 5, name: Tesla K80, pci bus id: 0000:00:09.0)
-2017-08-01 20:00:15.745987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:6) -> (device: 6, name: Tesla K80, pci bus id: 0000:00:0a.0)
-2017-08-01 20:00:15.745997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:7) -> (device: 7, name: Tesla K80, pci bus id: 0000:00:0b.0)
-INFO:tensorflow:Restoring parameters from gs://path/model_dir/model.ckpt-10023
-INFO:tensorflow:Evaluation [1/100]
-INFO:tensorflow:Evaluation [2/100]
-INFO:tensorflow:Evaluation [3/100]
-INFO:tensorflow:Evaluation [4/100]
-INFO:tensorflow:Evaluation [5/100]
-INFO:tensorflow:Evaluation [6/100]
-INFO:tensorflow:Evaluation [7/100]
-INFO:tensorflow:Evaluation [8/100]
-INFO:tensorflow:Evaluation [9/100]
-INFO:tensorflow:Evaluation [10/100]
-INFO:tensorflow:Evaluation [11/100]
-INFO:tensorflow:Evaluation [12/100]
-INFO:tensorflow:Evaluation [13/100]
-...
-INFO:tensorflow:Evaluation [100/100]
-INFO:tensorflow:Finished evaluation at 2017-08-01-20:00:31
-INFO:tensorflow:Saving dict for global step 1: accuracy = 0.0994, global_step = 1, loss = 630.425
-```
-
-#### Worker
-
-Run this on worker:
-Runs an Experiment in sync mode on 4 GPUs using CPU as parameter server for
-40000 steps. It will run evaluation a couple of times during training. Make sure
-the model_dir is the same as defined on the TF_CONFIG.
-
-```shell
-python cifar10_main.py --data-dir=gs://path/cifar-10-data \
-                       --job-dir=gs://path/model_dir/ \
-                       --num-gpus=4 \
-                       --train-steps=40000 \
-                       --sync
-```
-
-*Output:*
-
-```shell
-INFO:tensorflow:Using model_dir in TF_CONFIG: gs://path/model_dir/
-INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600,
-'_num_ps_replicas': 1, '_keep_checkpoint_max': 5, '_task_type': u'worker',
-'_is_chief': False, '_cluster_spec':
-<tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6918438e10>,
-'_model_dir': 'gs://<path>/model_dir/',
-'_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000,
-'_session_config': intra_op_parallelism_threads: 1
-gpu_options {
-}
-allow_soft_placement: true
-, '_tf_random_seed': None, '_environment': u'cloud', '_num_worker_replicas': 1,
-'_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {
-  per_process_gpu_memory_fraction: 1.0
-  }
-...
-2017-08-01 19:59:26.496208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:
-name: Tesla K80
-major: 3 minor: 7 memoryClockRate (GHz) 0.8235
-pciBusID 0000:00:04.0
-Total memory: 11.17GiB
-Free memory: 11.09GiB
-2017-08-01 19:59:26.775660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 1 with properties:
-name: Tesla K80
-major: 3 minor: 7 memoryClockRate (GHz) 0.8235
-pciBusID 0000:00:05.0
-Total memory: 11.17GiB
-Free memory: 11.10GiB
-...
-2017-08-01 19:59:29.675171: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:8000
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 16, 32, 32)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 32, 16, 16)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 32, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 64, 8, 8)
-INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)
-INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)
-INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=2; total_num_replicas=2
-INFO:tensorflow:Create CheckpointSaverHook.
-2017-07-31 22:38:04.629150: I
-tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting
-for response from worker: /job:master/replica:0/task:0
-2017-07-31 22:38:09.263492: I
-tensorflow/core/distributed_runtime/master_session.cc:999] Start master
-session cc58f93b1e259b0c with config:
-intra_op_parallelism_threads: 1
-gpu_options {
-per_process_gpu_memory_fraction: 1
-}
-allow_soft_placement: true
-INFO:tensorflow:loss = 5.82382, step = 0
-INFO:tensorflow:loss = 5.82382, learning_rate = 0.8
-INFO:tensorflow:Average examples/sec: 1116.92 (1116.92), step = 10
-INFO:tensorflow:Average examples/sec: 1233.73 (1377.83), step = 20
-INFO:tensorflow:Average examples/sec: 1485.43 (2509.3), step = 30
-INFO:tensorflow:Average examples/sec: 1680.27 (2770.39), step = 40
-INFO:tensorflow:Average examples/sec: 1825.38 (2788.78), step = 50
-INFO:tensorflow:Average examples/sec: 1929.32 (2697.27), step = 60
-INFO:tensorflow:Average examples/sec: 2015.17 (2749.05), step = 70
-INFO:tensorflow:loss = 37.6272, step = 79 (19.554 sec)
-INFO:tensorflow:loss = 37.6272, learning_rate = 0.8 (19.554 sec)
-INFO:tensorflow:Average examples/sec: 2074.92 (2618.36), step = 80
-INFO:tensorflow:Average examples/sec: 2132.71 (2744.13), step = 90
-INFO:tensorflow:Average examples/sec: 2183.38 (2777.21), step = 100
-INFO:tensorflow:Average examples/sec: 2224.4 (2739.03), step = 110
-INFO:tensorflow:Average examples/sec: 2240.28 (2431.26), step = 120
-INFO:tensorflow:Average examples/sec: 2272.12 (2739.32), step = 130
-INFO:tensorflow:Average examples/sec: 2300.68 (2750.03), step = 140
-INFO:tensorflow:Average examples/sec: 2325.81 (2745.63), step = 150
-INFO:tensorflow:Average examples/sec: 2347.14 (2721.53), step = 160
-INFO:tensorflow:Average examples/sec: 2367.74 (2754.54), step = 170
-INFO:tensorflow:loss = 27.8453, step = 179 (18.893 sec)
-...
-```
-
-#### PS
-
-Run this on ps:
-The ps will not do training so most of the arguments won't affect the execution
-
-```shell
-python cifar10_main.py --job-dir=gs://path/model_dir/
-```
-
-*Output:*
-
-```shell
-INFO:tensorflow:Using model_dir in TF_CONFIG: gs://path/model_dir/
-INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 1, '_keep_checkpoint_max': 5, '_task_type': u'ps', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f48f1addf90>, '_model_dir': 'gs://path/model_dir/', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': intra_op_parallelism_threads: 1
-gpu_options {
-}
-allow_soft_placement: true
-, '_tf_random_seed': None, '_environment': u'cloud', '_num_worker_replicas': 1, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {
-  per_process_gpu_memory_fraction: 1.0
-}
-, '_evaluation_master': '', '_master': u'grpc://master-ip:8000'}
-2017-07-31 22:54:58.928088: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-ip:8000}
-2017-07-31 22:54:58.928153: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:8000}
-2017-07-31 22:54:58.928160: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-ip:8000}
-2017-07-31 22:54:58.929873: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:8000
-```
-
-## Visualizing results with TensorBoard
-
-When using Estimators you can also visualize your data in TensorBoard, with no
-changes in your code. You can use TensorBoard to visualize your TensorFlow
-graph, plot quantitative metrics about the execution of your graph, and show
-additional data like images that pass through it.
-
-You'll see something similar to this if you "point" TensorBoard to the
-`job dir` parameter you used to train or evaluate your model.
-
-Check TensorBoard during training or after it. Just point TensorBoard to the
-model_dir you chose on the previous step.
-
-```shell
-tensorboard --log-dir="<job dir>"
-```
-
-## Warnings
-
-When runninng `cifar10_main.py` with `--sync` argument you may see an error
-similar to:
-
-```python
-File "cifar10_main.py", line 538, in <module>
-    tf.app.run()
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 48, in run
-    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
-File "cifar10_main.py", line 518, in main
-    hooks), run_config=config)
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 210, in run
-    return _execute_schedule(experiment, schedule)
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py", line 47, in _execute_schedule
-    return task()
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 501, in train_and_evaluate
-    hooks=self._eval_hooks)
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 681, in _call_evaluate
-    hooks=hooks)
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py", line 292, in evaluate
-    name=name)
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py", line 638, in _evaluate_model
-    features, labels, model_fn_lib.ModeKeys.EVAL)
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py", line 545, in _call_model_fn
-    features=features, labels=labels, **kwargs)
-File "cifar10_main.py", line 331, in _resnet_model_fn
-    gradvars, global_step=tf.train.get_global_step())
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py", line 252, in apply_gradients
-    variables.global_variables())
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py", line 170, in wrapped
-    return _add_should_use_warning(fn(*args, **kwargs))
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py", line 139, in _add_should_use_warning
-    wrapped = TFShouldUseWarningWrapper(x)
-File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py", line 96, in __init__
-    stack = [s.strip() for s in traceback.format_stack()]
-```
-
-This should not affect your training, and should be fixed on the next releases.
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10.py b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10.py
deleted file mode 100644
index 5e1a70895ad8..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10.py
+++ /dev/null
@@ -1,113 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""CIFAR-10 data set.
-
-See http://www.cs.toronto.edu/~kriz/cifar.html.
-"""
-import os
-
-import tensorflow as tf
-
-HEIGHT = 32
-WIDTH = 32
-DEPTH = 3
-
-
-class Cifar10DataSet(object):
-  """Cifar10 data set.
-
-  Described by http://www.cs.toronto.edu/~kriz/cifar.html.
-  """
-
-  def __init__(self, data_dir, subset='train', use_distortion=True):
-    self.data_dir = data_dir
-    self.subset = subset
-    self.use_distortion = use_distortion
-
-  def get_filenames(self):
-    if self.subset in ['train', 'validation', 'eval']:
-      return [os.path.join(self.data_dir, self.subset + '.tfrecords')]
-    else:
-      raise ValueError('Invalid data subset "%s"' % self.subset)
-
-  def parser(self, serialized_example):
-    """Parses a single tf.Example into image and label tensors."""
-    # Dimensions of the images in the CIFAR-10 dataset.
-    # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the
-    # input format.
-    features = tf.parse_single_example(
-        serialized_example,
-        features={
-            'image': tf.FixedLenFeature([], tf.string),
-            'label': tf.FixedLenFeature([], tf.int64),
-        })
-    image = tf.decode_raw(features['image'], tf.uint8)
-    image.set_shape([DEPTH * HEIGHT * WIDTH])
-
-    # Reshape from [depth * height * width] to [depth, height, width].
-    image = tf.cast(
-        tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0]),
-        tf.float32)
-    label = tf.cast(features['label'], tf.int32)
-
-    # Custom preprocessing.
-    image = self.preprocess(image)
-
-    return image, label
-
-  def make_batch(self, batch_size):
-    """Read the images and labels from 'filenames'."""
-    filenames = self.get_filenames()
-    # Repeat infinitely.
-    dataset = tf.data.TFRecordDataset(filenames).repeat()
-
-    # Parse records.
-    dataset = dataset.map(
-        self.parser, num_parallel_calls=batch_size)
-
-    # Potentially shuffle records.
-    if self.subset == 'train':
-      min_queue_examples = int(
-          Cifar10DataSet.num_examples_per_epoch(self.subset) * 0.4)
-      # Ensure that the capacity is sufficiently large to provide good random
-      # shuffling.
-      dataset = dataset.shuffle(buffer_size=min_queue_examples + 3 * batch_size)
-
-    # Batch it up.
-    dataset = dataset.batch(batch_size)
-    iterator = dataset.make_one_shot_iterator()
-    image_batch, label_batch = iterator.get_next()
-
-    return image_batch, label_batch
-
-  def preprocess(self, image):
-    """Preprocess a single image in [height, width, depth] layout."""
-    if self.subset == 'train' and self.use_distortion:
-      # Pad 4 pixels on each dimension of feature map, done in mini-batch
-      image = tf.image.resize_image_with_crop_or_pad(image, 40, 40)
-      image = tf.random_crop(image, [HEIGHT, WIDTH, DEPTH])
-      image = tf.image.random_flip_left_right(image)
-    return image
-
-  @staticmethod
-  def num_examples_per_epoch(subset='train'):
-    if subset == 'train':
-      return 45000
-    elif subset == 'validation':
-      return 5000
-    elif subset == 'eval':
-      return 10000
-    else:
-      raise ValueError('Invalid data subset "%s"' % subset)
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10_main.py b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10_main.py
deleted file mode 100644
index 51da6b94fa2d..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10_main.py
+++ /dev/null
@@ -1,521 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""ResNet model for classifying images from CIFAR-10 dataset.
-
-Support single-host training with one or multiple devices.
-
-ResNet as proposed in:
-Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
-Deep Residual Learning for Image Recognition. arXiv:1512.03385
-
-CIFAR-10 as in:
-http://www.cs.toronto.edu/~kriz/cifar.html
-
-
-"""
-from __future__ import division
-from __future__ import print_function
-
-import argparse
-import functools
-import itertools
-import os
-
-import cifar10
-import cifar10_model
-import cifar10_utils
-import numpy as np
-import six
-from six.moves import xrange  # pylint: disable=redefined-builtin
-import tensorflow as tf
-
-tf.logging.set_verbosity(tf.logging.INFO)
-
-
-def get_model_fn(num_gpus, variable_strategy, num_workers):
-  """Returns a function that will build the resnet model."""
-
-  def _resnet_model_fn(features, labels, mode, params):
-    """Resnet model body.
-
-    Support single host, one or more GPU training. Parameter distribution can
-    be either one of the following scheme.
-    1. CPU is the parameter server and manages gradient updates.
-    2. Parameters are distributed evenly across all GPUs, and the first GPU
-       manages gradient updates.
-
-    Args:
-      features: a list of tensors, one for each tower
-      labels: a list of tensors, one for each tower
-      mode: ModeKeys.TRAIN or EVAL
-      params: Hyperparameters suitable for tuning
-    Returns:
-      A EstimatorSpec object.
-    """
-    is_training = (mode == tf.estimator.ModeKeys.TRAIN)
-    weight_decay = params.weight_decay
-    momentum = params.momentum
-
-    tower_features = features
-    tower_labels = labels
-    tower_losses = []
-    tower_gradvars = []
-    tower_preds = []
-
-    # channels first (NCHW) is normally optimal on GPU and channels last (NHWC)
-    # on CPU. The exception is Intel MKL on CPU which is optimal with
-    # channels_last.
-    data_format = params.data_format
-    if not data_format:
-      if num_gpus == 0:
-        data_format = 'channels_last'
-      else:
-        data_format = 'channels_first'
-
-    if num_gpus == 0:
-      num_devices = 1
-      device_type = 'cpu'
-    else:
-      num_devices = num_gpus
-      device_type = 'gpu'
-
-    for i in range(num_devices):
-      worker_device = '/{}:{}'.format(device_type, i)
-      if variable_strategy == 'CPU':
-        device_setter = cifar10_utils.local_device_setter(
-            worker_device=worker_device)
-      elif variable_strategy == 'GPU':
-        device_setter = cifar10_utils.local_device_setter(
-            ps_device_type='gpu',
-            worker_device=worker_device,
-            ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(
-                num_gpus, tf.contrib.training.byte_size_load_fn))
-      with tf.variable_scope('resnet', reuse=bool(i != 0)):
-        with tf.name_scope('tower_%d' % i) as name_scope:
-          with tf.device(device_setter):
-            loss, gradvars, preds = _tower_fn(
-                is_training, weight_decay, tower_features[i], tower_labels[i],
-                data_format, params.num_layers, params.batch_norm_decay,
-                params.batch_norm_epsilon)
-            tower_losses.append(loss)
-            tower_gradvars.append(gradvars)
-            tower_preds.append(preds)
-            if i == 0:
-              # Only trigger batch_norm moving mean and variance update from
-              # the 1st tower. Ideally, we should grab the updates from all
-              # towers but these stats accumulate extremely fast so we can
-              # ignore the other stats from the other towers without
-              # significant detriment.
-              update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,
-                                             name_scope)
-
-    # Now compute global loss and gradients.
-    gradvars = []
-    with tf.name_scope('gradient_averaging'):
-      all_grads = {}
-      for grad, var in itertools.chain(*tower_gradvars):
-        if grad is not None:
-          all_grads.setdefault(var, []).append(grad)
-      for var, grads in six.iteritems(all_grads):
-        # Average gradients on the same device as the variables
-        # to which they apply.
-        with tf.device(var.device):
-          if len(grads) == 1:
-            avg_grad = grads[0]
-          else:
-            avg_grad = tf.multiply(tf.add_n(grads), 1. / len(grads))
-        gradvars.append((avg_grad, var))
-
-    # Device that runs the ops to apply global gradient updates.
-    consolidation_device = '/gpu:0' if variable_strategy == 'GPU' else '/cpu:0'
-    with tf.device(consolidation_device):
-      # Suggested learning rate scheduling from
-      # https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/cifar10-resnet.py#L155
-      num_batches_per_epoch = cifar10.Cifar10DataSet.num_examples_per_epoch(
-          'train') // (params.train_batch_size * num_workers)
-      boundaries = [
-          num_batches_per_epoch * x
-          for x in np.array([82, 123, 300], dtype=np.int64)
-      ]
-      staged_lr = [params.learning_rate * x for x in [1, 0.1, 0.01, 0.002]]
-
-      learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(),
-                                                  boundaries, staged_lr)
-
-      loss = tf.reduce_mean(tower_losses, name='loss')
-
-      examples_sec_hook = cifar10_utils.ExamplesPerSecondHook(
-          params.train_batch_size, every_n_steps=10)
-
-      tensors_to_log = {'learning_rate': learning_rate, 'loss': loss}
-
-      logging_hook = tf.train.LoggingTensorHook(
-          tensors=tensors_to_log, every_n_iter=100)
-
-      train_hooks = [logging_hook, examples_sec_hook]
-
-      optimizer = tf.train.MomentumOptimizer(
-          learning_rate=learning_rate, momentum=momentum)
-
-      if params.sync:
-        optimizer = tf.train.SyncReplicasOptimizer(
-            optimizer, replicas_to_aggregate=num_workers)
-        sync_replicas_hook = optimizer.make_session_run_hook(params.is_chief)
-        train_hooks.append(sync_replicas_hook)
-
-      # Create single grouped train op
-      train_op = [
-          optimizer.apply_gradients(
-              gradvars, global_step=tf.train.get_global_step())
-      ]
-      train_op.extend(update_ops)
-      train_op = tf.group(*train_op)
-
-      predictions = {
-          'classes':
-              tf.concat([p['classes'] for p in tower_preds], axis=0),
-          'probabilities':
-              tf.concat([p['probabilities'] for p in tower_preds], axis=0)
-      }
-      stacked_labels = tf.concat(labels, axis=0)
-      metrics = {
-          'accuracy':
-              tf.metrics.accuracy(stacked_labels, predictions['classes'])
-      }
-
-    return tf.estimator.EstimatorSpec(
-        mode=mode,
-        predictions=predictions,
-        loss=loss,
-        train_op=train_op,
-        training_hooks=train_hooks,
-        eval_metric_ops=metrics)
-
-  return _resnet_model_fn
-
-
-def _tower_fn(is_training, weight_decay, feature, label, data_format,
-              num_layers, batch_norm_decay, batch_norm_epsilon):
-  """Build computation tower (Resnet).
-
-  Args:
-    is_training: true if is training graph.
-    weight_decay: weight regularization strength, a float.
-    feature: a Tensor.
-    label: a Tensor.
-    data_format: channels_last (NHWC) or channels_first (NCHW).
-    num_layers: number of layers, an int.
-    batch_norm_decay: decay for batch normalization, a float.
-    batch_norm_epsilon: epsilon for batch normalization, a float.
-
-  Returns:
-    A tuple with the loss for the tower, the gradients and parameters, and
-    predictions.
-
-  """
-  model = cifar10_model.ResNetCifar10(
-      num_layers,
-      batch_norm_decay=batch_norm_decay,
-      batch_norm_epsilon=batch_norm_epsilon,
-      is_training=is_training,
-      data_format=data_format)
-  logits = model.forward_pass(feature, input_data_format='channels_last')
-  tower_pred = {
-      'classes': tf.argmax(input=logits, axis=1),
-      'probabilities': tf.nn.softmax(logits)
-  }
-
-  tower_loss = tf.losses.sparse_softmax_cross_entropy(
-      logits=logits, labels=label)
-  tower_loss = tf.reduce_mean(tower_loss)
-
-  model_params = tf.trainable_variables()
-  tower_loss += weight_decay * tf.add_n(
-      [tf.nn.l2_loss(v) for v in model_params])
-
-  tower_grad = tf.gradients(tower_loss, model_params)
-
-  return tower_loss, zip(tower_grad, model_params), tower_pred
-
-
-def input_fn(data_dir,
-             subset,
-             num_shards,
-             batch_size,
-             use_distortion_for_training=True):
-  """Create input graph for model.
-
-  Args:
-    data_dir: Directory where TFRecords representing the dataset are located.
-    subset: one of 'train', 'validate' and 'eval'.
-    num_shards: num of towers participating in data-parallel training.
-    batch_size: total batch size for training to be divided by the number of
-    shards.
-    use_distortion_for_training: True to use distortions.
-  Returns:
-    two lists of tensors for features and labels, each of num_shards length.
-  """
-  with tf.device('/cpu:0'):
-    use_distortion = subset == 'train' and use_distortion_for_training
-    dataset = cifar10.Cifar10DataSet(data_dir, subset, use_distortion)
-    image_batch, label_batch = dataset.make_batch(batch_size)
-    if num_shards <= 1:
-      # No GPU available or only 1 GPU.
-      return [image_batch], [label_batch]
-
-    # Note that passing num=batch_size is safe here, even though
-    # dataset.batch(batch_size) can, in some cases, return fewer than batch_size
-    # examples. This is because it does so only when repeating for a limited
-    # number of epochs, but our dataset repeats forever.
-    image_batch = tf.unstack(image_batch, num=batch_size, axis=0)
-    label_batch = tf.unstack(label_batch, num=batch_size, axis=0)
-    feature_shards = [[] for i in range(num_shards)]
-    label_shards = [[] for i in range(num_shards)]
-    for i in xrange(batch_size):
-      idx = i % num_shards
-      feature_shards[idx].append(image_batch[i])
-      label_shards[idx].append(label_batch[i])
-    feature_shards = [tf.parallel_stack(x) for x in feature_shards]
-    label_shards = [tf.parallel_stack(x) for x in label_shards]
-    return feature_shards, label_shards
-
-
-def get_experiment_fn(data_dir,
-                      num_gpus,
-                      variable_strategy,
-                      use_distortion_for_training=True):
-  """Returns an Experiment function.
-
-  Experiments perform training on several workers in parallel,
-  in other words experiments know how to invoke train and eval in a sensible
-  fashion for distributed training. Arguments passed directly to this
-  function are not tunable, all other arguments should be passed within
-  tf.HParams, passed to the enclosed function.
-
-  Args:
-      data_dir: str. Location of the data for input_fns.
-      num_gpus: int. Number of GPUs on each worker.
-      variable_strategy: String. CPU to use CPU as the parameter server
-      and GPU to use the GPUs as the parameter server.
-      use_distortion_for_training: bool. See cifar10.Cifar10DataSet.
-  Returns:
-      A function (tf.estimator.RunConfig, tf.contrib.training.HParams) ->
-      tf.contrib.learn.Experiment.
-
-      Suitable for use by tf.contrib.learn.learn_runner, which will run various
-      methods on Experiment (train, evaluate) based on information
-      about the current runner in `run_config`.
-  """
-
-  def _experiment_fn(run_config, hparams):
-    """Returns an Experiment."""
-    # Create estimator.
-    train_input_fn = functools.partial(
-        input_fn,
-        data_dir,
-        subset='train',
-        num_shards=num_gpus,
-        batch_size=hparams.train_batch_size,
-        use_distortion_for_training=use_distortion_for_training)
-
-    eval_input_fn = functools.partial(
-        input_fn,
-        data_dir,
-        subset='eval',
-        batch_size=hparams.eval_batch_size,
-        num_shards=num_gpus)
-
-    num_eval_examples = cifar10.Cifar10DataSet.num_examples_per_epoch('eval')
-    if num_eval_examples % hparams.eval_batch_size != 0:
-      raise ValueError(
-          'validation set size must be multiple of eval_batch_size')
-
-    train_steps = hparams.train_steps
-    eval_steps = num_eval_examples // hparams.eval_batch_size
- 
-    classifier = tf.estimator.Estimator(
-        model_fn=get_model_fn(num_gpus, variable_strategy,
-                              run_config.num_worker_replicas or 1),
-        config=run_config,
-        params=hparams)
-
-    # Create experiment.
-    return tf.contrib.learn.Experiment(
-        classifier,
-        train_input_fn=train_input_fn,
-        eval_input_fn=eval_input_fn,
-        train_steps=train_steps,
-        eval_steps=eval_steps)
-
-  return _experiment_fn
-
-
-def main(job_dir, data_dir, num_gpus, variable_strategy,
-         use_distortion_for_training, log_device_placement, num_intra_threads,
-         **hparams):
-  # The env variable is on deprecation path, default is set to off.
-  os.environ['TF_SYNC_ON_FINISH'] = '0'
-  os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'
-
-  # Session configuration.
-  sess_config = tf.ConfigProto(
-      allow_soft_placement=True,
-      log_device_placement=log_device_placement,
-      intra_op_parallelism_threads=num_intra_threads,
-      gpu_options=tf.GPUOptions(force_gpu_compatible=True))
-
-  config = cifar10_utils.RunConfig(
-      session_config=sess_config, model_dir=job_dir)
-  tf.contrib.learn.learn_runner.run(
-      get_experiment_fn(data_dir, num_gpus, variable_strategy,
-                        use_distortion_for_training),
-      run_config=config,
-      hparams=tf.contrib.training.HParams(
-          is_chief=config.is_chief,
-          **hparams))
-
-
-if __name__ == '__main__':
-  parser = argparse.ArgumentParser()
-  parser.add_argument(
-      '--data-dir',
-      type=str,
-      required=True,
-      help='The directory where the CIFAR-10 input data is stored.')
-  parser.add_argument(
-      '--job-dir',
-      type=str,
-      required=True,
-      help='The directory where the model will be stored.')
-  parser.add_argument(
-      '--variable-strategy',
-      choices=['CPU', 'GPU'],
-      type=str,
-      default='CPU',
-      help='Where to locate variable operations')
-  parser.add_argument(
-      '--num-gpus',
-      type=int,
-      default=1,
-      help='The number of gpus used. Uses only CPU if set to 0.')
-  parser.add_argument(
-      '--num-layers',
-      type=int,
-      default=44,
-      help='The number of layers of the model.')
-  parser.add_argument(
-      '--train-steps',
-      type=int,
-      default=80000,
-      help='The number of steps to use for training.')
-  parser.add_argument(
-      '--train-batch-size',
-      type=int,
-      default=128,
-      help='Batch size for training.')
-  parser.add_argument(
-      '--eval-batch-size',
-      type=int,
-      default=100,
-      help='Batch size for validation.')
-  parser.add_argument(
-      '--momentum',
-      type=float,
-      default=0.9,
-      help='Momentum for MomentumOptimizer.')
-  parser.add_argument(
-      '--weight-decay',
-      type=float,
-      default=2e-4,
-      help='Weight decay for convolutions.')
-  parser.add_argument(
-      '--learning-rate',
-      type=float,
-      default=0.1,
-      help="""\
-      This is the inital learning rate value. The learning rate will decrease
-      during training. For more details check the model_fn implementation in
-      this file.\
-      """)
-  parser.add_argument(
-      '--use-distortion-for-training',
-      type=bool,
-      default=True,
-      help='If doing image distortion for training.')
-  parser.add_argument(
-      '--sync',
-      action='store_true',
-      default=False,
-      help="""\
-      If present when running in a distributed environment will run on sync mode.\
-      """)
-  parser.add_argument(
-      '--num-intra-threads',
-      type=int,
-      default=0,
-      help="""\
-      Number of threads to use for intra-op parallelism. When training on CPU
-      set to 0 to have the system pick the appropriate number or alternatively
-      set it to the number of physical CPU cores.\
-      """)
-  parser.add_argument(
-      '--num-inter-threads',
-      type=int,
-      default=0,
-      help="""\
-      Number of threads to use for inter-op parallelism. If set to 0, the
-      system will pick an appropriate number.\
-      """)
-  parser.add_argument(
-      '--data-format',
-      type=str,
-      default=None,
-      help="""\
-      If not set, the data format best for the training device is used. 
-      Allowed values: channels_first (NCHW) channels_last (NHWC).\
-      """)
-  parser.add_argument(
-      '--log-device-placement',
-      action='store_true',
-      default=False,
-      help='Whether to log device placement.')
-  parser.add_argument(
-      '--batch-norm-decay',
-      type=float,
-      default=0.997,
-      help='Decay for batch norm.')
-  parser.add_argument(
-      '--batch-norm-epsilon',
-      type=float,
-      default=1e-5,
-      help='Epsilon for batch norm.')
-  args = parser.parse_args()
-
-  if args.num_gpus > 0:
-    assert tf.test.is_gpu_available(), "Requested GPUs but none found."
-  if args.num_gpus < 0:
-    raise ValueError(
-        'Invalid GPU count: \"--num-gpus\" must be 0 or a positive integer.')
-  if args.num_gpus == 0 and args.variable_strategy == 'GPU':
-    raise ValueError('num-gpus=0, CPU must be used as parameter server. Set'
-                     '--variable-strategy=CPU.')
-  if (args.num_layers - 2) % 6 != 0:
-    raise ValueError('Invalid --num-layers parameter.')
-  if args.num_gpus != 0 and args.train_batch_size % args.num_gpus != 0:
-    raise ValueError('--train-batch-size must be multiple of --num-gpus.')
-  if args.num_gpus != 0 and args.eval_batch_size % args.num_gpus != 0:
-    raise ValueError('--eval-batch-size must be multiple of --num-gpus.')
-
-  main(**vars(args))
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10_model.py b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10_model.py
deleted file mode 100644
index d67c233dbba5..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10_model.py
+++ /dev/null
@@ -1,80 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""Model class for Cifar10 Dataset."""
-from __future__ import division
-from __future__ import print_function
-
-import tensorflow as tf
-
-import model_base
-
-
-class ResNetCifar10(model_base.ResNet):
-  """Cifar10 model with ResNetV1 and basic residual block."""
-
-  def __init__(self,
-               num_layers,
-               is_training,
-               batch_norm_decay,
-               batch_norm_epsilon,
-               data_format='channels_first'):
-    super(ResNetCifar10, self).__init__(
-        is_training,
-        data_format,
-        batch_norm_decay,
-        batch_norm_epsilon
-    )
-    self.n = (num_layers - 2) // 6
-    # Add one in case label starts with 1. No impact if label starts with 0.
-    self.num_classes = 10 + 1
-    self.filters = [16, 16, 32, 64]
-    self.strides = [1, 2, 2]
-
-  def forward_pass(self, x, input_data_format='channels_last'):
-    """Build the core model within the graph."""
-    if self._data_format != input_data_format:
-      if input_data_format == 'channels_last':
-        # Computation requires channels_first.
-        x = tf.transpose(x, [0, 3, 1, 2])
-      else:
-        # Computation requires channels_last.
-        x = tf.transpose(x, [0, 2, 3, 1])
-
-    # Image standardization.
-    x = x / 128 - 1
-
-    x = self._conv(x, 3, 16, 1)
-    x = self._batch_norm(x)
-    x = self._relu(x)
-
-    # Use basic (non-bottleneck) block and ResNet V1 (post-activation).
-    res_func = self._residual_v1
-
-    # 3 stages of block stacking.
-    for i in range(3):
-      with tf.name_scope('stage'):
-        for j in range(self.n):
-          if j == 0:
-            # First block in a stage, filters and strides may change.
-            x = res_func(x, 3, self.filters[i], self.filters[i + 1],
-                         self.strides[i])
-          else:
-            # Following blocks in a stage, constant filters and unit stride.
-            x = res_func(x, 3, self.filters[i + 1], self.filters[i + 1], 1)
-
-    x = self._global_avg_pool(x)
-    x = self._fully_connected(x, self.num_classes)
-
-    return x
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10_utils.py b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10_utils.py
deleted file mode 100644
index 5eb2c3f62fe9..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/cifar10_utils.py
+++ /dev/null
@@ -1,153 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-import collections
-import six
-
-import tensorflow as tf
-
-from tensorflow.python.platform import tf_logging as logging
-from tensorflow.core.framework import node_def_pb2
-from tensorflow.python.framework import device as pydev
-from tensorflow.python.training import basic_session_run_hooks
-from tensorflow.python.training import session_run_hook
-from tensorflow.python.training import training_util
-from tensorflow.python.training import device_setter
-from tensorflow.contrib.learn.python.learn import run_config
-
-
-# TODO(b/64848083) Remove once uid bug is fixed
-class RunConfig(tf.contrib.learn.RunConfig): 
-  def uid(self, whitelist=None):
-    """Generates a 'Unique Identifier' based on all internal fields.
-    Caller should use the uid string to check `RunConfig` instance integrity
-    in one session use, but should not rely on the implementation details, which
-    is subject to change.
-    Args:
-      whitelist: A list of the string names of the properties uid should not
-        include. If `None`, defaults to `_DEFAULT_UID_WHITE_LIST`, which
-        includes most properties user allowes to change.
-    Returns:
-      A uid string.
-    """
-    if whitelist is None:
-      whitelist = run_config._DEFAULT_UID_WHITE_LIST
-
-    state = {k: v for k, v in self.__dict__.items() if not k.startswith('__')}
-    # Pop out the keys in whitelist.
-    for k in whitelist:
-      state.pop('_' + k, None)
-
-    ordered_state = collections.OrderedDict(
-        sorted(state.items(), key=lambda t: t[0]))
-    # For class instance without __repr__, some special cares are required.
-    # Otherwise, the object address will be used.
-    if '_cluster_spec' in ordered_state:
-      ordered_state['_cluster_spec'] = collections.OrderedDict(
-         sorted(ordered_state['_cluster_spec'].as_dict().items(),
-                key=lambda t: t[0])
-      )
-    return ', '.join(
-        '%s=%r' % (k, v) for (k, v) in six.iteritems(ordered_state)) 
-
-
-class ExamplesPerSecondHook(session_run_hook.SessionRunHook):
-  """Hook to print out examples per second.
-
-    Total time is tracked and then divided by the total number of steps
-    to get the average step time and then batch_size is used to determine
-    the running average of examples per second. The examples per second for the
-    most recent interval is also logged.
-  """
-
-  def __init__(
-      self,
-      batch_size,
-      every_n_steps=100,
-      every_n_secs=None,):
-    """Initializer for ExamplesPerSecondHook.
-
-      Args:
-      batch_size: Total batch size used to calculate examples/second from
-      global time.
-      every_n_steps: Log stats every n steps.
-      every_n_secs: Log stats every n seconds.
-    """
-    if (every_n_steps is None) == (every_n_secs is None):
-      raise ValueError('exactly one of every_n_steps'
-                       ' and every_n_secs should be provided.')
-    self._timer = basic_session_run_hooks.SecondOrStepTimer(
-        every_steps=every_n_steps, every_secs=every_n_secs)
-
-    self._step_train_time = 0
-    self._total_steps = 0
-    self._batch_size = batch_size
-
-  def begin(self):
-    self._global_step_tensor = training_util.get_global_step()
-    if self._global_step_tensor is None:
-      raise RuntimeError(
-          'Global step should be created to use StepCounterHook.')
-
-  def before_run(self, run_context):  # pylint: disable=unused-argument
-    return basic_session_run_hooks.SessionRunArgs(self._global_step_tensor)
-
-  def after_run(self, run_context, run_values):
-    _ = run_context
-
-    global_step = run_values.results
-    if self._timer.should_trigger_for_step(global_step):
-      elapsed_time, elapsed_steps = self._timer.update_last_triggered_step(
-          global_step)
-      if elapsed_time is not None:
-        steps_per_sec = elapsed_steps / elapsed_time
-        self._step_train_time += elapsed_time
-        self._total_steps += elapsed_steps
-
-        average_examples_per_sec = self._batch_size * (
-            self._total_steps / self._step_train_time)
-        current_examples_per_sec = steps_per_sec * self._batch_size
-        # Average examples/sec followed by current examples/sec
-        logging.info('%s: %g (%g), step = %g', 'Average examples/sec',
-                     average_examples_per_sec, current_examples_per_sec,
-                     self._total_steps)
-
-def local_device_setter(num_devices=1,
-                        ps_device_type='cpu',
-                        worker_device='/cpu:0',
-                        ps_ops=None,
-                        ps_strategy=None):
-  if ps_ops == None:
-    ps_ops = ['Variable', 'VariableV2', 'VarHandleOp']
-
-  if ps_strategy is None:
-    ps_strategy = device_setter._RoundRobinStrategy(num_devices)
-  if not six.callable(ps_strategy):
-    raise TypeError("ps_strategy must be callable")
-
-  def _local_device_chooser(op):
-    current_device = pydev.DeviceSpec.from_string(op.device or "")
-
-    node_def = op if isinstance(op, node_def_pb2.NodeDef) else op.node_def
-    if node_def.op in ps_ops:
-      ps_device_spec = pydev.DeviceSpec.from_string(
-          '/{}:{}'.format(ps_device_type, ps_strategy(op)))
-
-      ps_device_spec.merge_from(current_device)
-      return ps_device_spec.to_string()
-    else:
-      worker_device_spec = pydev.DeviceSpec.from_string(worker_device or "")
-      worker_device_spec.merge_from(current_device)
-      return worker_device_spec.to_string()
-  return _local_device_chooser
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/generate_cifar10_tfrecords.py b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/generate_cifar10_tfrecords.py
deleted file mode 100644
index d1a599c31bfb..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/generate_cifar10_tfrecords.py
+++ /dev/null
@@ -1,118 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""Read CIFAR-10 data from pickled numpy arrays and writes TFRecords.
-
-Generates tf.train.Example protos and writes them to TFRecord files from the
-python version of the CIFAR-10 dataset downloaded from
-https://www.cs.toronto.edu/~kriz/cifar.html.
-"""
-
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import argparse
-import os
-import sys
-
-import tarfile
-from six.moves import cPickle as pickle
-from six.moves import xrange  # pylint: disable=redefined-builtin
-import tensorflow as tf
-
-CIFAR_FILENAME = 'cifar-10-python.tar.gz'
-CIFAR_DOWNLOAD_URL = 'https://www.cs.toronto.edu/~kriz/' + CIFAR_FILENAME
-CIFAR_LOCAL_FOLDER = 'cifar-10-batches-py'
-
-
-def download_and_extract(data_dir):
-  # download CIFAR-10 if not already downloaded.
-  tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir,
-                                                CIFAR_DOWNLOAD_URL)
-  tarfile.open(os.path.join(data_dir, CIFAR_FILENAME),
-               'r:gz').extractall(data_dir)
-
-
-def _int64_feature(value):
-  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))
-
-
-def _bytes_feature(value):
-  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
-
-
-def _get_file_names():
-  """Returns the file names expected to exist in the input_dir."""
-  file_names = {}
-  file_names['train'] = ['data_batch_%d' % i for i in xrange(1, 5)]
-  file_names['validation'] = ['data_batch_5']
-  file_names['eval'] = ['test_batch']
-  return file_names
-
-
-def read_pickle_from_file(filename):
-  with tf.gfile.Open(filename, 'rb') as f:
-    if sys.version_info >= (3, 0):
-      data_dict = pickle.load(f, encoding='bytes')
-    else:
-      data_dict = pickle.load(f)
-  return data_dict
-
-
-def convert_to_tfrecord(input_files, output_file):
-  """Converts a file to TFRecords."""
-  print('Generating %s' % output_file)
-  with tf.python_io.TFRecordWriter(output_file) as record_writer:
-    for input_file in input_files:
-      data_dict = read_pickle_from_file(input_file)
-      data = data_dict[b'data']
-      labels = data_dict[b'labels']
-      num_entries_in_batch = len(labels)
-      for i in range(num_entries_in_batch):
-        example = tf.train.Example(features=tf.train.Features(
-            feature={
-                'image': _bytes_feature(data[i].tobytes()),
-                'label': _int64_feature(labels[i])
-            }))
-        record_writer.write(example.SerializeToString())
-
-
-def main(data_dir):
-  print('Download from {} and extract.'.format(CIFAR_DOWNLOAD_URL))
-  download_and_extract(data_dir)
-  file_names = _get_file_names()
-  input_dir = os.path.join(data_dir, CIFAR_LOCAL_FOLDER)
-  for mode, files in file_names.items():
-    input_files = [os.path.join(input_dir, f) for f in files]
-    output_file = os.path.join(data_dir, mode + '.tfrecords')
-    try:
-      os.remove(output_file)
-    except OSError:
-      pass
-    # Convert to tf.train.Example and write the to TFRecords.
-    convert_to_tfrecord(input_files, output_file)
-  print('Done!')
-
-
-if __name__ == '__main__':
-  parser = argparse.ArgumentParser()
-  parser.add_argument(
-      '--data-dir',
-      type=str,
-      default='',
-      help='Directory to download and extract CIFAR-10 to.')
-
-  args = parser.parse_args()
-  main(args.data_dir)
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/model_base.py b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/model_base.py
deleted file mode 100644
index 35e52b8355d0..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/with-cifar10-models/ubuntu-16.04/cifar10_estimator_tf_1.13.1/model_base.py
+++ /dev/null
@@ -1,219 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""ResNet model.
-
-Related papers:
-https://arxiv.org/pdf/1603.05027v2.pdf
-https://arxiv.org/pdf/1512.03385v1.pdf
-https://arxiv.org/pdf/1605.07146v1.pdf
-"""
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import tensorflow as tf
-
-
-class ResNet(object):
-  """ResNet model."""
-
-  def __init__(self, is_training, data_format, batch_norm_decay, batch_norm_epsilon):
-    """ResNet constructor.
-
-    Args:
-      is_training: if build training or inference model.
-      data_format: the data_format used during computation.
-                   one of 'channels_first' or 'channels_last'.
-    """
-    self._batch_norm_decay = batch_norm_decay
-    self._batch_norm_epsilon = batch_norm_epsilon
-    self._is_training = is_training
-    assert data_format in ('channels_first', 'channels_last')
-    self._data_format = data_format
-
-  def forward_pass(self, x):
-    raise NotImplementedError(
-        'forward_pass() is implemented in ResNet sub classes')
-
-  def _residual_v1(self,
-                   x,
-                   kernel_size,
-                   in_filter,
-                   out_filter,
-                   stride,
-                   activate_before_residual=False):
-    """Residual unit with 2 sub layers, using Plan A for shortcut connection."""
-
-    del activate_before_residual
-    with tf.name_scope('residual_v1') as name_scope:
-      orig_x = x
-
-      x = self._conv(x, kernel_size, out_filter, stride)
-      x = self._batch_norm(x)
-      x = self._relu(x)
-
-      x = self._conv(x, kernel_size, out_filter, 1)
-      x = self._batch_norm(x)
-
-      if in_filter != out_filter:
-        orig_x = self._avg_pool(orig_x, stride, stride)
-        pad = (out_filter - in_filter) // 2
-        if self._data_format == 'channels_first':
-          orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])
-        else:
-          orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])
-
-      x = self._relu(tf.add(x, orig_x))
-
-      tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())
-      return x
-
-  def _residual_v2(self,
-                   x,
-                   in_filter,
-                   out_filter,
-                   stride,
-                   activate_before_residual=False):
-    """Residual unit with 2 sub layers with preactivation, plan A shortcut."""
-
-    with tf.name_scope('residual_v2') as name_scope:
-      if activate_before_residual:
-        x = self._batch_norm(x)
-        x = self._relu(x)
-        orig_x = x
-      else:
-        orig_x = x
-        x = self._batch_norm(x)
-        x = self._relu(x)
-
-      x = self._conv(x, 3, out_filter, stride)
-
-      x = self._batch_norm(x)
-      x = self._relu(x)
-      x = self._conv(x, 3, out_filter, [1, 1, 1, 1])
-
-      if in_filter != out_filter:
-        pad = (out_filter - in_filter) // 2
-        orig_x = self._avg_pool(orig_x, stride, stride)
-        if self._data_format == 'channels_first':
-          orig_x = tf.pad(orig_x, [[0, 0], [pad, pad], [0, 0], [0, 0]])
-        else:
-          orig_x = tf.pad(orig_x, [[0, 0], [0, 0], [0, 0], [pad, pad]])
-
-      x = tf.add(x, orig_x)
-
-      tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())
-      return x
-
-  def _bottleneck_residual_v2(self,
-                              x,
-                              in_filter,
-                              out_filter,
-                              stride,
-                              activate_before_residual=False):
-    """Bottleneck residual unit with 3 sub layers, plan B shortcut."""
-
-    with tf.name_scope('bottle_residual_v2') as name_scope:
-      if activate_before_residual:
-        x = self._batch_norm(x)
-        x = self._relu(x)
-        orig_x = x
-      else:
-        orig_x = x
-        x = self._batch_norm(x)
-        x = self._relu(x)
-
-      x = self._conv(x, 1, out_filter // 4, stride, is_atrous=True)
-
-      x = self._batch_norm(x)
-      x = self._relu(x)
-      # pad when stride isn't unit
-      x = self._conv(x, 3, out_filter // 4, 1, is_atrous=True)
-
-      x = self._batch_norm(x)
-      x = self._relu(x)
-      x = self._conv(x, 1, out_filter, 1, is_atrous=True)
-
-      if in_filter != out_filter:
-        orig_x = self._conv(orig_x, 1, out_filter, stride, is_atrous=True)
-      x = tf.add(x, orig_x)
-
-      tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())
-      return x
-
-  def _conv(self, x, kernel_size, filters, strides, is_atrous=False):
-    """Convolution."""
-
-    padding = 'SAME'
-    if not is_atrous and strides > 1:
-      pad = kernel_size - 1
-      pad_beg = pad // 2
-      pad_end = pad - pad_beg
-      if self._data_format == 'channels_first':
-        x = tf.pad(x, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])
-      else:
-        x = tf.pad(x, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])
-      padding = 'VALID'
-    return tf.layers.conv2d(
-        inputs=x,
-        kernel_size=kernel_size,
-        filters=filters,
-        strides=strides,
-        padding=padding,
-        use_bias=False,
-        data_format=self._data_format)
-
-  def _batch_norm(self, x):
-    if self._data_format == 'channels_first':
-      data_format = 'NCHW'
-    else:
-      data_format = 'NHWC'
-    return tf.contrib.layers.batch_norm(
-        x,
-        decay=self._batch_norm_decay,
-        center=True,
-        scale=True,
-        epsilon=self._batch_norm_epsilon,
-        is_training=self._is_training,
-        fused=True,
-        data_format=data_format)
-
-  def _relu(self, x):
-    return tf.nn.relu(x)
-
-  def _fully_connected(self, x, out_dim):
-    with tf.name_scope('fully_connected') as name_scope:
-      x = tf.layers.dense(x, out_dim)
-
-    tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())
-    return x
-
-  def _avg_pool(self, x, pool_size, stride):
-    with tf.name_scope('avg_pool') as name_scope:
-      x = tf.layers.average_pooling2d(
-          x, pool_size, stride, 'SAME', data_format=self._data_format)
-
-    tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())
-    return x
-
-  def _global_avg_pool(self, x):
-    with tf.name_scope('global_avg_pool') as name_scope:
-      assert x.get_shape().ndims == 4
-      if self._data_format == 'channels_first':
-        x = tf.reduce_mean(x, [2, 3])
-      else:
-        x = tf.reduce_mean(x, [1, 2])
-    tf.logging.info('image after unit %s: %s', name_scope, x.get_shape())
-    return x
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/Dockerfile.gpu b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/Dockerfile.gpu
deleted file mode 100644
index 05d5fe733c1d..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/Dockerfile.gpu
+++ /dev/null
@@ -1,75 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-FROM nvidia/cuda:9.0-base-ubuntu16.04
-
-RUN  echo "$LOG_TAG update and install basic packages" && \
-     apt-get -y update && apt-get install -y --no-install-recommends \
-        build-essential \
-        curl \
-        libfreetype6-dev \
-        libpng12-dev \
-        libzmq3-dev \
-        pkg-config \
-        rsync \
-        software-properties-common \
-        unzip \
-        vim \
-        wget \
-        && \
-    apt-get install -y locales && \
-    locale-gen $LANG && \
-    apt-get clean && \
-    apt -y autoclean && \
-    apt -y dist-upgrade && \
-    apt-get install -y build-essential && \
-    rm -rf /var/lib/apt/lists/*
-
-ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
-RUN echo "$LOG_TAG Install java8" && \
-    apt-get -y update && \
-    apt-get install -y openjdk-8-jdk && \
-    rm -rf /var/lib/apt/lists/*
-
-# Install Zeppelin
-ENV Z_VERSION="0.7.3" \
-    Z_HOME="/zeppelin"
-
-RUN echo "$LOG_TAG Download Zeppelin binary" && \
-    wget -O /tmp/zeppelin-${Z_VERSION}-bin-all.tgz http://archive.apache.org/dist/zeppelin/zeppelin-${Z_VERSION}/zeppelin-${Z_VERSION}-bin-all.tgz && \
-    tar -zxvf /tmp/zeppelin-${Z_VERSION}-bin-all.tgz && \
-    rm -rf /tmp/zeppelin-${Z_VERSION}-bin-all.tgz && \
-    mv /zeppelin-${Z_VERSION}-bin-all ${Z_HOME}
-ENV PATH="${Z_HOME}/bin:${PATH}"
-
-RUN echo "$LOG_TAG Set locale" && \
-    echo "LC_ALL=en_US.UTF-8" >> /etc/environment && \
-    echo "en_US.UTF-8 UTF-8" >> /etc/locale.gen && \
-    echo "LANG=en_US.UTF-8" > /etc/locale.conf && \
-    locale-gen en_US.UTF-8
-
-ENV LANG=en_US.UTF-8 \
-    LC_ALL=en_US.UTF-8
-
-COPY zeppelin-site.xml $Z_HOME/conf/zeppelin-site.xml
-COPY shiro.ini ${Z_HOME}/conf/shiro.ini
-RUN chmod 777 -R ${Z_HOME}
-
-COPY run_container.sh /usr/local/bin/run_container.sh
-RUN chmod 755 /usr/local/bin/run_container.sh
-
-EXPOSE 8080
-CMD ["/usr/local/bin/run_container.sh"]
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/run_container.sh b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/run_container.sh
deleted file mode 100644
index 8b909209915a..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/run_container.sh
+++ /dev/null
@@ -1,22 +0,0 @@
-#!/usr/bin/env bash
-
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"${Z_HOME}/bin/zeppelin-daemon.sh" start
-while true; do
-    #perform the test
-    sleep 5
-done
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/shiro.ini b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/shiro.ini
deleted file mode 100644
index 89f976af128c..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/shiro.ini
+++ /dev/null
@@ -1,120 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-[users]
-# List of users with their password allowed to access Zeppelin.
-# To use a different strategy (LDAP / Database / ...) check the shiro doc at http://shiro.apache.org/configuration.html#Configuration-INISections
-# To enable admin user, uncomment the following line and set an appropriate password.
-admin = admin, admin
-user1 = password2, role1, role2
-user2 = password3, role3
-user3 = password4, role2
-
-# Sample LDAP configuration, for user Authentication, currently tested for single Realm
-[main]
-### A sample for configuring Active Directory Realm
-#activeDirectoryRealm = org.apache.zeppelin.realm.ActiveDirectoryGroupRealm
-#activeDirectoryRealm.systemUsername = userNameA
-
-#use either systemPassword or hadoopSecurityCredentialPath, more details in http://zeppelin.apache.org/docs/latest/security/shiroauthentication.html
-#activeDirectoryRealm.systemPassword = passwordA
-#activeDirectoryRealm.hadoopSecurityCredentialPath = jceks://file/user/zeppelin/zeppelin.jceks
-#activeDirectoryRealm.searchBase = CN=Users,DC=SOME_GROUP,DC=COMPANY,DC=COM
-#activeDirectoryRealm.url = ldap://ldap.test.com:389
-#activeDirectoryRealm.groupRolesMap = "CN=admin,OU=groups,DC=SOME_GROUP,DC=COMPANY,DC=COM":"admin","CN=finance,OU=groups,DC=SOME_GROUP,DC=COMPANY,DC=COM":"finance","CN=hr,OU=groups,DC=SOME_GROUP,DC=COMPANY,DC=COM":"hr"
-#activeDirectoryRealm.authorizationCachingEnabled = false
-
-### A sample for configuring LDAP Directory Realm
-#ldapRealm = org.apache.zeppelin.realm.LdapGroupRealm
-## search base for ldap groups (only relevant for LdapGroupRealm):
-#ldapRealm.contextFactory.environment[ldap.searchBase] = dc=COMPANY,dc=COM
-#ldapRealm.contextFactory.url = ldap://ldap.test.com:389
-#ldapRealm.userDnTemplate = uid={0},ou=Users,dc=COMPANY,dc=COM
-#ldapRealm.contextFactory.authenticationMechanism = simple
-
-### A sample PAM configuration
-#pamRealm=org.apache.zeppelin.realm.PamRealm
-#pamRealm.service=sshd
-
-### A sample for configuring ZeppelinHub Realm
-#zeppelinHubRealm = org.apache.zeppelin.realm.ZeppelinHubRealm
-## Url of ZeppelinHub
-#zeppelinHubRealm.zeppelinhubUrl = https://www.zeppelinhub.com
-#securityManager.realms = $zeppelinHubRealm
-
-## A same for configuring Knox SSO Realm
-#knoxJwtRealm = org.apache.zeppelin.realm.jwt.KnoxJwtRealm
-#knoxJwtRealm.providerUrl = https://domain.example.com/
-#knoxJwtRealm.login = gateway/knoxsso/knoxauth/login.html
-#knoxJwtRealm.logout = gateway/knoxssout/api/v1/webssout
-#knoxJwtRealm.logoutAPI = true
-#knoxJwtRealm.redirectParam = originalUrl
-#knoxJwtRealm.cookieName = hadoop-jwt
-#knoxJwtRealm.publicKeyPath = /etc/zeppelin/conf/knox-sso.pem
-#
-#knoxJwtRealm.groupPrincipalMapping = group.principal.mapping
-#knoxJwtRealm.principalMapping = principal.mapping
-#authc = org.apache.zeppelin.realm.jwt.KnoxAuthenticationFilter
-
-sessionManager = org.apache.shiro.web.session.mgt.DefaultWebSessionManager
-
-### If caching of user is required then uncomment below lines
-#cacheManager = org.apache.shiro.cache.MemoryConstrainedCacheManager
-#securityManager.cacheManager = $cacheManager
-
-### Enables 'HttpOnly' flag in Zeppelin cookies
-cookie = org.apache.shiro.web.servlet.SimpleCookie
-cookie.name = JSESSIONID
-cookie.httpOnly = true
-### Uncomment the below line only when Zeppelin is running over HTTPS
-#cookie.secure = true
-sessionManager.sessionIdCookie = $cookie
-
-securityManager.sessionManager = $sessionManager
-# 86,400,000 milliseconds = 24 hour
-securityManager.sessionManager.globalSessionTimeout = 86400000
-shiro.loginUrl = /api/login
-
-[roles]
-role1 = *
-role2 = *
-role3 = *
-admin = *
-
-[urls]
-# This section is used for url-based security. For details see the shiro.ini documentation.
-#
-# You can secure interpreter, configuration and credential information by urls.
-# Comment or uncomment the below urls that you want to hide:
-# anon means the access is anonymous.
-# authc means form based auth Security.
-#
-# IMPORTANT: Order matters: URL path expressions are evaluated against an incoming request
-# in the order they are defined and the FIRST MATCH WINS.
-#
-# To allow anonymous access to all but the stated urls,
-# uncomment the line second last line (/** = anon) and comment the last line (/** = authc)
-#
-/api/version = anon
-# Allow all authenticated users to restart interpreters on a notebook page.
-# Comment out the following line if you would like to authorize only admin users to restart interpreters.
-/api/interpreter/setting/restart/** = authc
-/api/interpreter/** = authc, roles[admin]
-/api/configurations/** = authc, roles[admin]
-/api/credential/** = authc, roles[admin]
-#/** = anon
-/** = authc
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/zeppelin-site.xml b/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/zeppelin-site.xml
deleted file mode 100644
index 2bde161bd6a7..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/docker/tensorflow/zeppelin-notebook-example/zeppelin-site.xml
+++ /dev/null
@@ -1,569 +0,0 @@
-<?xml version="1.0"?>
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<configuration>
-
-  <property>
-    <name>zeppelin.server.addr</name>
-    <value>0.0.0.0</value>
-    <description>Server address</description>
-  </property>
-
-  <property>
-    <name>zeppelin.server.port</name>
-    <value>8080</value>
-    <description>Server port.</description>
-  </property>
-
-  <property>
-    <name>zeppelin.server.ssl.port</name>
-    <value>8443</value>
-    <description>Server ssl port. (used when ssl property is set to true)</description>
-  </property>
-
-  <property>
-    <name>zeppelin.server.context.path</name>
-    <value>/</value>
-    <description>Context Path of the Web Application</description>
-  </property>
-
-  <property>
-    <name>zeppelin.war.tempdir</name>
-    <value>webapps</value>
-    <description>Location of jetty temporary directory</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.dir</name>
-    <value>notebook</value>
-    <description>path or URI for notebook persist</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.homescreen</name>
-    <value></value>
-    <description>id of notebook to be displayed in homescreen. ex) 2A94M5J1Z Empty value displays default home screen</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.homescreen.hide</name>
-    <value>false</value>
-    <description>hide homescreen notebook from list when this value set to true</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.collaborative.mode.enable</name>
-    <value>true</value>
-    <description>Enable collaborative mode</description>
-  </property>
-
-  <!-- Google Cloud Storage notebook storage -->
-  <!--
-  <property>
-    <name>zeppelin.notebook.gcs.dir</name>
-    <value></value>
-    <description>
-      A GCS path in the form gs://bucketname/path/to/dir.
-      Notes are stored at {zeppelin.notebook.gcs.dir}/{notebook-id}/note.json
-   </description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.storage</name>
-    <value>org.apache.zeppelin.notebook.repo.GCSNotebookRepo</value>
-    <description>notebook persistence layer implementation</description>
-  </property>
-  -->
-
-  <!-- Amazon S3 notebook storage -->
-  <!-- Creates the following directory structure: s3://{bucket}/{username}/{notebook-id}/note.json -->
-  <!--
-  <property>
-    <name>zeppelin.notebook.s3.user</name>
-    <value>user</value>
-    <description>user name for s3 folder structure</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.s3.bucket</name>
-    <value>zeppelin</value>
-    <description>bucket name for notebook storage</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.s3.endpoint</name>
-    <value>s3.amazonaws.com</value>
-    <description>endpoint for s3 bucket</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.storage</name>
-    <value>org.apache.zeppelin.notebook.repo.S3NotebookRepo</value>
-    <description>notebook persistence layer implementation</description>
-  </property>
-  -->
-
-  <!-- Additionally, encryption is supported for notebook data stored in S3 -->
-  <!-- Use the AWS KMS to encrypt data -->
-  <!-- If used, the EC2 role assigned to the EMR cluster must have rights to use the given key -->
-  <!-- See https://aws.amazon.com/kms/ and http://docs.aws.amazon.com/kms/latest/developerguide/concepts.html -->
-  <!--
-  <property>
-    <name>zeppelin.notebook.s3.kmsKeyID</name>
-    <value>AWS-KMS-Key-UUID</value>
-    <description>AWS KMS key ID used to encrypt notebook data in S3</description>
-  </property>
-  -->
-
-  <!-- provide region of your KMS key -->
-  <!-- See http://docs.aws.amazon.com/general/latest/gr/rande.html#kms_region for region codes names -->
-  <!--
-  <property>
-    <name>zeppelin.notebook.s3.kmsKeyRegion</name>
-    <value>us-east-1</value>
-    <description>AWS KMS key region in your AWS account</description>
-  </property>
-  -->
-
-  <!-- Use a custom encryption materials provider to encrypt data -->
-  <!-- No configuration is given to the provider, so you must use system properties or another means to configure -->
-  <!-- See https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/model/EncryptionMaterialsProvider.html -->
-  <!--
-  <property>
-    <name>zeppelin.notebook.s3.encryptionMaterialsProvider</name>
-    <value>provider implementation class name</value>
-    <description>Custom encryption materials provider used to encrypt notebook data in S3</description>
-  </property>
-  -->
-
-  <!-- Server-side encryption enabled for notebooks -->
-  <!--
-  <property>
-    <name>zeppelin.notebook.s3.sse</name>
-    <value>true</value>
-    <description>Server-side encryption enabled for notebooks</description>
-  </property>
-  -->
-
-  <!-- Optional override to control which signature algorithm should be used to sign AWS requests -->
-  <!-- Set this property to "S3SignerType" if your AWS S3 compatible APIs support only AWS Signature Version 2 such as Ceph. -->
-  <!--
-  <property>
-    <name>zeppelin.notebook.s3.signerOverride</name>
-    <value>S3SignerType</value>
-    <description>optional override to control which signature algorithm should be used to sign AWS requests</description>
-  </property>
-  -->
-
-  <!-- If using Azure for storage use the following settings -->
-  <!--
-  <property>
-    <name>zeppelin.notebook.azure.connectionString</name>
-    <value>DefaultEndpointsProtocol=https;AccountName=<accountName>;AccountKey=<accountKey></value>
-    <description>Azure account credentials</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.azure.share</name>
-    <value>zeppelin</value>
-    <description>share name for notebook storage</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.azure.user</name>
-    <value>user</value>
-    <description>optional user name for Azure folder structure</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.storage</name>
-    <value>org.apache.zeppelin.notebook.repo.AzureNotebookRepo</value>
-    <description>notebook persistence layer implementation</description>
-  </property>
-  -->
-
-  <!-- Notebook storage layer using local file system
-  <property>
-    <name>zeppelin.notebook.storage</name>
-    <value>org.apache.zeppelin.notebook.repo.VFSNotebookRepo</value>
-    <description>local notebook persistence layer implementation</description>
-  </property>
-  -->
-
-  <!-- Notebook storage layer using hadoop compatible file system
-  <property>
-    <name>zeppelin.notebook.storage</name>
-    <value>org.apache.zeppelin.notebook.repo.FileSystemNotebookRepo</value>
-    <description>Hadoop compatible file system notebook persistence layer implementation, such as local file system, hdfs, azure wasb, s3 and etc.</description>
-  </property>
-
-  <property>
-    <name>zeppelin.server.kerberos.keytab</name>
-    <value></value>
-    <description>keytab for accessing kerberized hdfs</description>
-  </property>
-
-  <property>
-    <name>zeppelin.server.kerberos.principal</name>
-    <value></value>
-    <description>principal for accessing kerberized hdfs</description>
-  </property>
-  -->
-
-  <!-- For connecting your Zeppelin with ZeppelinHub -->
-  <!--
-  <property>
-    <name>zeppelin.notebook.storage</name>
-    <value>org.apache.zeppelin.notebook.repo.GitNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo</value>
-    <description>two notebook persistence layers (versioned local + ZeppelinHub)</description>
-  </property>
-  -->
-
-  <!-- MongoDB notebook storage -->
-  <!--
-  <property>
-    <name>zeppelin.notebook.storage</name>
-    <value>org.apache.zeppelin.notebook.repo.MongoNotebookRepo</value>
-    <description>notebook persistence layer implementation</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.mongo.uri</name>
-    <value>mongodb://localhost</value>
-    <description>MongoDB connection URI used to connect to a MongoDB database server</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.mongo.database</name>
-    <value>zeppelin</value>
-    <description>database name for notebook storage</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.mongo.collection</name>
-    <value>notes</value>
-    <description>collection name for notebook storage</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.mongo.autoimport</name>
-    <value>false</value>
-    <description>import local notes into MongoDB automatically on startup</description>
-  </property>
-  -->
-
-  <property>
-    <name>zeppelin.notebook.storage</name>
-    <value>org.apache.zeppelin.notebook.repo.GitNotebookRepo</value>
-    <description>versioned notebook persistence layer implementation</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.one.way.sync</name>
-    <value>false</value>
-    <description>If there are multiple notebook storages, should we treat the first one as the only source of truth?</description>
-  </property>
-
-  <property>
-    <name>zeppelin.interpreter.dir</name>
-    <value>interpreter</value>
-    <description>Interpreter implementation base directory</description>
-  </property>
-
-  <property>
-    <name>zeppelin.interpreter.localRepo</name>
-    <value>local-repo</value>
-    <description>Local repository for interpreter's additional dependency loading</description>
-  </property>
-
-  <property>
-    <name>zeppelin.interpreter.dep.mvnRepo</name>
-    <value>http://repo1.maven.org/maven2/</value>
-    <description>Remote principal repository for interpreter's additional dependency loading</description>
-  </property>
-
-  <property>
-    <name>zeppelin.dep.localrepo</name>
-    <value>local-repo</value>
-    <description>Local repository for dependency loader</description>
-  </property>
-
-  <property>
-    <name>zeppelin.helium.node.installer.url</name>
-    <value>https://nodejs.org/dist/</value>
-    <description>Remote Node installer url for Helium dependency loader</description>
-  </property>
-
-  <property>
-    <name>zeppelin.helium.npm.installer.url</name>
-    <value>http://registry.npmjs.org/</value>
-    <description>Remote Npm installer url for Helium dependency loader</description>
-  </property>
-
-  <property>
-    <name>zeppelin.helium.yarnpkg.installer.url</name>
-    <value>https://github.com/yarnpkg/yarn/releases/download/</value>
-    <description>Remote Yarn package installer url for Helium dependency loader</description>
-  </property>
-
-  <property>
-    <name>zeppelin.interpreters</name>
-    <value>org.apache.zeppelin.spark.SparkInterpreter,org.apache.zeppelin.spark.PySparkInterpreter,org.apache.zeppelin.rinterpreter.RRepl,org.apache.zeppelin.rinterpreter.KnitR,org.apache.zeppelin.spark.SparkRInterpreter,org.apache.zeppelin.spark.SparkSqlInterpreter,org.apache.zeppelin.spark.DepInterpreter,org.apache.zeppelin.markdown.Markdown,org.apache.zeppelin.angular.AngularInterpreter,org.apache.zeppelin.shell.ShellInterpreter,org.apache.zeppelin.file.HDFSFileInterpreter,org.apache.zeppelin.flink.FlinkInterpreter,,org.apache.zeppelin.python.PythonInterpreter,org.apache.zeppelin.python.PythonInterpreterPandasSql,org.apache.zeppelin.python.PythonCondaInterpreter,org.apache.zeppelin.python.PythonDockerInterpreter,org.apache.zeppelin.lens.LensInterpreter,org.apache.zeppelin.ignite.IgniteInterpreter,org.apache.zeppelin.ignite.IgniteSqlInterpreter,org.apache.zeppelin.cassandra.CassandraInterpreter,org.apache.zeppelin.geode.GeodeOqlInterpreter,org.apache.zeppelin.jdbc.JDBCInterpreter,org.apache.zeppelin.kylin.KylinInterpreter,org.apache.zeppelin.elasticsearch.ElasticsearchInterpreter,org.apache.zeppelin.scalding.ScaldingInterpreter,org.apache.zeppelin.alluxio.AlluxioInterpreter,org.apache.zeppelin.hbase.HbaseInterpreter,org.apache.zeppelin.livy.LivySparkInterpreter,org.apache.zeppelin.livy.LivyPySparkInterpreter,org.apache.zeppelin.livy.LivyPySpark3Interpreter,org.apache.zeppelin.livy.LivySparkRInterpreter,org.apache.zeppelin.livy.LivySparkSQLInterpreter,org.apache.zeppelin.bigquery.BigQueryInterpreter,org.apache.zeppelin.beam.BeamInterpreter,org.apache.zeppelin.pig.PigInterpreter,org.apache.zeppelin.pig.PigQueryInterpreter,org.apache.zeppelin.scio.ScioInterpreter,org.apache.zeppelin.groovy.GroovyInterpreter</value>
-    <description>Comma separated interpreter configurations. First interpreter become a default</description>
-  </property>
-
-  <property>
-    <name>zeppelin.interpreter.group.order</name>
-    <value>spark,md,angular,sh,livy,alluxio,file,psql,flink,python,ignite,lens,cassandra,geode,kylin,elasticsearch,scalding,jdbc,hbase,bigquery,beam,groovy</value>
-    <description></description>
-  </property>
-
-  <property>
-    <name>zeppelin.interpreter.connect.timeout</name>
-    <value>30000</value>
-    <description>Interpreter process connect timeout in msec.</description>
-  </property>
-
-  <property>
-    <name>zeppelin.interpreter.output.limit</name>
-    <value>102400</value>
-    <description>Output message from interpreter exceeding the limit will be truncated</description>
-  </property>
-
-  <property>
-    <name>zeppelin.ssl</name>
-    <value>false</value>
-    <description>Should SSL be used by the servers?</description>
-  </property>
-
-  <property>
-    <name>zeppelin.ssl.client.auth</name>
-    <value>false</value>
-    <description>Should client authentication be used for SSL connections?</description>
-  </property>
-
-  <property>
-    <name>zeppelin.ssl.keystore.path</name>
-    <value>keystore</value>
-    <description>Path to keystore relative to Zeppelin configuration directory</description>
-  </property>
-
-  <property>
-    <name>zeppelin.ssl.keystore.type</name>
-    <value>JKS</value>
-    <description>The format of the given keystore (e.g. JKS or PKCS12)</description>
-  </property>
-
-  <property>
-    <name>zeppelin.ssl.keystore.password</name>
-    <value>change me</value>
-    <description>Keystore password. Can be obfuscated by the Jetty Password tool</description>
-  </property>
-
-  <!--
-  <property>
-    <name>zeppelin.ssl.key.manager.password</name>
-    <value>change me</value>
-    <description>Key Manager password. Defaults to keystore password. Can be obfuscated.</description>
-  </property>
-  -->
-
-  <property>
-    <name>zeppelin.ssl.truststore.path</name>
-    <value>truststore</value>
-    <description>Path to truststore relative to Zeppelin configuration directory. Defaults to the keystore path</description>
-  </property>
-
-  <property>
-    <name>zeppelin.ssl.truststore.type</name>
-    <value>JKS</value>
-    <description>The format of the given truststore (e.g. JKS or PKCS12). Defaults to the same type as the keystore type</description>
-  </property>
-
-  <!--
-  <property>
-    <name>zeppelin.ssl.truststore.password</name>
-    <value>change me</value>
-    <description>Truststore password. Can be obfuscated by the Jetty Password tool. Defaults to the keystore password</description>
-  </property>
-  -->
-
-  <property>
-    <name>zeppelin.server.allowed.origins</name>
-    <value>*</value>
-    <description>Allowed sources for REST and WebSocket requests (i.e. http://onehost:8080,http://otherhost.com). If you leave * you are vulnerable to https://issues.apache.org/jira/browse/ZEPPELIN-173</description>
-  </property>
-
-  <property>
-    <name>zeppelin.anonymous.allowed</name>
-    <value>false</value>
-    <description>Anonymous user allowed by default</description>
-  </property>
-
-  <property>
-    <name>zeppelin.username.force.lowercase</name>
-    <value>false</value>
-    <description>Force convert username case to lower case, useful for Active Directory/LDAP. Default is not to change case</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.default.owner.username</name>
-    <value></value>
-    <description>Set owner role by default</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.public</name>
-    <value>true</value>
-    <description>Make notebook public by default when created, private otherwise</description>
-  </property>
-
-  <property>
-    <name>zeppelin.websocket.max.text.message.size</name>
-    <value>1024000</value>
-    <description>Size in characters of the maximum text message to be received by websocket. Defaults to 1024000</description>
-  </property>
-
-  <property>
-    <name>zeppelin.server.default.dir.allowed</name>
-    <value>false</value>
-    <description>Enable directory listings on server.</description>
-  </property>
-
-  <!--
-  <property>
-    <name>zeppelin.interpreter.lifecyclemanager.class</name>
-    <value>org.apache.zeppelin.interpreter.lifecycle.TimeoutLifecycleManager</value>
-    <description>LifecycleManager class for managing the lifecycle of interpreters, by default interpreter will
-    be closed after timeout</description>
-  </property>
-
-  <property>
-    <name>zeppelin.interpreter.lifecyclemanager.timeout.checkinterval</name>
-    <value>60000</value>
-    <description>Milliseconds of the interval to checking whether interpreter is time out</description>
-  </property>
-
-  <property>
-    <name>zeppelin.interpreter.lifecyclemanager.timeout.threshold</name>
-    <value>3600000</value>
-    <description>Milliseconds of the interpreter timeout threshold, by default it is 1 hour</description>
-  </property>
-  -->
-
-  <!--
-  <property>
-      <name>zeppelin.server.jetty.name</name>
-      <value>Jetty(7.6.0.v20120127)</value>
-      <description>Hardcoding Application Server name to Prevent Fingerprinting</description>
-  </property>
-  -->
-
-  <!--
-  <property>
-      <name>zeppelin.server.jetty.request.header.size</name>
-      <value>8192</value>
-      <description>Http Request Header Size Limit (to prevent HTTP 413)</description>
-  </property>
-  -->
-
-  <!--
-  <property>
-    <name>zeppelin.server.xframe.options</name>
-    <value>SAMEORIGIN</value>
-    <description>The X-Frame-Options HTTP response header can be used to indicate whether or not a browser should be allowed to render a page in a frame/iframe/object.</description>
-  </property>
-  -->
-
-  <!--
-  <property>
-    <name>zeppelin.server.strict.transport</name>
-    <value>max-age=631138519</value>
-    <description>The HTTP Strict-Transport-Security response header is a security feature that lets a web site tell browsers that it should only be communicated with using HTTPS, instead of using HTTP. Enable this when Zeppelin is running on HTTPS. Value is in Seconds, the default value is equivalent to 20 years.</description>
-  </property>
-  -->
-  <!--
-
-  <property>
-    <name>zeppelin.server.xxss.protection</name>
-    <value>1</value>
-    <description>The HTTP X-XSS-Protection response header is a feature of Internet Explorer, Chrome and Safari that stops pages from loading when they detect reflected cross-site scripting (XSS) attacks. When value is set to 1 and a cross-site scripting attack is detected, the browser will sanitize the page (remove the unsafe parts).</description>
-  </property>
-  -->
-
-  <!--
-  <property>
-    <name>zeppelin.interpreter.callback.portRange</name>
-    <value>10000:10010</value>
-  </property>
-  -->
-
-  <!--
-  <property>
-    <name>zeppelin.recovery.storage.class</name>
-    <value>org.apache.zeppelin.interpreter.recovery.FileSystemRecoveryStorage</value>
-    <description>ReoveryStorage implementation</description>
-  </property>
-  -->
-
-  <!--
-  <property>
-    <name>zeppelin.recovery.dir</name>
-    <value>recovery</value>
-    <description>Location where recovery metadata is stored</description>
-  </property>
-  -->
-
-  <!-- GitHub configurations
-  <property>
-    <name>zeppelin.notebook.git.remote.url</name>
-    <value></value>
-    <description>remote Git repository URL</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.git.remote.username</name>
-    <value>token</value>
-    <description>remote Git repository username</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.git.remote.access-token</name>
-    <value></value>
-    <description>remote Git repository password</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.git.remote.origin</name>
-    <value>origin</value>
-    <description>Git repository remote</description>
-  </property>
-
-  <property>
-    <name>zeppelin.notebook.cron.enable</name>
-    <value>false</value>
-    <description>Notebook enable cron scheduler feature</description>
-  </property>
-  <property>
-    <name>zeppelin.notebook.cron.folders</name>
-    <value></value>
-    <description>Notebook cron folders</description>
-  </property>
-  -->
-</configuration>
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/AbstractCli.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/AbstractCli.java
deleted file mode 100644
index f6a92147c168..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/AbstractCli.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli;
-
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineException;
-
-import java.io.IOException;
-
-public abstract class AbstractCli implements Tool {
-  protected ClientContext clientContext;
-
-  public AbstractCli(ClientContext cliContext) {
-    this.clientContext = cliContext;
-  }
-
-  @Override
-  public abstract int run(String[] args)
-      throws ParseException, IOException, YarnException, InterruptedException,
-      SubmarineException;
-
-  @Override
-  public void setConf(Configuration conf) {
-    clientContext.setSubmarineConfig(conf);
-  }
-
-  @Override
-  public Configuration getConf() {
-    return clientContext.getSubmarineConfig();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/Cli.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/Cli.java
deleted file mode 100644
index b1fd90f7c1b2..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/Cli.java
+++ /dev/null
@@ -1,106 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli;
-
-import java.util.Arrays;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.RunJobCli;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.runtimes.RuntimeFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class Cli {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(Cli.class);
-
-  private static void printHelp() {
-    StringBuilder helpMsg = new StringBuilder();
-    helpMsg.append("\n\nUsage: <object> [<action>] [<args>]\n");
-    helpMsg.append("  Below are all objects / actions:\n");
-    helpMsg.append("    job \n");
-    helpMsg.append("       run : run a job, please see 'job run --help' for usage \n");
-    helpMsg.append("       show : get status of job, please see 'job show --help' for usage \n");
-    helpMsg.append("       kill : kill a job, please see 'job kill --help' for usage \n");
-    System.out.println(helpMsg.toString());
-  }
-
-  private static ClientContext getClientContext() {
-    Configuration conf = new YarnConfiguration();
-    ClientContext clientContext = new ClientContext();
-    clientContext.setConfiguration(conf);
-    RuntimeFactory runtimeFactory = RuntimeFactory.getRuntimeFactory(
-        clientContext);
-    clientContext.setRuntimeFactory(runtimeFactory);
-    return clientContext;
-  }
-
-  public static void main(String[] args) throws Exception {
-    System.out.println("              _                              _              \n"
-        + "             | |                            (_)             \n"
-        + "  ___  _   _ | |__   _ __ ___    __ _  _ __  _  _ __    ___ \n"
-        + " / __|| | | || '_ \\ | '_ ` _ \\  / _` || '__|| || '_ \\  / _ \\\n"
-        + " \\__ \\| |_| || |_) || | | | | || (_| || |   | || | | ||  __/\n"
-        + " |___/ \\__,_||_.__/ |_| |_| |_| \\__,_||_|   |_||_| |_| \\___|\n"
-        + "                                                    \n"
-        + "                             ?\n"
-        + " ~~~~~~~~~~~~~~~~~~~~~~~~~~~|^\"~~~~~~~~~~~~~~~~~~~~~~~~~o~~~~~~~~~~~\n"
-        + "        o                   |                  o      __o\n"
-        + "         o                  |                 o     |X__>\n"
-        + "       ___o                 |                __o\n"
-        + "     (X___>--             __|__            |X__>     o\n"
-        + "                         |     \\                   __o\n"
-        + "                         |      \\                |X__>\n"
-        + "  _______________________|_______\\________________\n"
-        + " <                                                \\____________   _\n"
-        + "  \\                                                            \\ (_)\n"
-        + "   \\    O       O       O                                       >=)\n"
-        + "    \\__________________________________________________________/ (_)\n"
-        + "\n");
-
-    if (CliUtils.argsForHelp(args)) {
-      printHelp();
-      System.exit(0);
-    }
-
-    if (args.length < 2) {
-      LOG.error("Bad parameters specified.");
-      printHelp();
-      System.exit(-1);
-    }
-
-    String[] moduleArgs = Arrays.copyOfRange(args, 2, args.length);
-    ClientContext clientContext = getClientContext();
-
-    if (args[0].equals("job")) {
-      String subCmd = args[1];
-      if (subCmd.equals(CliConstants.RUN)) {
-        new RunJobCli(clientContext).run(moduleArgs);
-      } else if (subCmd.equals(CliConstants.SHOW)) {
-        new ShowJobCli(clientContext).run(moduleArgs);
-      } else if (subCmd.equals(CliConstants.KILL)) {
-        new KillJobCli(clientContext).run(moduleArgs);
-      } else {
-        printHelp();
-        throw new IllegalArgumentException("Unknown option for job");
-      }
-    } else {
-      printHelp();
-      throw new IllegalArgumentException("Unrecognized option: " + args[0]);
-    }
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/CliConstants.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/CliConstants.java
deleted file mode 100644
index 1f879342a833..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/CliConstants.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli;
-
-/*
- * NOTE: use lowercase + "_" for the option name
- */
-public class CliConstants {
-  public static final String KILL = "kill";
-  public static final String RUN = "run";
-  public static final String SERVE = "serve";
-  public static final String LIST = "list";
-  public static final String SHOW = "show";
-  public static final String NAME = "name";
-  public static final String INPUT_PATH = "input_path";
-  public static final String CHECKPOINT_PATH = "checkpoint_path";
-  public static final String SAVED_MODEL_PATH = "saved_model_path";
-  public static final String N_WORKERS = "num_workers";
-  public static final String N_SERVING_TASKS = "num_serving_tasks";
-  public static final String N_PS = "num_ps";
-  public static final String WORKER_RES = "worker_resources";
-  public static final String SERVING_RES = "serving_resources";
-  public static final String PS_RES = "ps_resources";
-  public static final String DOCKER_IMAGE = "docker_image";
-  public static final String QUEUE = "queue";
-  public static final String TENSORBOARD = "tensorboard";
-  public static final String TENSORBOARD_RESOURCES = "tensorboard_resources";
-  public static final String TENSORBOARD_DEFAULT_RESOURCES =
-      "memory=4G,vcores=1";
-  public static final String ARG_CONF = "conf";
-
-  public static final String WORKER_LAUNCH_CMD = "worker_launch_cmd";
-  public static final String SERVING_LAUNCH_CMD = "serving_launch_cmd";
-  public static final String PS_LAUNCH_CMD = "ps_launch_cmd";
-  public static final String ENV = "env";
-  public static final String VERBOSE = "verbose";
-  public static final String SERVING_FRAMEWORK = "serving_framework";
-  public static final String STOP = "stop";
-  public static final String WAIT_JOB_FINISH = "wait_job_finish";
-  public static final String PS_DOCKER_IMAGE = "ps_docker_image";
-  public static final String WORKER_DOCKER_IMAGE = "worker_docker_image";
-  public static final String QUICKLINK = "quicklink";
-  public static final String TENSORBOARD_DOCKER_IMAGE =
-      "tensorboard_docker_image";
-  public static final String LOCALIZATION = "localization";
-  public static final String KEYTAB = "keytab";
-  public static final String PRINCIPAL = "principal";
-  public static final String DISTRIBUTE_KEYTAB = "distribute_keytab";
-  public static final String YAML_CONFIG = "f";
-  public static final String INSECURE_CLUSTER = "insecure";
-
-  public static final String FRAMEWORK = "framework";
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/CliUtils.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/CliUtils.java
deleted file mode 100644
index cdbe6a13d366..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/CliUtils.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli;
-
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineRuntimeException;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.apache.hadoop.yarn.submarine.client.cli.CliConstants.KEYTAB;
-import static org.apache.hadoop.yarn.submarine.client.cli.CliConstants.PRINCIPAL;
-
-public class CliUtils {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(CliUtils.class);
-  /**
-   * Replace patterns inside cli
-   *
-   * @return launch command after pattern replace
-   */
-  public static String replacePatternsInLaunchCommand(String specifiedCli,
-      RunJobParameters jobRunParameters,
-      RemoteDirectoryManager directoryManager) throws IOException {
-    String input = jobRunParameters.getInputPath();
-    String jobDir = jobRunParameters.getCheckpointPath();
-    String savedModelDir = jobRunParameters.getSavedModelPath();
-
-    Map<String, String> replacePattern = new HashMap<>();
-    if (jobDir != null) {
-      replacePattern.put("%" + CliConstants.CHECKPOINT_PATH + "%", jobDir);
-    }
-    if (input != null) {
-      replacePattern.put("%" + CliConstants.INPUT_PATH + "%", input);
-    }
-    if (savedModelDir != null) {
-      replacePattern.put("%" + CliConstants.SAVED_MODEL_PATH + "%",
-          savedModelDir);
-    }
-
-    String newCli = specifiedCli;
-    for (Map.Entry<String, String> replace : replacePattern.entrySet()) {
-      newCli = newCli.replace(replace.getKey(), replace.getValue());
-    }
-
-    return newCli;
-  }
-
-  // Is it for help?
-  public static boolean argsForHelp(String[] args) {
-    if (args == null || args.length == 0)
-      return true;
-
-    if (args.length == 1) {
-      return args[0].equals("-h") || args[0].equals("--help");
-    }
-
-    return false;
-  }
-
-  public static void doLoginIfSecure(String keytab, String principal) throws
-      IOException {
-    if (!UserGroupInformation.isSecurityEnabled()) {
-      return;
-    }
-
-    if (StringUtils.isEmpty(keytab) || StringUtils.isEmpty(principal)) {
-      if (StringUtils.isNotEmpty(keytab)) {
-        SubmarineRuntimeException e = new SubmarineRuntimeException("The " +
-            "parameter of " + PRINCIPAL + " is missing.");
-        LOG.error(e.getMessage(), e);
-        throw e;
-      }
-
-      if (StringUtils.isNotEmpty(principal)) {
-        SubmarineRuntimeException e = new SubmarineRuntimeException("The " +
-            "parameter of " + KEYTAB + " is missing.");
-        LOG.error(e.getMessage(), e);
-        throw e;
-      }
-
-      UserGroupInformation user = UserGroupInformation.getCurrentUser();
-      if(user == null || user.getAuthenticationMethod() ==
-          UserGroupInformation.AuthenticationMethod.SIMPLE) {
-        SubmarineRuntimeException e = new SubmarineRuntimeException("Failed " +
-            "to authenticate in secure environment. Please run kinit " +
-            "command in advance or use " + "--" + KEYTAB + "/--" + PRINCIPAL +
-            " parameters");
-        LOG.error(e.getMessage(), e);
-        throw e;
-      }
-      LOG.info("Submarine job is submitted by user: " + user.getUserName());
-      return;
-    }
-
-    File keytabFile = new File(keytab);
-    if (!keytabFile.exists()) {
-      SubmarineRuntimeException e =  new SubmarineRuntimeException("No " +
-          "keytab localized at  " + keytab);
-      LOG.error(e.getMessage(), e);
-      throw e;
-    }
-    UserGroupInformation.loginUserFromKeytab(principal, keytab);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/Command.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/Command.java
deleted file mode 100644
index 8f3051f69ea2..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/Command.java
+++ /dev/null
@@ -1,24 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli;
-
-/**
- * Represents a Submarine command.
- */
-public enum Command {
-  RUN_JOB, SHOW_JOB, KILL_JOB
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/KillJobCli.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/KillJobCli.java
deleted file mode 100644
index 98eeea401e91..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/KillJobCli.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli;
-
-import static org.apache.hadoop.yarn.client.api.AppAdminClient.DEFAULT_TYPE;
-
-import java.io.IOException;
-
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.GnuParser;
-import org.apache.commons.cli.HelpFormatter;
-import org.apache.commons.cli.Options;
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.client.api.AppAdminClient;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.param.KillJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.annotations.VisibleForTesting;
-
-public class KillJobCli extends AbstractCli {
-  private static final Logger LOG = LoggerFactory.getLogger(ShowJobCli.class);
-
-  private Options options;
-  private ParametersHolder parametersHolder;
-
-  public KillJobCli(ClientContext cliContext) {
-    super(cliContext);
-    options = generateOptions();
-  }
-
-  public void printUsages() {
-    new HelpFormatter().printHelp("job kill", options);
-  }
-
-  private Options generateOptions() {
-    Options options = new Options();
-    options.addOption(CliConstants.NAME, true, "Name of the job");
-    options.addOption("h", "help", false, "Print help");
-    return options;
-  }
-
-  private void parseCommandLineAndGetKillJobParameters(String[] args)
-      throws IOException, YarnException {
-    // Do parsing
-    GnuParser parser = new GnuParser();
-    CommandLine cli;
-    try {
-      cli = parser.parse(options, args);
-      parametersHolder =
-          ParametersHolder.createWithCmdLine(cli, Command.KILL_JOB);
-      parametersHolder.updateParameters(clientContext);
-    } catch (ParseException e) {
-      LOG.error(("Error parsing command-line options: " + e.getMessage()));
-      printUsages();
-    }
-  }
-
-  @VisibleForTesting
-  protected boolean killJob() throws IOException, YarnException {
-    String jobName = getParameters().getName();
-    AppAdminClient appAdminClient = AppAdminClient
-        .createAppAdminClient(DEFAULT_TYPE, clientContext.getYarnConfig());
-
-    if (appAdminClient.actionStop(jobName) != 0) {
-      LOG.error("appAdminClient fail to stop application");
-      return false;
-    }
-    if (appAdminClient.actionDestroy(jobName) != 0) {
-      LOG.error("appAdminClient fail to destroy application");
-      return false;
-    }
-
-    appAdminClient.stop();
-    return true;
-  }
-
-  @VisibleForTesting
-  public KillJobParameters getParameters() {
-    return (KillJobParameters) parametersHolder.getParameters();
-  }
-
-  @Override
-  public int run(String[] args) throws ParseException, IOException,
-      YarnException, InterruptedException, SubmarineException {
-    if (CliUtils.argsForHelp(args)) {
-      printUsages();
-      return 0;
-    }
-    parseCommandLineAndGetKillJobParameters(args);
-    if (killJob() == true) {
-      LOG.info("Kill job successfully !");
-    }
-    return 0;
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/ShowJobCli.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/ShowJobCli.java
deleted file mode 100644
index 34dbffee5139..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/ShowJobCli.java
+++ /dev/null
@@ -1,126 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli;
-
-import com.google.common.annotations.VisibleForTesting;
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.GnuParser;
-import org.apache.commons.cli.HelpFormatter;
-import org.apache.commons.cli.Options;
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ShowJobParameters;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineException;
-import org.apache.hadoop.yarn.submarine.runtimes.common.StorageKeyConstants;
-import org.apache.hadoop.yarn.submarine.runtimes.common.SubmarineStorage;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-import java.util.Map;
-
-public class ShowJobCli extends AbstractCli {
-  private static final Logger LOG = LoggerFactory.getLogger(ShowJobCli.class);
-
-  private Options options;
-  private ParametersHolder parametersHolder;
-
-  public ShowJobCli(ClientContext cliContext) {
-    super(cliContext);
-    options = generateOptions();
-  }
-
-  public void printUsages() {
-    new HelpFormatter().printHelp("job show", options);
-  }
-
-  private Options generateOptions() {
-    Options options = new Options();
-    options.addOption(CliConstants.NAME, true, "Name of the job");
-    options.addOption("h", "help", false, "Print help");
-    return options;
-  }
-
-  private void parseCommandLineAndGetShowJobParameters(String[] args)
-      throws IOException, YarnException {
-    // Do parsing
-    GnuParser parser = new GnuParser();
-    CommandLine cli;
-    try {
-      cli = parser.parse(options, args);
-      parametersHolder = ParametersHolder
-          .createWithCmdLine(cli, Command.SHOW_JOB);
-      parametersHolder.updateParameters(clientContext);
-    } catch (ParseException e) {
-      printUsages();
-    }
-  }
-
-  private void printIfNotNull(String keyForPrint, String keyInStorage,
-      Map<String, String> jobInfo) {
-    if (jobInfo.containsKey(keyInStorage)) {
-      System.out.println("\t" + keyForPrint + ": " + jobInfo.get(keyInStorage));
-    }
-  }
-
-  private void printJobInfo(Map<String, String> jobInfo) {
-    System.out.println("Job Meta Info:");
-    printIfNotNull("Application Id", StorageKeyConstants.APPLICATION_ID,
-        jobInfo);
-    printIfNotNull("Input Path", StorageKeyConstants.INPUT_PATH, jobInfo);
-    printIfNotNull("Saved Model Path", StorageKeyConstants.SAVED_MODEL_PATH,
-        jobInfo);
-    printIfNotNull("Checkpoint Path", StorageKeyConstants.CHECKPOINT_PATH,
-        jobInfo);
-    printIfNotNull("Run Parameters", StorageKeyConstants.JOB_RUN_ARGS,
-        jobInfo);
-  }
-
-  @VisibleForTesting
-  protected void getAndPrintJobInfo() throws IOException {
-    SubmarineStorage storage =
-        clientContext.getRuntimeFactory().getSubmarineStorage();
-
-    Map<String, String> jobInfo = null;
-    try {
-      jobInfo = storage.getJobInfoByName(getParameters().getName());
-    } catch (IOException e) {
-      LOG.error("Failed to retrieve job info", e);
-      throw e;
-    }
-
-    printJobInfo(jobInfo);
-  }
-
-  @VisibleForTesting
-  public ShowJobParameters getParameters() {
-    return (ShowJobParameters) parametersHolder.getParameters();
-  }
-
-  @Override
-  public int run(String[] args)
-      throws ParseException, IOException, YarnException, InterruptedException,
-      SubmarineException {
-    if (CliUtils.argsForHelp(args)) {
-      printUsages();
-      return 0;
-    }
-    parseCommandLineAndGetShowJobParameters(args);
-    getAndPrintJobInfo();
-    return 0;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/BaseParameters.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/BaseParameters.java
deleted file mode 100644
index 468306c79df2..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/BaseParameters.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param;
-
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.CliConstants;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-
-import java.io.IOException;
-
-/**
- * Base class of all parameters.
- */
-public abstract class BaseParameters {
-  private String name;
-
-  public void updateParameters(ParametersHolder parametersHolder,
-      ClientContext clientContext)
-      throws ParseException, IOException, YarnException {
-    String name = parametersHolder.getOptionValue(CliConstants.NAME);
-    if (name == null) {
-      throw new ParseException("--name is absent");
-    }
-
-    if (parametersHolder.hasOption(CliConstants.VERBOSE)) {
-      SubmarineLogs.verboseOn();
-    }
-
-    this.setName(name);
-  }
-
-  public String getName() {
-    return name;
-  }
-
-  public BaseParameters setName(String name) {
-    this.name = name;
-    return this;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/ConfigType.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/ConfigType.java
deleted file mode 100644
index 45876e6427cc..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/ConfigType.java
+++ /dev/null
@@ -1,24 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param;
-
-/**
- * Represents the source of configuration.
- */
-public enum ConfigType {
-  YAML, CLI
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/KillJobParameters.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/KillJobParameters.java
deleted file mode 100644
index b761ee23f43d..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/KillJobParameters.java
+++ /dev/null
@@ -1,19 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param;
-
-public class KillJobParameters extends BaseParameters {
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/Localization.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/Localization.java
deleted file mode 100644
index fe9c6de4123c..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/Localization.java
+++ /dev/null
@@ -1,133 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param;
-
-import org.apache.commons.cli.ParseException;
-
-import java.util.Arrays;
-import java.util.List;
-
-/**
- * Localization parameter.
- * */
-public class Localization {
-
-  private String mountPermissionPattern = "(wr|rw)$";
-  /**
-   * Regex for directory/file path in container.
-   * YARN only support absolute path for mount, but we can
-   * support some relative path.
-   * For relative path, we only allow ".", "./","./name".
-   * relative path like "./a/b" is not allowed.
-   * "." and "./" means original dir/file name in container working directory
-   * "./name" means use same or new "name" in container working directory
-   * A absolute path means same path in container filesystem
-   */
-  private String localPathPattern = "((^\\.$)|(^\\./$)|(^\\./[^/]+)|(^/.*))";
-  private String remoteUri;
-  private String localPath;
-
-  // Read write by default
-  private String mountPermission = "rw";
-
-  private static final List<String> SUPPORTED_SCHEME = Arrays.asList(
-      "hdfs", "oss", "s3a", "s3n", "wasb",
-      "wasbs", "abfs", "abfss", "adl", "har",
-      "ftp", "http", "https", "viewfs", "swebhdfs",
-      "webhdfs", "swift");
-
-  public void parse(String arg) throws ParseException {
-    String[] tokens = arg.split(":");
-    int minimum = "a:b".split(":").length;
-    int minimumWithPermission = "a:b:rw".split(":").length;
-    int minimumParts = minimum;
-    int miniPartsWithRemoteScheme = "scheme://a:b".split(":").length;
-    int maximumParts = "scheme://a:b:rw".split(":").length;
-    // If remote uri starts with a remote scheme
-    if (isSupportedScheme(tokens[0])) {
-      minimumParts = miniPartsWithRemoteScheme;
-    }
-    if (tokens.length < minimumParts
-        || tokens.length > maximumParts) {
-      throw new ParseException("Invalid parameter,"
-          + "should be \"remoteUri:localPath[:rw|:wr]\" "
-          + "format for --localizations");
-    }
-
-    /**
-     * RemoteUri starts with remote scheme.
-     * Merge part 0 and 1 to build a hdfs path in token[0].
-     * toke[1] will be localPath to ease following logic
-     * */
-    if (minimumParts == miniPartsWithRemoteScheme) {
-      tokens[0] = tokens[0] + ":" + tokens[1];
-      tokens[1] = tokens[2];
-      if (tokens.length == maximumParts) {
-        // Has permission part
-        mountPermission = tokens[maximumParts - 1];
-      }
-    }
-    // RemoteUri starts with linux file path
-    if (minimumParts == minimum
-        && tokens.length == minimumWithPermission) {
-      // Has permission part
-      mountPermission = tokens[minimumWithPermission - 1];
-    }
-    remoteUri = tokens[0];
-    localPath = tokens[1];
-    if (!localPath.matches(localPathPattern)) {
-      throw new ParseException("Invalid local file path:"
-          + localPath
-          + ", it only support \".\", \"./\", \"./name\" and "
-          + "absolute path.");
-    }
-    if (!mountPermission.matches(mountPermissionPattern)) {
-      throw new ParseException("Invalid mount permission (ro is not "
-          + "supported yet), " + mountPermission);
-    }
-  }
-
-  public String getRemoteUri() {
-    return remoteUri;
-  }
-
-  public void setRemoteUri(String rUti) {
-    this.remoteUri = rUti;
-  }
-
-  public String getLocalPath() {
-    return localPath;
-  }
-
-  public void setLocalPath(String lPath) {
-    this.localPath = lPath;
-  }
-
-  public String getMountPermission() {
-    return mountPermission;
-  }
-
-  public void setMountPermission(String mPermission) {
-    this.mountPermission = mPermission;
-  }
-
-  private boolean isSupportedScheme(String scheme) {
-    return SUPPORTED_SCHEME.contains(scheme);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/ParametersHolder.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/ParametersHolder.java
deleted file mode 100644
index 182dda254feb..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/ParametersHolder.java
+++ /dev/null
@@ -1,443 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param;
-
-import static org.apache.hadoop.yarn.submarine.client.cli.runjob.RunJobCli.YAML_PARSE_FAILED;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.stream.Collectors;
-
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.CliConstants;
-import org.apache.hadoop.yarn.submarine.client.cli.Command;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Configs;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Role;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Roles;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Scheduling;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Security;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.TensorBoard;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.YamlConfigFile;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.YamlParseException;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.Framework;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.collect.ImmutableSet;
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
-
-/**
- * This class acts as a wrapper of {@code CommandLine} values along with
- * YAML configuration values.
- * YAML configuration is only stored if the -f &lt;filename&gt;
- * option is specified along the CLI arguments.
- * Using this wrapper class makes easy to deal with
- * any form of configuration source potentially added into Submarine,
- * in the future.
- * If both YAML and CLI value is found for a config, this is an error case.
- */
-public final class ParametersHolder {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(ParametersHolder.class);
-
-  public static final String SUPPORTED_FRAMEWORKS_MESSAGE =
-      "TensorFlow and PyTorch are the only supported frameworks for now!";
-  public static final String SUPPORTED_COMMANDS_MESSAGE =
-      "'Show job' and 'run job' are the only supported commands for now!";
-
-
-
-  private final CommandLine parsedCommandLine;
-  private final Map<String, String> yamlStringConfigs;
-  private final Map<String, List<String>> yamlListConfigs;
-  private final ConfigType configType;
-  private Command command;
-  private final Set onlyDefinedWithCliArgs = ImmutableSet.of(
-      CliConstants.VERBOSE);
-  private final Framework framework;
-  private final BaseParameters parameters;
-
-  private ParametersHolder(CommandLine parsedCommandLine,
-      YamlConfigFile yamlConfig, ConfigType configType, Command command)
-      throws ParseException, YarnException {
-    this.parsedCommandLine = parsedCommandLine;
-    this.yamlStringConfigs = initStringConfigValues(yamlConfig);
-    this.yamlListConfigs = initListConfigValues(yamlConfig);
-    this.configType = configType;
-    this.command = command;
-    this.framework = determineFrameworkType();
-    this.ensureOnlyValidSectionsAreDefined(yamlConfig);
-    this.parameters = createParameters();
-  }
-
-  private BaseParameters createParameters() {
-    if (command == Command.RUN_JOB) {
-      if (framework == Framework.TENSORFLOW) {
-        return new TensorFlowRunJobParameters();
-      } else if (framework == Framework.PYTORCH) {
-        return new PyTorchRunJobParameters();
-      } else {
-        throw new UnsupportedOperationException(SUPPORTED_FRAMEWORKS_MESSAGE);
-      }
-    } else if (command == Command.SHOW_JOB) {
-      return new ShowJobParameters();
-    } else if (command == Command.KILL_JOB) {
-      return new KillJobParameters();
-    } else {
-      throw new UnsupportedOperationException(SUPPORTED_COMMANDS_MESSAGE);
-    }
-  }
-
-  private void ensureOnlyValidSectionsAreDefined(YamlConfigFile yamlConfig) {
-    if (isCommandRunJob() && isFrameworkPyTorch() &&
-        isPsSectionDefined(yamlConfig)) {
-      throw new YamlParseException(
-          "PS section should not be defined when PyTorch " +
-              "is the selected framework!");
-    }
-
-    if (isCommandRunJob() && isFrameworkPyTorch() &&
-        isTensorboardSectionDefined(yamlConfig)) {
-      throw new YamlParseException(
-          "TensorBoard section should not be defined when PyTorch " +
-              "is the selected framework!");
-    }
-  }
-
-  private boolean isCommandRunJob() {
-    return command == Command.RUN_JOB;
-  }
-
-  private boolean isFrameworkPyTorch() {
-    return framework == Framework.PYTORCH;
-  }
-
-  private boolean isPsSectionDefined(YamlConfigFile yamlConfig) {
-    return yamlConfig != null &&
-        yamlConfig.getRoles() != null &&
-        yamlConfig.getRoles().getPs() != null;
-  }
-
-  private boolean isTensorboardSectionDefined(YamlConfigFile yamlConfig) {
-    return yamlConfig != null &&
-        yamlConfig.getTensorBoard() != null;
-  }
-
-  private Framework determineFrameworkType()
-      throws ParseException, YarnException {
-    if (!isCommandRunJob()) {
-      return null;
-    }
-    String frameworkStr = getOptionValue(CliConstants.FRAMEWORK);
-    if (frameworkStr == null) {
-      LOG.info("Framework is not defined in config, falling back to " +
-          "TensorFlow as a default.");
-      return Framework.TENSORFLOW;
-    }
-    Framework framework = Framework.parseByValue(frameworkStr);
-    if (framework == null) {
-      if (getConfigType() == ConfigType.CLI) {
-        throw new ParseException("Failed to parse Framework type! "
-            + "Valid values are: " + Framework.getValues());
-      } else {
-        throw new YamlParseException(YAML_PARSE_FAILED +
-            ", framework should is defined, but it has an invalid value! " +
-            "Valid values are: " + Framework.getValues());
-      }
-    }
-    return framework;
-  }
-
-  /**
-   * Maps every value coming from the passed yamlConfig to {@code CliConstants}.
-   * @param yamlConfig Parsed YAML config
-   * @return A map of config values, keys are {@code CliConstants}
-   * and values are Strings.
-   */
-  private Map<String, String> initStringConfigValues(
-      YamlConfigFile yamlConfig) {
-    if (yamlConfig == null) {
-      return Collections.emptyMap();
-    }
-    Map<String, String> yamlConfigValues = Maps.newHashMap();
-    Roles roles = yamlConfig.getRoles();
-
-    initGenericConfigs(yamlConfig, yamlConfigValues);
-    initPs(yamlConfigValues, roles.getPs());
-    initWorker(yamlConfigValues, roles.getWorker());
-    initScheduling(yamlConfigValues, yamlConfig.getScheduling());
-    initSecurity(yamlConfigValues, yamlConfig.getSecurity());
-    initTensorBoard(yamlConfigValues, yamlConfig.getTensorBoard());
-
-    return yamlConfigValues;
-  }
-
-  private Map<String, List<String>> initListConfigValues(
-      YamlConfigFile yamlConfig) {
-    if (yamlConfig == null) {
-      return Collections.emptyMap();
-    }
-
-    Map<String, List<String>> yamlConfigValues = Maps.newHashMap();
-    Configs configs = yamlConfig.getConfigs();
-    yamlConfigValues.put(CliConstants.LOCALIZATION, configs.getLocalizations());
-    yamlConfigValues.put(CliConstants.ENV,
-        convertToEnvsList(configs.getEnvs()));
-    yamlConfigValues.put(CliConstants.QUICKLINK, configs.getQuicklinks());
-
-    return yamlConfigValues;
-  }
-
-  private void initGenericConfigs(YamlConfigFile yamlConfig,
-      Map<String, String> yamlConfigs) {
-    yamlConfigs.put(CliConstants.NAME, yamlConfig.getSpec().getName());
-    yamlConfigs.put(CliConstants.FRAMEWORK,
-        yamlConfig.getSpec().getFramework());
-
-    Configs configs = yamlConfig.getConfigs();
-    yamlConfigs.put(CliConstants.INPUT_PATH, configs.getInputPath());
-    yamlConfigs.put(CliConstants.CHECKPOINT_PATH, configs.getCheckpointPath());
-    yamlConfigs.put(CliConstants.SAVED_MODEL_PATH, configs.getSavedModelPath());
-    yamlConfigs.put(CliConstants.DOCKER_IMAGE, configs.getDockerImage());
-    yamlConfigs.put(CliConstants.WAIT_JOB_FINISH, configs.getWaitJobFinish());
-  }
-
-  private void initPs(Map<String, String> yamlConfigs, Role ps) {
-    if (ps == null) {
-      return;
-    }
-    yamlConfigs.put(CliConstants.N_PS, String.valueOf(ps.getReplicas()));
-    yamlConfigs.put(CliConstants.PS_RES, ps.getResources());
-    yamlConfigs.put(CliConstants.PS_DOCKER_IMAGE, ps.getDockerImage());
-    yamlConfigs.put(CliConstants.PS_LAUNCH_CMD, ps.getLaunchCmd());
-  }
-
-  private void initWorker(Map<String, String> yamlConfigs, Role worker) {
-    if (worker == null) {
-      return;
-    }
-    yamlConfigs.put(CliConstants.N_WORKERS,
-        String.valueOf(worker.getReplicas()));
-    yamlConfigs.put(CliConstants.WORKER_RES, worker.getResources());
-    yamlConfigs.put(CliConstants.WORKER_DOCKER_IMAGE, worker.getDockerImage());
-    yamlConfigs.put(CliConstants.WORKER_LAUNCH_CMD, worker.getLaunchCmd());
-  }
-
-  private void initScheduling(Map<String, String> yamlConfigValues,
-      Scheduling scheduling) {
-    if (scheduling == null) {
-      return;
-    }
-    yamlConfigValues.put(CliConstants.QUEUE, scheduling.getQueue());
-  }
-
-  private void initSecurity(Map<String, String> yamlConfigValues,
-      Security security) {
-    if (security == null) {
-      return;
-    }
-    yamlConfigValues.put(CliConstants.KEYTAB, security.getKeytab());
-    yamlConfigValues.put(CliConstants.PRINCIPAL, security.getPrincipal());
-    yamlConfigValues.put(CliConstants.DISTRIBUTE_KEYTAB,
-        String.valueOf(security.isDistributeKeytab()));
-  }
-
-  private void initTensorBoard(Map<String, String> yamlConfigValues,
-      TensorBoard tensorBoard) {
-    if (tensorBoard == null) {
-      return;
-    }
-    yamlConfigValues.put(CliConstants.TENSORBOARD, Boolean.TRUE.toString());
-    yamlConfigValues.put(CliConstants.TENSORBOARD_DOCKER_IMAGE,
-        tensorBoard.getDockerImage());
-    yamlConfigValues.put(CliConstants.TENSORBOARD_RESOURCES,
-        tensorBoard.getResources());
-  }
-
-  private List<String> convertToEnvsList(Map<String, String> envs) {
-    if (envs == null) {
-      return Collections.emptyList();
-    }
-    return envs.entrySet().stream()
-        .map(e -> String.format("%s=%s", e.getKey(), e.getValue()))
-        .collect(Collectors.toList());
-  }
-
-  public static ParametersHolder createWithCmdLine(CommandLine cli,
-      Command command) throws ParseException, YarnException {
-    return new ParametersHolder(cli, null, ConfigType.CLI, command);
-  }
-
-  public static ParametersHolder createWithCmdLineAndYaml(CommandLine cli,
-      YamlConfigFile yamlConfig, Command command) throws ParseException,
-      YarnException {
-    return new ParametersHolder(cli, yamlConfig, ConfigType.YAML, command);
-  }
-
-  /**
-   * Gets the option value, either from the CLI arguments or YAML config,
-   * if present.
-   * @param option Name of the config.
-   * @return The value of the config
-   */
-  public String getOptionValue(String option) throws YarnException {
-    ensureConfigIsDefinedOnce(option, true);
-    if (onlyDefinedWithCliArgs.contains(option) ||
-        parsedCommandLine.hasOption(option)) {
-      return getValueFromCLI(option);
-    }
-    return getValueFromYaml(option);
-  }
-
-  /**
-   * Gets the option values, either from the CLI arguments or YAML config,
-   * if present.
-   * @param option Name of the config.
-   * @return The values of the config
-   */
-  public List<String> getOptionValues(String option) throws YarnException {
-    ensureConfigIsDefinedOnce(option, false);
-    if (onlyDefinedWithCliArgs.contains(option) ||
-        parsedCommandLine.hasOption(option)) {
-      return getValuesFromCLI(option);
-    }
-    return getValuesFromYaml(option);
-  }
-
-  private void ensureConfigIsDefinedOnce(String option, boolean stringValue)
-      throws YarnException {
-    boolean definedWithYaml;
-    if (stringValue) {
-      definedWithYaml = yamlStringConfigs.containsKey(option);
-    } else {
-      definedWithYaml = yamlListConfigs.containsKey(option);
-    }
-
-    if (parsedCommandLine.hasOption(option) && definedWithYaml) {
-      throw new YarnException("Config '%s' is defined both with YAML config" +
-          " and with CLI argument, please only use either way!");
-    }
-  }
-
-  private String getValueFromCLI(String option) {
-    String value = parsedCommandLine.getOptionValue(option);
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Found config value {} for key {} " +
-          "from CLI configuration.", value, option);
-    }
-    return value;
-  }
-
-  private List<String> getValuesFromCLI(String option) {
-    String[] optionValues = parsedCommandLine.getOptionValues(option);
-    if (optionValues != null) {
-      List<String> values = Arrays.asList(optionValues);
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("Found config values {} for key {} " +
-            "from CLI configuration.", values, option);
-      }
-      return values;
-    } else {
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("No config values found for key {} " +
-            "from CLI configuration.", option);
-      }
-      return Lists.newArrayList();
-    }
-  }
-
-  private String getValueFromYaml(String option) {
-    String value = yamlStringConfigs.get(option);
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Found config value {} for key {} " +
-          "from YAML configuration.", value, option);
-    }
-    return value;
-  }
-
-  private List<String> getValuesFromYaml(String option) {
-    List<String> values = yamlListConfigs.get(option);
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Found config values {} for key {} " +
-          "from YAML configuration.", values, option);
-    }
-    return values;
-  }
-
-  /**
-   * Returns the boolean value of option.
-   * First, we check if the CLI value is defined for the option.
-   * If not, then we check the YAML value.
-   * @param option name of the option
-   * @return true, if the option is found in the CLI args or in the YAML config,
-   * false otherwise.
-   */
-  public boolean hasOption(String option) {
-    if (onlyDefinedWithCliArgs.contains(option)) {
-      boolean value = parsedCommandLine.hasOption(option);
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("Found boolean config with value {} for key {} " +
-            "from CLI configuration.", value, option);
-      }
-      return value;
-    }
-    if (parsedCommandLine.hasOption(option)) {
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("Found boolean config value for key {} " +
-            "from CLI configuration.", option);
-      }
-      return true;
-    }
-    return getBooleanValueFromYaml(option);
-  }
-
-  private boolean getBooleanValueFromYaml(String option) {
-    String stringValue = yamlStringConfigs.get(option);
-    boolean result = stringValue != null
-        && Boolean.valueOf(stringValue).equals(Boolean.TRUE);
-    LOG.debug("Found config value {} for key {} " +
-        "from YAML configuration.", result, option);
-    return result;
-  }
-
-  public ConfigType getConfigType() {
-    return configType;
-  }
-
-  public Framework getFramework() {
-    return framework;
-  }
-
-  public void updateParameters(ClientContext clientContext)
-      throws ParseException, YarnException, IOException {
-    parameters.updateParameters(this, clientContext);
-  }
-
-  public BaseParameters getParameters() {
-    return parameters;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/Quicklink.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/Quicklink.java
deleted file mode 100644
index ea8732ce5072..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/Quicklink.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param;
-
-import org.apache.commons.cli.ParseException;
-
-/**
- * A class represents quick links to a web page.
- */
-public class Quicklink {
-  private String label;
-  private String componentInstanceName;
-  private String protocol;
-  private int port;
-
-  public void parse(String quicklinkStr) throws ParseException {
-    if (!quicklinkStr.contains("=")) {
-      throw new ParseException("Should be <label>=<link> format for quicklink");
-    }
-
-    int index = quicklinkStr.indexOf("=");
-    label = quicklinkStr.substring(0, index);
-    quicklinkStr = quicklinkStr.substring(index + 1);
-
-    if (quicklinkStr.startsWith("http://")) {
-      protocol = "http://";
-    } else if (quicklinkStr.startsWith("https://")) {
-      protocol = "https://";
-    } else {
-      throw new ParseException("Quicklink should start with http or https");
-    }
-
-    quicklinkStr = quicklinkStr.substring(protocol.length());
-    index = quicklinkStr.indexOf(":");
-
-    if (index == -1) {
-      throw new ParseException("Quicklink should be componet-id:port form");
-    }
-
-    componentInstanceName = quicklinkStr.substring(0, index);
-    port = Integer.parseInt(quicklinkStr.substring(index + 1));
-  }
-
-  public String getLabel() {
-    return label;
-  }
-
-  public String getComponentInstanceName() {
-    return componentInstanceName;
-  }
-
-  public String getProtocol() {
-    return protocol;
-  }
-
-  public int getPort() {
-    return port;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/RunParameters.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/RunParameters.java
deleted file mode 100644
index 34fbab2cb1d3..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/RunParameters.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param;
-
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.CliConstants;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-/**
- * Parameters required to run anything on cluster. Such as run job / serve model
- */
-public abstract class RunParameters extends BaseParameters {
-  private String savedModelPath;
-  private String dockerImageName;
-  private List<String> envars = new ArrayList<>();
-  private String queue;
-
-  @Override
-  public void updateParameters(ParametersHolder parametersHolder,
-      ClientContext clientContext) throws ParseException,
-      IOException, YarnException {
-    String savedModelPath = parametersHolder.getOptionValue(
-        CliConstants.SAVED_MODEL_PATH);
-    this.setSavedModelPath(savedModelPath);
-
-    List<String> envVars = getEnvVars(parametersHolder);
-    this.setEnvars(envVars);
-
-    String queue = parametersHolder.getOptionValue(
-        CliConstants.QUEUE);
-    this.setQueue(queue);
-
-    String dockerImage = parametersHolder.getOptionValue(
-        CliConstants.DOCKER_IMAGE);
-    this.setDockerImageName(dockerImage);
-
-    super.updateParameters(parametersHolder, clientContext);
-  }
-
-  private List<String> getEnvVars(ParametersHolder parametersHolder)
-      throws YarnException {
-    List<String> result = new ArrayList<>();
-    List<String> envVarsArray = parametersHolder.getOptionValues(
-        CliConstants.ENV);
-    if (envVarsArray != null) {
-      result.addAll(envVarsArray);
-    }
-    return result;
-  }
-
-  public String getQueue() {
-    return queue;
-  }
-
-  public RunParameters setQueue(String queue) {
-    this.queue = queue;
-    return this;
-  }
-
-  public String getDockerImageName() {
-    return dockerImageName;
-  }
-
-  public RunParameters setDockerImageName(String dockerImageName) {
-    this.dockerImageName = dockerImageName;
-    return this;
-  }
-
-
-  public List<String> getEnvars() {
-    return envars;
-  }
-
-  public RunParameters setEnvars(List<String> envars) {
-    this.envars = envars;
-    return this;
-  }
-
-  public String getSavedModelPath() {
-    return savedModelPath;
-  }
-
-  public RunParameters setSavedModelPath(String savedModelPath) {
-    this.savedModelPath = savedModelPath;
-    return this;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/ShowJobParameters.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/ShowJobParameters.java
deleted file mode 100644
index e5f19d663f31..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/ShowJobParameters.java
+++ /dev/null
@@ -1,18 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param;
-
-public class ShowJobParameters extends BaseParameters {
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/package-info.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/package-info.java
deleted file mode 100644
index 2df120f31c09..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/package-info.java
+++ /dev/null
@@ -1,19 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/PyTorchRunJobParameters.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/PyTorchRunJobParameters.java
deleted file mode 100644
index 4f72e40480ca..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/PyTorchRunJobParameters.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.runjob;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.commons.cli.ParseException;
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.CliConstants;
-import org.apache.hadoop.yarn.submarine.client.cli.CliUtils;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-
-import com.google.common.collect.Lists;
-
-/**
- * Parameters for PyTorch job.
- */
-public class PyTorchRunJobParameters extends RunJobParameters {
-
-  private static final String CANNOT_BE_DEFINED_FOR_PYTORCH =
-      "cannot be defined for PyTorch jobs!";
-
-  @Override
-  public void updateParameters(ParametersHolder parametersHolder,
-      ClientContext clientContext)
-      throws ParseException, IOException, YarnException {
-    checkArguments(parametersHolder);
-
-    super.updateParameters(parametersHolder, clientContext);
-
-    String input = parametersHolder.getOptionValue(CliConstants.INPUT_PATH);
-    this.workerParameters =
-        getWorkerParameters(clientContext, parametersHolder, input);
-    this.distributed = determineIfDistributed(workerParameters.getReplicas());
-    executePostOperations(clientContext);
-  }
-
-  private void checkArguments(ParametersHolder parametersHolder)
-      throws YarnException, ParseException {
-    if (parametersHolder.getOptionValue(CliConstants.N_PS) != null) {
-      throw new ParseException(getParamCannotBeDefinedErrorMessage(
-          CliConstants.N_PS));
-    } else if (parametersHolder.getOptionValue(CliConstants.PS_RES) != null) {
-      throw new ParseException(getParamCannotBeDefinedErrorMessage(
-          CliConstants.PS_RES));
-    } else if (parametersHolder
-        .getOptionValue(CliConstants.PS_DOCKER_IMAGE) != null) {
-      throw new ParseException(getParamCannotBeDefinedErrorMessage(
-          CliConstants.PS_DOCKER_IMAGE));
-    } else if (parametersHolder
-        .getOptionValue(CliConstants.PS_LAUNCH_CMD) != null) {
-      throw new ParseException(getParamCannotBeDefinedErrorMessage(
-          CliConstants.PS_LAUNCH_CMD));
-    } else if (parametersHolder.hasOption(CliConstants.TENSORBOARD)) {
-      throw new ParseException(getParamCannotBeDefinedErrorMessage(
-          CliConstants.TENSORBOARD));
-    } else if (parametersHolder
-        .getOptionValue(CliConstants.TENSORBOARD_RESOURCES) != null) {
-      throw new ParseException(getParamCannotBeDefinedErrorMessage(
-          CliConstants.TENSORBOARD_RESOURCES));
-    } else if (parametersHolder
-        .getOptionValue(CliConstants.TENSORBOARD_DOCKER_IMAGE) != null) {
-      throw new ParseException(getParamCannotBeDefinedErrorMessage(
-          CliConstants.TENSORBOARD_DOCKER_IMAGE));
-    }
-  }
-
-  private String getParamCannotBeDefinedErrorMessage(String cliName) {
-    return String.format(
-        "Parameter '%s' " + CANNOT_BE_DEFINED_FOR_PYTORCH, cliName);
-  }
-
-  @Override
-  void executePostOperations(ClientContext clientContext) throws IOException {
-    // Set default job dir / saved model dir, etc.
-    setDefaultDirs(clientContext);
-    replacePatternsInParameters(clientContext);
-  }
-
-  private void replacePatternsInParameters(ClientContext clientContext)
-      throws IOException {
-    if (StringUtils.isNotEmpty(getWorkerLaunchCmd())) {
-      String afterReplace =
-          CliUtils.replacePatternsInLaunchCommand(getWorkerLaunchCmd(), this,
-              clientContext.getRemoteDirectoryManager());
-      setWorkerLaunchCmd(afterReplace);
-    }
-  }
-
-  @Override
-  public List<String> getLaunchCommands() {
-    return Lists.newArrayList(getWorkerLaunchCmd());
-  }
-
-  /**
-   * We only support non-distributed PyTorch integration for now.
-   * @param nWorkers
-   * @return
-   */
-  private boolean determineIfDistributed(int nWorkers) {
-    return false;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/RunJobParameters.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/RunJobParameters.java
deleted file mode 100644
index 037593b2ddd8..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/RunJobParameters.java
+++ /dev/null
@@ -1,348 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.runjob;
-
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.CaseFormat;
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.CliConstants;
-import org.apache.hadoop.yarn.submarine.client.cli.CliUtils;
-import org.apache.hadoop.yarn.submarine.client.cli.param.Localization;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-import org.apache.hadoop.yarn.submarine.client.cli.param.Quicklink;
-import org.apache.hadoop.yarn.submarine.client.cli.param.RunParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.RoleParameters;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.common.resource.ResourceUtils;
-import org.yaml.snakeyaml.introspector.Property;
-import org.yaml.snakeyaml.introspector.PropertyUtils;
-
-import java.beans.IntrospectionException;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-/**
- * Parameters used to run a job
- */
-public abstract class RunJobParameters extends RunParameters {
-  private String input;
-  private String checkpointPath;
-
-  private List<Quicklink> quicklinks = new ArrayList<>();
-  private List<Localization> localizations = new ArrayList<>();
-
-  private boolean waitJobFinish = false;
-  protected boolean distributed = false;
-
-  private boolean securityDisabled = false;
-  private String keytab;
-  private String principal;
-  private boolean distributeKeytab = false;
-  private List<String> confPairs = new ArrayList<>();
-
-  RoleParameters workerParameters =
-      RoleParameters.createEmpty(TensorFlowRole.WORKER);
-
-  @Override
-  public void updateParameters(ParametersHolder parametersHolder,
-      ClientContext clientContext)
-      throws ParseException, IOException, YarnException {
-
-    String input = parametersHolder.getOptionValue(CliConstants.INPUT_PATH);
-    String jobDir = parametersHolder.getOptionValue(
-        CliConstants.CHECKPOINT_PATH);
-
-    if (parametersHolder.hasOption(CliConstants.INSECURE_CLUSTER)) {
-      setSecurityDisabled(true);
-    }
-
-    String kerberosKeytab = parametersHolder.getOptionValue(
-        CliConstants.KEYTAB);
-    String kerberosPrincipal = parametersHolder.getOptionValue(
-        CliConstants.PRINCIPAL);
-    CliUtils.doLoginIfSecure(kerberosKeytab, kerberosPrincipal);
-
-    if (parametersHolder.hasOption(CliConstants.WAIT_JOB_FINISH)) {
-      this.waitJobFinish = true;
-    }
-
-    // Quicklinks
-    List<String> quicklinkStrs = parametersHolder.getOptionValues(
-        CliConstants.QUICKLINK);
-    if (quicklinkStrs != null) {
-      for (String ql : quicklinkStrs) {
-        Quicklink quicklink = new Quicklink();
-        quicklink.parse(ql);
-        quicklinks.add(quicklink);
-      }
-    }
-
-    // Localizations
-    List<String> localizationsStr = parametersHolder.getOptionValues(
-        CliConstants.LOCALIZATION);
-    if (null != localizationsStr) {
-      for (String loc : localizationsStr) {
-        Localization localization = new Localization();
-        localization.parse(loc);
-        localizations.add(localization);
-      }
-    }
-    boolean distributeKerberosKeytab = parametersHolder.hasOption(CliConstants
-        .DISTRIBUTE_KEYTAB);
-
-    List<String> configPairs = parametersHolder
-        .getOptionValues(CliConstants.ARG_CONF);
-
-    this.setInputPath(input).setCheckpointPath(jobDir)
-        .setKeytab(kerberosKeytab)
-        .setPrincipal(kerberosPrincipal)
-        .setDistributeKeytab(distributeKerberosKeytab)
-        .setConfPairs(configPairs);
-
-    super.updateParameters(parametersHolder, clientContext);
-  }
-
-  abstract void executePostOperations(ClientContext clientContext)
-      throws IOException;
-
-  void setDefaultDirs(ClientContext clientContext) throws IOException {
-    // Create directories if needed
-    String jobDir = getCheckpointPath();
-    if (jobDir == null) {
-      jobDir = getJobDir(clientContext);
-      setCheckpointPath(jobDir);
-    }
-
-    if (getNumWorkers() > 0) {
-      String savedModelDir = getSavedModelPath();
-      if (savedModelDir == null) {
-        savedModelDir = jobDir;
-        setSavedModelPath(savedModelDir);
-      }
-    }
-  }
-
-  private String getJobDir(ClientContext clientContext) throws IOException {
-    RemoteDirectoryManager rdm = clientContext.getRemoteDirectoryManager();
-    if (getNumWorkers() > 0) {
-      return rdm.getJobCheckpointDir(getName(), true).toString();
-    } else {
-      // when #workers == 0, it means we only launch TB. In that case,
-      // point job dir to root dir so all job's metrics will be shown.
-      return rdm.getUserRootFolder().toString();
-    }
-  }
-
-  public abstract List<String> getLaunchCommands();
-
-  public String getInputPath() {
-    return input;
-  }
-
-  public RunJobParameters setInputPath(String input) {
-    this.input = input;
-    return this;
-  }
-
-  public String getCheckpointPath() {
-    return checkpointPath;
-  }
-
-  public RunJobParameters setCheckpointPath(String checkpointPath) {
-    this.checkpointPath = checkpointPath;
-    return this;
-  }
-
-  public boolean isWaitJobFinish() {
-    return waitJobFinish;
-  }
-
-  public List<Quicklink> getQuicklinks() {
-    return quicklinks;
-  }
-
-  public List<Localization> getLocalizations() {
-    return localizations;
-  }
-
-  public String getKeytab() {
-    return keytab;
-  }
-
-  public RunJobParameters setKeytab(String kerberosKeytab) {
-    this.keytab = kerberosKeytab;
-    return this;
-  }
-
-  public String getPrincipal() {
-    return principal;
-  }
-
-  public RunJobParameters setPrincipal(String kerberosPrincipal) {
-    this.principal = kerberosPrincipal;
-    return this;
-  }
-
-  public boolean isSecurityDisabled() {
-    return securityDisabled;
-  }
-
-  public void setSecurityDisabled(boolean securityDisabled) {
-    this.securityDisabled = securityDisabled;
-  }
-
-  public boolean isDistributeKeytab() {
-    return distributeKeytab;
-  }
-
-  public RunJobParameters setDistributeKeytab(
-      boolean distributeKerberosKeytab) {
-    this.distributeKeytab = distributeKerberosKeytab;
-    return this;
-  }
-
-  public List<String> getConfPairs() {
-    return confPairs;
-  }
-
-  public RunJobParameters setConfPairs(List<String> confPairs) {
-    this.confPairs = confPairs;
-    return this;
-  }
-
-  public void setDistributed(boolean distributed) {
-    this.distributed = distributed;
-  }
-
-  RoleParameters getWorkerParameters(ClientContext clientContext,
-      ParametersHolder parametersHolder, String input)
-      throws ParseException, YarnException, IOException {
-    int nWorkers = getNumberOfWorkers(parametersHolder, input);
-    Resource workerResource =
-        determineWorkerResource(parametersHolder, nWorkers, clientContext);
-    String workerDockerImage =
-        parametersHolder.getOptionValue(CliConstants.WORKER_DOCKER_IMAGE);
-    String workerLaunchCmd =
-        parametersHolder.getOptionValue(CliConstants.WORKER_LAUNCH_CMD);
-    return new RoleParameters(TensorFlowRole.WORKER, nWorkers,
-        workerLaunchCmd, workerDockerImage, workerResource);
-  }
-
-  private Resource determineWorkerResource(ParametersHolder parametersHolder,
-      int nWorkers, ClientContext clientContext)
-      throws ParseException, YarnException, IOException {
-    if (nWorkers > 0) {
-      String workerResourceStr =
-          parametersHolder.getOptionValue(CliConstants.WORKER_RES);
-      if (workerResourceStr == null) {
-        throw new ParseException(
-            "--" + CliConstants.WORKER_RES + " is absent.");
-      }
-      return ResourceUtils.createResourceFromString(workerResourceStr);
-    }
-    return null;
-  }
-
-  private int getNumberOfWorkers(ParametersHolder parametersHolder,
-      String input) throws ParseException, YarnException {
-    int nWorkers = 1;
-    if (parametersHolder.getOptionValue(CliConstants.N_WORKERS) != null) {
-      nWorkers = Integer
-          .parseInt(parametersHolder.getOptionValue(CliConstants.N_WORKERS));
-      // Only check null value.
-      // Training job shouldn't ignore INPUT_PATH option
-      // But if nWorkers is 0, INPUT_PATH can be ignored because
-      // user can only run Tensorboard
-      if (null == input && 0 != nWorkers) {
-        throw new ParseException(
-            "\"--" + CliConstants.INPUT_PATH + "\" is absent");
-      }
-    }
-    return nWorkers;
-  }
-
-  public String getWorkerLaunchCmd() {
-    return workerParameters.getLaunchCommand();
-  }
-
-  public void setWorkerLaunchCmd(String launchCmd) {
-    workerParameters.setLaunchCommand(launchCmd);
-  }
-
-  public int getNumWorkers() {
-    return workerParameters.getReplicas();
-  }
-
-  public void setNumWorkers(int numWorkers) {
-    workerParameters.setReplicas(numWorkers);
-  }
-
-  public Resource getWorkerResource() {
-    return workerParameters.getResource();
-  }
-
-  public void setWorkerResource(Resource resource) {
-    workerParameters.setResource(resource);
-  }
-
-  public String getWorkerDockerImage() {
-    return workerParameters.getDockerImage();
-  }
-
-  public void setWorkerDockerImage(String image) {
-    workerParameters.setDockerImage(image);
-  }
-
-  public boolean isDistributed() {
-    return distributed;
-  }
-
-  @VisibleForTesting
-  public static class UnderscoreConverterPropertyUtils extends PropertyUtils {
-    @Override
-    public Property getProperty(Class<? extends Object> type, String name)
-        throws IntrospectionException {
-      if (name.indexOf('_') > -1) {
-        name = convertName(name);
-      }
-      return super.getProperty(type, name);
-    }
-
-    private static String convertName(String name) {
-      return CaseFormat.UPPER_UNDERSCORE.to(CaseFormat.LOWER_CAMEL, name);
-    }
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/TensorFlowRunJobParameters.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/TensorFlowRunJobParameters.java
deleted file mode 100644
index b0deeb70d8f8..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/TensorFlowRunJobParameters.java
+++ /dev/null
@@ -1,213 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.runjob;
-
-import com.google.common.collect.Lists;
-import org.apache.commons.cli.ParseException;
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.CliConstants;
-import org.apache.hadoop.yarn.submarine.client.cli.CliUtils;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.RoleParameters;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.common.resource.ResourceUtils;
-
-import java.io.IOException;
-import java.util.List;
-
-/**
- * Parameters for TensorFlow job.
- */
-public class TensorFlowRunJobParameters extends RunJobParameters {
-  private boolean tensorboardEnabled;
-  private RoleParameters psParameters =
-      RoleParameters.createEmpty(TensorFlowRole.PS);
-  private RoleParameters tensorBoardParameters =
-      RoleParameters.createEmpty(TensorFlowRole.TENSORBOARD);
-
-  @Override
-  public void updateParameters(ParametersHolder parametersHolder,
-      ClientContext clientContext)
-      throws ParseException, IOException, YarnException {
-    super.updateParameters(parametersHolder, clientContext);
-
-    String input = parametersHolder.getOptionValue(CliConstants.INPUT_PATH);
-    this.workerParameters =
-        getWorkerParameters(clientContext, parametersHolder, input);
-    this.psParameters = getPSParameters(clientContext, parametersHolder);
-    this.distributed = determineIfDistributed(workerParameters.getReplicas(),
-        psParameters.getReplicas());
-
-    if (parametersHolder.hasOption(CliConstants.TENSORBOARD)) {
-      this.tensorboardEnabled = true;
-      this.tensorBoardParameters =
-          getTensorBoardParameters(parametersHolder, clientContext);
-    }
-    executePostOperations(clientContext);
-  }
-
-  @Override
-  void executePostOperations(ClientContext clientContext) throws IOException {
-    // Set default job dir / saved model dir, etc.
-    setDefaultDirs(clientContext);
-    replacePatternsInParameters(clientContext);
-  }
-
-  private void replacePatternsInParameters(ClientContext clientContext)
-      throws IOException {
-    if (StringUtils.isNotEmpty(getPSLaunchCmd())) {
-      String afterReplace = CliUtils.replacePatternsInLaunchCommand(
-          getPSLaunchCmd(), this, clientContext.getRemoteDirectoryManager());
-      setPSLaunchCmd(afterReplace);
-    }
-
-    if (StringUtils.isNotEmpty(getWorkerLaunchCmd())) {
-      String afterReplace =
-          CliUtils.replacePatternsInLaunchCommand(getWorkerLaunchCmd(), this,
-              clientContext.getRemoteDirectoryManager());
-      setWorkerLaunchCmd(afterReplace);
-    }
-  }
-
-  @Override
-  public List<String> getLaunchCommands() {
-    return Lists.newArrayList(getWorkerLaunchCmd(), getPSLaunchCmd());
-  }
-
-  private boolean determineIfDistributed(int nWorkers, int nPS)
-      throws ParseException {
-    // Check #workers and #ps.
-    // When distributed training is required
-    if (nWorkers >= 2 && nPS > 0) {
-      return true;
-    } else if (nWorkers <= 1 && nPS > 0) {
-      throw new ParseException("Only specified one worker but non-zero PS, "
-          + "please double check.");
-    }
-    return false;
-  }
-
-  private RoleParameters getPSParameters(ClientContext clientContext,
-      ParametersHolder parametersHolder)
-      throws YarnException, IOException, ParseException {
-    int nPS = getNumberOfPS(parametersHolder);
-    Resource psResource =
-        determinePSResource(parametersHolder, nPS, clientContext);
-    String psDockerImage =
-        parametersHolder.getOptionValue(CliConstants.PS_DOCKER_IMAGE);
-    String psLaunchCommand =
-        parametersHolder.getOptionValue(CliConstants.PS_LAUNCH_CMD);
-    return new RoleParameters(TensorFlowRole.PS, nPS, psLaunchCommand,
-        psDockerImage, psResource);
-  }
-
-  private Resource determinePSResource(ParametersHolder parametersHolder,
-      int nPS, ClientContext clientContext)
-      throws ParseException, YarnException, IOException {
-    if (nPS > 0) {
-      String psResourceStr =
-          parametersHolder.getOptionValue(CliConstants.PS_RES);
-      if (psResourceStr == null) {
-        throw new ParseException("--" + CliConstants.PS_RES + " is absent.");
-      }
-      return ResourceUtils.createResourceFromString(psResourceStr);
-    }
-    return null;
-  }
-
-  private int getNumberOfPS(ParametersHolder parametersHolder)
-      throws YarnException {
-    int nPS = 0;
-    if (parametersHolder.getOptionValue(CliConstants.N_PS) != null) {
-      nPS =
-          Integer.parseInt(parametersHolder.getOptionValue(CliConstants.N_PS));
-    }
-    return nPS;
-  }
-
-  private RoleParameters getTensorBoardParameters(
-      ParametersHolder parametersHolder, ClientContext clientContext)
-      throws YarnException, IOException {
-    String tensorboardResourceStr =
-        parametersHolder.getOptionValue(CliConstants.TENSORBOARD_RESOURCES);
-    if (tensorboardResourceStr == null || tensorboardResourceStr.isEmpty()) {
-      tensorboardResourceStr = CliConstants.TENSORBOARD_DEFAULT_RESOURCES;
-    }
-    Resource tensorboardResource = ResourceUtils.createResourceFromString(
-            tensorboardResourceStr);
-    String tensorboardDockerImage =
-        parametersHolder.getOptionValue(CliConstants.TENSORBOARD_DOCKER_IMAGE);
-    return new RoleParameters(TensorFlowRole.TENSORBOARD, 1, null,
-        tensorboardDockerImage, tensorboardResource);
-  }
-
-  public int getNumPS() {
-    return psParameters.getReplicas();
-  }
-
-  public void setNumPS(int numPS) {
-    psParameters.setReplicas(numPS);
-  }
-
-  public Resource getPsResource() {
-    return psParameters.getResource();
-  }
-
-  public void setPsResource(Resource resource) {
-    psParameters.setResource(resource);
-  }
-
-  public String getPsDockerImage() {
-    return psParameters.getDockerImage();
-  }
-
-  public void setPsDockerImage(String image) {
-    psParameters.setDockerImage(image);
-  }
-
-  public String getPSLaunchCmd() {
-    return psParameters.getLaunchCommand();
-  }
-
-  public void setPSLaunchCmd(String launchCmd) {
-    psParameters.setLaunchCommand(launchCmd);
-  }
-
-  public boolean isTensorboardEnabled() {
-    return tensorboardEnabled;
-  }
-
-  public Resource getTensorboardResource() {
-    return tensorBoardParameters.getResource();
-  }
-
-  public void setTensorboardResource(Resource resource) {
-    tensorBoardParameters.setResource(resource);
-  }
-
-  public String getTensorboardDockerImage() {
-    return tensorBoardParameters.getDockerImage();
-  }
-
-  public void setTensorboardDockerImage(String image) {
-    tensorBoardParameters.setDockerImage(image);
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/package-info.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/package-info.java
deleted file mode 100644
index e0193a51e362..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/package-info.java
+++ /dev/null
@@ -1,20 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains classes that hold run job parameters for
- * TensorFlow / PyTorch jobs.
- */
-package org.apache.hadoop.yarn.submarine.client.cli.param.runjob;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Configs.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Configs.java
deleted file mode 100644
index 4636c8000da4..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Configs.java
+++ /dev/null
@@ -1,107 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-import java.util.List;
-import java.util.Map;
-
-/**
- * Class that holds values found in 'configs' section of YAML configuration.
- */
-public class Configs {
-  private String dockerImage;
-  private String inputPath;
-  private String savedModelPath;
-  private String checkpointPath;
-  private List<String> quicklinks;
-  private String waitJobFinish;
-  private Map<String, String> envs;
-  private List<String> localizations;
-  private List<String> mounts;
-
-  public String getDockerImage() {
-    return dockerImage;
-  }
-
-  public void setDockerImage(String dockerImage) {
-    this.dockerImage = dockerImage;
-  }
-
-  public String getInputPath() {
-    return inputPath;
-  }
-
-  public void setInputPath(String inputPath) {
-    this.inputPath = inputPath;
-  }
-
-  public String getSavedModelPath() {
-    return savedModelPath;
-  }
-
-  public void setSavedModelPath(String savedModelPath) {
-    this.savedModelPath = savedModelPath;
-  }
-
-  public String getCheckpointPath() {
-    return checkpointPath;
-  }
-
-  public void setCheckpointPath(String checkpointPath) {
-    this.checkpointPath = checkpointPath;
-  }
-
-  public Map<String, String> getEnvs() {
-    return envs;
-  }
-
-  public void setEnvs(Map<String, String> envs) {
-    this.envs = envs;
-  }
-
-  public List<String> getLocalizations() {
-    return localizations;
-  }
-
-  public void setLocalizations(List<String> localizations) {
-    this.localizations = localizations;
-  }
-
-  public List<String> getMounts() {
-    return mounts;
-  }
-
-  public void setMounts(List<String> mounts) {
-    this.mounts = mounts;
-  }
-
-  public List<String> getQuicklinks() {
-    return quicklinks;
-  }
-
-  public void setQuicklinks(List<String> quicklinks) {
-    this.quicklinks = quicklinks;
-  }
-
-  public String getWaitJobFinish() {
-    return waitJobFinish;
-  }
-
-  public void setWaitJobFinish(String waitJobFinish) {
-    this.waitJobFinish = waitJobFinish;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/PsRole.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/PsRole.java
deleted file mode 100644
index 9009c7ecfbc2..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/PsRole.java
+++ /dev/null
@@ -1,25 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-/**
- * Holds configuration values for PS (parameter server).
- * 'ps' is a section underneath the 'roles' section of the YAML
- * configuration file.
- */
-public class PsRole extends Role {
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Role.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Role.java
deleted file mode 100644
index 172fe65fb1c1..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Role.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-import java.util.List;
-import java.util.Map;
-
-/**
- * Base class for Roles. 'roles' is a section of the YAML configuration file.
- */
-public class Role {
-  private String resources;
-  private int replicas;
-  private String launchCmd;
-
-  //Optional parameters (Can override global config)
-  private String dockerImage;
-  private Map<String, String> envs;
-  private List<String> localizations;
-  private List<String> mounts;
-
-  public String getResources() {
-    return resources;
-  }
-
-  public void setResources(String resources) {
-    this.resources = resources;
-  }
-
-  public int getReplicas() {
-    return replicas;
-  }
-
-  public void setReplicas(int replicas) {
-    this.replicas = replicas;
-  }
-
-  public String getLaunchCmd() {
-    return launchCmd;
-  }
-
-  public void setLaunchCmd(String launchCmd) {
-    this.launchCmd = launchCmd;
-  }
-
-  public String getDockerImage() {
-    return dockerImage;
-  }
-
-  public void setDockerImage(String dockerImage) {
-    this.dockerImage = dockerImage;
-  }
-
-  public Map<String, String> getEnvs() {
-    return envs;
-  }
-
-  public void setEnvs(Map<String, String> envs) {
-    this.envs = envs;
-  }
-
-  public List<String> getLocalizations() {
-    return localizations;
-  }
-
-  public void setLocalizations(List<String> localizations) {
-    this.localizations = localizations;
-  }
-
-  public List<String> getMounts() {
-    return mounts;
-  }
-
-  public void setMounts(List<String> mounts) {
-    this.mounts = mounts;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Roles.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Roles.java
deleted file mode 100644
index faa7900dcca5..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Roles.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-/**
- * This class represents a section of the YAML configuration file.
- */
-public class Roles {
-  private Role worker;
-  private Role ps;
-
-  public Role getWorker() {
-    return worker;
-  }
-
-  public void setWorker(Role worker) {
-    this.worker = worker;
-  }
-
-  public Role getPs() {
-    return ps;
-  }
-
-  public void setPs(Role ps) {
-    this.ps = ps;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Scheduling.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Scheduling.java
deleted file mode 100644
index b304909f3445..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Scheduling.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-/**
- * Class that holds values found in 'scheduling' section of YAML configuration.
- */
-public class Scheduling {
-  private String queue;
-
-  public String getQueue() {
-    return queue;
-  }
-
-  public void setQueue(String queue) {
-    this.queue = queue;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Security.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Security.java
deleted file mode 100644
index b2465daff5ea..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Security.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-/**
- * Class that holds values found in 'security' section of YAML configuration.
- */
-public class Security {
-  private String keytab;
-  private String principal;
-  private boolean distributeKeytab;
-
-  public String getKeytab() {
-    return keytab;
-  }
-
-  public void setKeytab(String keytab) {
-    this.keytab = keytab;
-  }
-
-  public String getPrincipal() {
-    return principal;
-  }
-
-  public void setPrincipal(String principal) {
-    this.principal = principal;
-  }
-
-  public boolean isDistributeKeytab() {
-    return distributeKeytab;
-  }
-
-  public void setDistributeKeytab(boolean distributeKeytab) {
-    this.distributeKeytab = distributeKeytab;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Spec.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Spec.java
deleted file mode 100644
index 58b4c4b866b7..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Spec.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-/**
- * Class that holds values found in 'spec' section of YAML configuration.
- */
-public class Spec {
-  private String name;
-  private String jobType;
-  private String framework;
-
-  public String getJobType() {
-    return jobType;
-  }
-
-  public void setJobType(String jobtype) {
-    this.jobType = jobtype;
-  }
-
-  public String getName() {
-    return name;
-  }
-
-  public void setName(String name) {
-    this.name = name;
-  }
-
-  public String getFramework() {
-    return framework;
-  }
-
-  public void setFramework(String framework) {
-    this.framework = framework;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/TensorBoard.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/TensorBoard.java
deleted file mode 100644
index ea028a8651c8..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/TensorBoard.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-/**
- * Class that holds values found in 'tensorboard' section of YAML configuration.
- */
-public class TensorBoard {
-  private String dockerImage;
-  private String resources;
-
-  public String getDockerImage() {
-    return dockerImage;
-  }
-
-  public void setDockerImage(String dockerImage) {
-    this.dockerImage = dockerImage;
-  }
-
-  public String getResources() {
-    return resources;
-  }
-
-  public void setResources(String resources) {
-    this.resources = resources;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/WorkerRole.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/WorkerRole.java
deleted file mode 100644
index 8c50afc65c46..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/WorkerRole.java
+++ /dev/null
@@ -1,25 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-/**
- * Holds configuration values for the worker role.
- * 'worker' is a section underneath the 'roles' section of the YAML
- * configuration file.
- */
-public class WorkerRole extends Role {
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/YamlConfigFile.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/YamlConfigFile.java
deleted file mode 100644
index 82ec7ff205fe..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/YamlConfigFile.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-/**
- * Root class of YAML configuration.
- */
-public class YamlConfigFile {
-  private Spec spec;
-  private Configs configs;
-  private Roles roles;
-  private Scheduling scheduling;
-  private Security security;
-  private TensorBoard tensorBoard;
-
-  public Spec getSpec() {
-    return spec;
-  }
-
-  public void setSpec(Spec spec) {
-    this.spec = spec;
-  }
-
-  public Configs getConfigs() {
-    return configs;
-  }
-
-  public void setConfigs(Configs configs) {
-    this.configs = configs;
-  }
-
-  public Roles getRoles() {
-    return roles;
-  }
-
-  public void setRoles(Roles roles) {
-    this.roles = roles;
-  }
-
-  public Scheduling getScheduling() {
-    return scheduling;
-  }
-
-  public void setScheduling(Scheduling scheduling) {
-    this.scheduling = scheduling;
-  }
-
-  public Security getSecurity() {
-    return security;
-  }
-
-  public void setSecurity(Security security) {
-    this.security = security;
-  }
-
-  public TensorBoard getTensorBoard() {
-    return tensorBoard;
-  }
-
-  public void setTensorBoard(TensorBoard tensorBoard) {
-    this.tensorBoard = tensorBoard;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/YamlParseException.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/YamlParseException.java
deleted file mode 100644
index 6c5eedcd3f69..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/YamlParseException.java
+++ /dev/null
@@ -1,27 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
-
-/**
- * This exception is thrown if any issue arises while parsing the
- * YAML configuration.
- */
-public class YamlParseException extends RuntimeException {
-  public YamlParseException(String message) {
-    super(message);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/package-info.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/package-info.java
deleted file mode 100644
index 26d44af0ab48..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/package-info.java
+++ /dev/null
@@ -1,19 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains value classes for the YAML parser.
- */
-package org.apache.hadoop.yarn.submarine.client.cli.param.yaml;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/Framework.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/Framework.java
deleted file mode 100644
index e2243661985d..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/Framework.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob;
-
-import com.google.common.collect.Lists;
-
-import java.util.List;
-import java.util.stream.Collectors;
-
-/**
- * Represents the type of Machine learning framework to work with.
- */
-public enum Framework {
-  TENSORFLOW(Constants.TENSORFLOW_NAME), PYTORCH(Constants.PYTORCH_NAME);
-
-  private String value;
-
-  Framework(String value) {
-    this.value = value;
-  }
-
-  public String getValue() {
-    return value;
-  }
-
-  public static Framework parseByValue(String value) {
-    for (Framework fw : Framework.values()) {
-      if (fw.value.equalsIgnoreCase(value)) {
-        return fw;
-      }
-    }
-    return null;
-  }
-
-  public static String getValues() {
-    List<String> values = Lists.newArrayList(Framework.values()).stream()
-        .map(fw -> fw.value).collect(Collectors.toList());
-    return String.join(",", values);
-  }
-
-  private static class Constants {
-    static final String TENSORFLOW_NAME = "tensorflow";
-    static final String PYTORCH_NAME = "pytorch";
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/RoleParameters.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/RoleParameters.java
deleted file mode 100644
index db6a0196915a..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/RoleParameters.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob;
-
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-
-/**
- * This class encapsulates data related to a particular Role.
- * Some examples: TF Worker process, TF PS process or a PyTorch worker process.
- */
-public class RoleParameters {
-  private final Role role;
-  private int replicas;
-  private String launchCommand;
-  private String dockerImage;
-  private Resource resource;
-
-  public RoleParameters(Role role, int replicas,
-      String launchCommand, String dockerImage, Resource resource) {
-    this.role = role;
-    this.replicas = replicas;
-    this.launchCommand = launchCommand;
-    this.dockerImage = dockerImage;
-    this.resource = resource;
-  }
-
-  public static RoleParameters createEmpty(Role role) {
-    return new RoleParameters(role, 0, null, null, null);
-  }
-
-  public Role getRole() {
-    return role;
-  }
-
-  public int getReplicas() {
-    return replicas;
-  }
-
-  public String getLaunchCommand() {
-    return launchCommand;
-  }
-
-  public void setLaunchCommand(String launchCommand) {
-    this.launchCommand = launchCommand;
-  }
-
-  public String getDockerImage() {
-    return dockerImage;
-  }
-
-  public void setDockerImage(String dockerImage) {
-    this.dockerImage = dockerImage;
-  }
-
-  public Resource getResource() {
-    return resource;
-  }
-
-  public void setResource(Resource resource) {
-    this.resource = resource;
-  }
-
-  public void setReplicas(int replicas) {
-    this.replicas = replicas;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/RunJobCli.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/RunJobCli.java
deleted file mode 100644
index dfd951f21d8a..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/RunJobCli.java
+++ /dev/null
@@ -1,381 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob;
-
-import com.google.common.annotations.VisibleForTesting;
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.GnuParser;
-import org.apache.commons.cli.HelpFormatter;
-import org.apache.commons.cli.Options;
-import org.apache.commons.cli.ParseException;
-import org.apache.commons.io.FileUtils;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.AbstractCli;
-import org.apache.hadoop.yarn.submarine.client.cli.CliConstants;
-import org.apache.hadoop.yarn.submarine.client.cli.CliUtils;
-import org.apache.hadoop.yarn.submarine.client.cli.Command;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters.UnderscoreConverterPropertyUtils;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.YamlConfigFile;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.YamlParseException;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineException;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobMonitor;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.common.StorageKeyConstants;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.yaml.snakeyaml.Yaml;
-import org.yaml.snakeyaml.constructor.Constructor;
-
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * This purpose of this class is to handle / parse CLI arguments related to
- * the run job Submarine command.
- */
-public class RunJobCli extends AbstractCli {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(RunJobCli.class);
-
-  private static final String TENSORFLOW = "TensorFlow";
-  private static final String PYTORCH = "PyTorch";
-  private static final String PS = "PS";
-  private static final String WORKER = "worker";
-  private static final String TENSORBOARD = "TensorBoard";
-
-  private static final String CAN_BE_USED_WITH_TF_PYTORCH =
-      String.format("Can be used with %s or %s frameworks.",
-          TENSORFLOW, PYTORCH);
-  private static final String TENSORFLOW_ONLY =
-      String.format("Can only be used with %s framework.", TENSORFLOW);
-  public static final String YAML_PARSE_FAILED = "Failed to parse " +
-      "YAML config";
-  private static final String LOCAL_OR_ANY_FS_DIRECTORY = "Could be a local " +
-      "directory or any other directory on the file system.";
-
-  private Options options;
-  private JobSubmitter jobSubmitter;
-  private JobMonitor jobMonitor;
-  private ParametersHolder parametersHolder;
-
-  public RunJobCli(ClientContext cliContext) {
-    this(cliContext, cliContext.getRuntimeFactory().getJobSubmitterInstance(),
-        cliContext.getRuntimeFactory().getJobMonitorInstance());
-  }
-
-  @VisibleForTesting
-  public RunJobCli(ClientContext cliContext, JobSubmitter jobSubmitter,
-      JobMonitor jobMonitor) {
-    super(cliContext);
-    this.options = generateOptions();
-    this.jobSubmitter = jobSubmitter;
-    this.jobMonitor = jobMonitor;
-  }
-
-  public void printUsages() {
-    new HelpFormatter().printHelp("job run", options);
-  }
-
-  private Options generateOptions() {
-    Options options = new Options();
-    options.addOption(CliConstants.YAML_CONFIG, true,
-        "Config file (in YAML format)");
-    options.addOption(CliConstants.FRAMEWORK, true,
-        String.format("Framework to use. Valid values are: %s! " +
-                "The default framework is Tensorflow.",
-            Framework.getValues()));
-    options.addOption(CliConstants.NAME, true, "Name of the job");
-    options.addOption(CliConstants.INPUT_PATH, true,
-        "Input of the job. " + LOCAL_OR_ANY_FS_DIRECTORY);
-    options.addOption(CliConstants.CHECKPOINT_PATH, true,
-        "Training output directory of the job. " + LOCAL_OR_ANY_FS_DIRECTORY +
-            "This typically includes checkpoint files and exported model");
-    options.addOption(CliConstants.SAVED_MODEL_PATH, true,
-        "Model exported path (saved model) of the job, which is needed when " +
-            "exported model is not placed under ${checkpoint_path}. " +
-            LOCAL_OR_ANY_FS_DIRECTORY + "This will be used to serve");
-    options.addOption(CliConstants.DOCKER_IMAGE, true, "Docker image name/tag");
-    options.addOption(CliConstants.PS_DOCKER_IMAGE, true,
-        getDockerImageMessage(PS));
-    options.addOption(CliConstants.WORKER_DOCKER_IMAGE, true,
-        getDockerImageMessage(WORKER));
-    options.addOption(CliConstants.QUEUE, true,
-        "Name of queue to run the job. By default, the default queue is used");
-
-    addWorkerOptions(options);
-    addPSOptions(options);
-    addTensorboardOptions(options);
-
-    options.addOption(CliConstants.ENV, true,
-        "Common environment variable passed to worker / PS");
-    options.addOption(CliConstants.VERBOSE, false,
-        "Print verbose log for troubleshooting");
-    options.addOption(CliConstants.WAIT_JOB_FINISH, false,
-        "Specified when user wants to wait for jobs to finish");
-    options.addOption(CliConstants.QUICKLINK, true, "Specify quicklink so YARN "
-        + "web UI shows link to the given role instance and port. " +
-        "When --tensorboard is specified, quicklink to the " +
-        TENSORBOARD + " instance will be added automatically. " +
-        "The format of quick link is: "
-        + "Quick_link_label=http(or https)://role-name:port. " +
-        "For example, if users want to link to the first worker's 7070 port, " +
-        "and text of quicklink is Notebook_UI, " +
-        "users need to specify --quicklink Notebook_UI=https://master-0:7070");
-    options.addOption(CliConstants.LOCALIZATION, true, "Specify"
-        + " localization to make remote/local file/directory available to"
-        + " all container(Docker)."
-        + " Argument format is: \"RemoteUri:LocalFilePath[:rw] \" "
-        + "(ro permission is not supported yet)."
-        + " The RemoteUri can be a local file or directory on the filesystem."
-        + " Alternatively, the following remote file systems / "
-        + "transmit mechanisms can be used: "
-        + " HDFS, S3 or abfs, HTTP, etc."
-        + " The LocalFilePath can be absolute or relative."
-        + " If it is a relative path, it will be"
-        + " under container's implied working directory"
-        + " but sub-directory is not supported yet."
-        + " This option can be set multiple times."
-        + " Examples are \n"
-        + "-localization \"hdfs:///user/yarn/mydir2:/opt/data\"\n"
-        + "-localization \"s3a:///a/b/myfile1:./\"\n"
-        + "-localization \"https:///a/b/myfile2:./myfile\"\n"
-        + "-localization \"/user/yarn/mydir3:/opt/mydir3\"\n"
-        + "-localization \"./mydir1:.\"\n");
-    options.addOption(CliConstants.KEYTAB, true, "Specify keytab used by the " +
-        "job under a secured environment");
-    options.addOption(CliConstants.PRINCIPAL, true, "Specify principal used " +
-        "by the job under a secured environment");
-    options.addOption(CliConstants.DISTRIBUTE_KEYTAB, false, "Distribute " +
-        "local keytab to cluster machines for service authentication. " +
-        "If not specified, pre-distributed keytab of which path specified by" +
-        " parameter" + CliConstants.KEYTAB + " on cluster machines will be " +
-        "used");
-    options.addOption("h", "help", false, "Print help");
-    options.addOption("insecure", false, "Cluster is not Kerberos enabled.");
-    options.addOption("conf", true,
-        "User specified configuration, as key=val pairs.");
-    return options;
-  }
-
-  private void addWorkerOptions(Options options) {
-    options.addOption(CliConstants.N_WORKERS, true,
-        getNumberOfServiceMessage(WORKER, 1) +
-            CAN_BE_USED_WITH_TF_PYTORCH);
-    options.addOption(CliConstants.WORKER_DOCKER_IMAGE, true,
-        getDockerImageMessage(WORKER) +
-            CAN_BE_USED_WITH_TF_PYTORCH);
-    options.addOption(CliConstants.WORKER_LAUNCH_CMD, true,
-        getLaunchCommandMessage(WORKER) +
-            CAN_BE_USED_WITH_TF_PYTORCH);
-    options.addOption(CliConstants.WORKER_RES, true,
-        getServiceResourceMessage(WORKER) +
-            CAN_BE_USED_WITH_TF_PYTORCH);
-  }
-
-  private void addPSOptions(Options options) {
-    options.addOption(CliConstants.N_PS, true,
-        getNumberOfServiceMessage("PS", 0) +
-            TENSORFLOW_ONLY);
-    options.addOption(CliConstants.PS_DOCKER_IMAGE, true,
-        getDockerImageMessage(PS) +
-            TENSORFLOW_ONLY);
-    options.addOption(CliConstants.PS_LAUNCH_CMD, true,
-        getLaunchCommandMessage("PS") +
-            TENSORFLOW_ONLY);
-    options.addOption(CliConstants.PS_RES, true,
-        getServiceResourceMessage("PS") +
-            TENSORFLOW_ONLY);
-  }
-
-  private void addTensorboardOptions(Options options) {
-    options.addOption(CliConstants.TENSORBOARD, false,
-        "Should we run TensorBoard for this job? " +
-            "By default, TensorBoard is disabled." +
-            TENSORFLOW_ONLY);
-    options.addOption(CliConstants.TENSORBOARD_RESOURCES, true,
-        "Specifies resources of Tensorboard. The default resource is: "
-            + CliConstants.TENSORBOARD_DEFAULT_RESOURCES + "." +
-            TENSORFLOW_ONLY);
-    options.addOption(CliConstants.TENSORBOARD_DOCKER_IMAGE, true,
-        getDockerImageMessage(TENSORBOARD));
-  }
-
-  private String getLaunchCommandMessage(String service) {
-    return String.format("Launch command of the %s, arguments will be "
-        + "directly used to launch the %s", service, service);
-  }
-
-  private String getServiceResourceMessage(String serviceType) {
-    return String.format("Resource of each %s process, for example: "
-        + "memory-mb=2048,vcores=2,yarn.io/gpu=2", serviceType);
-  }
-
-  private String getNumberOfServiceMessage(String serviceType,
-      int defaultValue) {
-    return String.format("Number of %s processes for the job. " +
-        "The default value is %d.", serviceType, defaultValue);
-  }
-
-  private String getDockerImageMessage(String serviceType) {
-    return String.format("Specifies docker image for the %s process. " +
-            "When not specified, %s uses --%s as a default value.",
-        serviceType, serviceType, CliConstants.DOCKER_IMAGE);
-  }
-
-  private void parseCommandLineAndGetRunJobParameters(String[] args)
-      throws ParseException, IOException, YarnException {
-    try {
-      GnuParser parser = new GnuParser();
-      CommandLine cli = parser.parse(options, args);
-      parametersHolder = createParametersHolder(cli);
-      parametersHolder.updateParameters(clientContext);
-    } catch (ParseException e) {
-      LOG.error("Exception in parse: {}", e.getMessage());
-      printUsages();
-      throw e;
-    }
-  }
-
-  private ParametersHolder createParametersHolder(CommandLine cli)
-      throws ParseException, YarnException {
-    String yamlConfigFile =
-        cli.getOptionValue(CliConstants.YAML_CONFIG);
-    if (yamlConfigFile != null) {
-      YamlConfigFile yamlConfig = readYamlConfigFile(yamlConfigFile);
-      checkYamlConfig(yamlConfigFile, yamlConfig);
-      LOG.info("Using YAML configuration!");
-      return ParametersHolder.createWithCmdLineAndYaml(cli, yamlConfig,
-          Command.RUN_JOB);
-    } else {
-      LOG.info("Using CLI configuration!");
-      return ParametersHolder.createWithCmdLine(cli, Command.RUN_JOB);
-    }
-  }
-
-  private void checkYamlConfig(String yamlConfigFile,
-      YamlConfigFile yamlConfig) {
-    if (yamlConfig == null) {
-      throw new YamlParseException(String.format(
-          YAML_PARSE_FAILED + ", file is empty: %s", yamlConfigFile));
-    } else if (yamlConfig.getConfigs() == null) {
-      throw new YamlParseException(String.format(YAML_PARSE_FAILED +
-          ", config section should be defined, but it cannot be found in " +
-          "YAML file '%s'!", yamlConfigFile));
-    }
-  }
-
-  private YamlConfigFile readYamlConfigFile(String filename) {
-    Constructor constructor = new Constructor(YamlConfigFile.class);
-    constructor.setPropertyUtils(new UnderscoreConverterPropertyUtils());
-    try {
-      LOG.info("Reading YAML configuration from file: {}", filename);
-      Yaml yaml = new Yaml(constructor);
-      return yaml.loadAs(FileUtils.openInputStream(new File(filename)),
-          YamlConfigFile.class);
-    } catch (FileNotFoundException e) {
-      logExceptionOfYamlParse(filename, e);
-      throw new YamlParseException(YAML_PARSE_FAILED +
-          ", file does not exist!");
-    } catch (Exception e) {
-      logExceptionOfYamlParse(filename, e);
-      throw new YamlParseException(
-          String.format(YAML_PARSE_FAILED + ", details: %s", e.getMessage()));
-    }
-  }
-
-  private void logExceptionOfYamlParse(String filename, Exception e) {
-    LOG.error(String.format("Exception while parsing YAML file %s", filename),
-        e);
-  }
-
-  private void storeJobInformation(RunJobParameters parameters,
-      ApplicationId applicationId, String[] args) throws IOException {
-    String jobName = parameters.getName();
-    Map<String, String> jobInfo = new HashMap<>();
-    jobInfo.put(StorageKeyConstants.JOB_NAME, jobName);
-    jobInfo.put(StorageKeyConstants.APPLICATION_ID, applicationId.toString());
-
-    if (parameters.getCheckpointPath() != null) {
-      jobInfo.put(StorageKeyConstants.CHECKPOINT_PATH,
-          parameters.getCheckpointPath());
-    }
-    if (parameters.getInputPath() != null) {
-      jobInfo.put(StorageKeyConstants.INPUT_PATH,
-          parameters.getInputPath());
-    }
-    if (parameters.getSavedModelPath() != null) {
-      jobInfo.put(StorageKeyConstants.SAVED_MODEL_PATH,
-          parameters.getSavedModelPath());
-    }
-
-    String joinedArgs = String.join(" ", args);
-    jobInfo.put(StorageKeyConstants.JOB_RUN_ARGS, joinedArgs);
-    clientContext.getRuntimeFactory().getSubmarineStorage().addNewJob(jobName,
-        jobInfo);
-  }
-
-  @Override
-  public int run(String[] args)
-      throws ParseException, IOException, YarnException, SubmarineException {
-    if (CliUtils.argsForHelp(args)) {
-      printUsages();
-      return 0;
-    }
-
-    parseCommandLineAndGetRunJobParameters(args);
-    ApplicationId applicationId = jobSubmitter.submitJob(parametersHolder);
-    RunJobParameters parameters =
-        (RunJobParameters) parametersHolder.getParameters();
-    storeJobInformation(parameters, applicationId, args);
-    if (parameters.isWaitJobFinish()) {
-      this.jobMonitor.waitTrainingFinal(parameters.getName());
-    }
-
-    return 0;
-  }
-
-  @VisibleForTesting
-  public JobSubmitter getJobSubmitter() {
-    return jobSubmitter;
-  }
-
-  @VisibleForTesting
-  public RunJobParameters getRunJobParameters() {
-    return (RunJobParameters) parametersHolder.getParameters();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/package-info.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/package-info.java
deleted file mode 100644
index c714e9f98eee..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/package-info.java
+++ /dev/null
@@ -1,19 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains classes that are related to the run job command.
- */
-package org.apache.hadoop.yarn.submarine.client.cli.runjob;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/ClientContext.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/ClientContext.java
deleted file mode 100644
index 055b3c653767..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/ClientContext.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.client.api.YarnClient;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineConfiguration;
-import org.apache.hadoop.yarn.submarine.common.fs.DefaultRemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.runtimes.RuntimeFactory;
-
-public class ClientContext {
-  private Configuration yarnConf = new YarnConfiguration();
-
-  private volatile RemoteDirectoryManager remoteDirectoryManager;
-  private YarnClient yarnClient;
-  private Configuration submarineConfig;
-  private RuntimeFactory runtimeFactory;
-
-  public ClientContext() {
-    submarineConfig = new SubmarineConfiguration();
-  }
-
-  public synchronized YarnClient getOrCreateYarnClient() {
-    if (yarnClient == null) {
-      yarnClient = YarnClient.createYarnClient();
-      yarnClient.init(yarnConf);
-      yarnClient.start();
-    }
-    return yarnClient;
-  }
-
-  public Configuration getYarnConfig() {
-    return yarnConf;
-  }
-
-  public void setConfiguration(Configuration conf) {
-    this.yarnConf = conf;
-  }
-
-  public RemoteDirectoryManager getRemoteDirectoryManager() {
-    if(remoteDirectoryManager == null) {
-      synchronized (this) {
-        if(remoteDirectoryManager == null) {
-          remoteDirectoryManager = new DefaultRemoteDirectoryManager(this);
-        }
-      }
-    }
-    return remoteDirectoryManager;
-  }
-
-  public Configuration getSubmarineConfig() {
-    return submarineConfig;
-  }
-
-  public void setSubmarineConfig(Configuration submarineConfig) {
-    this.submarineConfig = submarineConfig;
-  }
-
-  public RuntimeFactory getRuntimeFactory() {
-    return runtimeFactory;
-  }
-
-  public void setRuntimeFactory(RuntimeFactory runtimeFactory) {
-    this.runtimeFactory = runtimeFactory;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/Envs.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/Envs.java
deleted file mode 100644
index a1d80dbc2941..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/Envs.java
+++ /dev/null
@@ -1,27 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common;
-
-public class Envs {
-  public static final String TASK_TYPE_ENV = "_TASK_TYPE";
-  public static final String TASK_INDEX_ENV = "_TASK_INDEX";
-
-  /*
-   * HDFS/HADOOP-related configs
-   */
-  public static final String HADOOP_HDFS_HOME = "HADOOP_HDFS_HOME";
-  public static final String JAVA_HOME = "JAVA_HOME";
-  public static final String HADOOP_CONF_DIR = "HADOOP_CONF_DIR";
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobComponentStatus.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobComponentStatus.java
deleted file mode 100644
index 699a3255d8c8..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobComponentStatus.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.common.api;
-
-/**
- * Status of component of training job
- */
-public class JobComponentStatus {
-  private String compName;
-  private long numReadyContainers = 0;
-  private long numRunningButUnreadyContainers = 0;
-  private long totalAskedContainers;
-
-  public JobComponentStatus(String compName, long nReadyContainers,
-      long nRunningButUnreadyContainers, long totalAskedContainers) {
-    this.compName = compName;
-    this.numReadyContainers = nReadyContainers;
-    this.numRunningButUnreadyContainers = nRunningButUnreadyContainers;
-    this.totalAskedContainers = totalAskedContainers;
-  }
-
-  public String getCompName() {
-    return compName;
-  }
-
-  public void setCompName(String compName) {
-    this.compName = compName;
-  }
-
-  public long getNumReadyContainers() {
-    return numReadyContainers;
-  }
-
-  public void setNumReadyContainers(long numReadyContainers) {
-    this.numReadyContainers = numReadyContainers;
-  }
-
-  public long getNumRunningButUnreadyContainers() {
-    return numRunningButUnreadyContainers;
-  }
-
-  public void setNumRunningButUnreadyContainers(
-      long numRunningButUnreadyContainers) {
-    this.numRunningButUnreadyContainers = numRunningButUnreadyContainers;
-  }
-
-  public long getTotalAskedContainers() {
-    return totalAskedContainers;
-  }
-
-  public void setTotalAskedContainers(long totalAskedContainers) {
-    this.totalAskedContainers = totalAskedContainers;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobState.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobState.java
deleted file mode 100644
index eef273a56076..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobState.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.common.api;
-
-/**
- * State of training job
- */
-public enum JobState {
-  /**
-   * Job accepted by scheduler and start running
-   */
-  RUNNING,
-
-  /**
-   * Job killed by user
-   */
-  KILLED,
-
-  /**
-   * Job failed
-   */
-  FAILED,
-
-  /**
-   * Job succeeded
-   */
-  SUCCEEDED,
-
-  /**
-   * Job paused by user
-   */
-  PAUSED;
-
-  public static boolean isFinal(JobState state) {
-    return state == KILLED || state == SUCCEEDED || state == FAILED;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobStatus.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobStatus.java
deleted file mode 100644
index 0ba9de9f211d..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobStatus.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.common.api;
-
-import java.io.PrintStream;
-import java.time.LocalDateTime;
-import java.util.List;
-
-/**
- * Status of training job.
- */
-public class JobStatus {
-
-  protected String jobName;
-  protected JobState state;
-  protected String tensorboardLink = "N/A";
-  protected List<JobComponentStatus> componentStatus;
-
-  public void nicePrint(PrintStream out) {
-    out.println(
-        "Job Name=" + this.jobName + ", status=" + state.name() + " time="
-            + LocalDateTime.now());
-    if (JobState.isFinal(this.state)) {
-      return;
-    }
-
-    if (tensorboardLink.startsWith("http")) {
-      out.println("  Tensorboard link: " + tensorboardLink);
-    }
-
-    out.println("  Components:");
-    for (JobComponentStatus comp : componentStatus) {
-      out.println("    [" + comp.getCompName() + "] Ready=" + comp
-          .getNumReadyContainers() + " + Running-But-Non-Ready=" + comp
-          .getNumRunningButUnreadyContainers() + " | Asked=" + comp
-          .getTotalAskedContainers());
-    }
-    out.println("------------------");
-  }
-
-  public JobState getState() {
-    return state;
-  }
-
-  public String getTensorboardLink() {
-    return tensorboardLink;
-  }
-
-  public List<JobComponentStatus> getComponentStatus() {
-    return componentStatus;
-  }
-
-  public String getJobName() {
-    return jobName;
-  }
-
-  public void setJobName(String jobName) {
-    this.jobName = jobName;
-  }
-
-  public void setState(JobState state) {
-    this.state = state;
-  }
-
-  public void setTensorboardLink(String tensorboardLink) {
-    this.tensorboardLink = tensorboardLink;
-  }
-
-  public void setComponentStatus(List<JobComponentStatus> componentStatus) {
-    this.componentStatus = componentStatus;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/PyTorchRole.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/PyTorchRole.java
deleted file mode 100644
index e4f4c2cd6360..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/PyTorchRole.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common.api;
-
-/**
- * Enum to represent a PyTorch Role.
- */
-public enum PyTorchRole implements Role {
-  PRIMARY_WORKER("master"),
-  WORKER("worker");
-
-  private String compName;
-
-  PyTorchRole(String compName) {
-    this.compName = compName;
-  }
-
-  public String getComponentName() {
-    return compName;
-  }
-
-  @Override
-  public String getName() {
-    return name();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/Role.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/Role.java
deleted file mode 100644
index 6dcdc66e55eb..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/Role.java
+++ /dev/null
@@ -1,25 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.common.api;
-
-/**
- * Interface for a Role.
- */
-public interface Role {
-  String getComponentName();
-  String getName();
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/Runtime.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/Runtime.java
deleted file mode 100644
index 5942c87ec39b..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/Runtime.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.common.api;
-
-import com.google.common.collect.Lists;
-import java.util.List;
-import java.util.stream.Collectors;
-
-/**
- * Represents the type of Runtime.
- */
-public enum Runtime {
-  TONY(Constants.TONY), YARN_SERVICE(Constants.YARN_SERVICE);
-
-  private String value;
-
-  Runtime(String value) {
-    this.value = value;
-  }
-
-  public String getValue() {
-    return value;
-  }
-
-  public static Runtime parseByValue(String value) {
-    for (Runtime rt : Runtime.values()) {
-      if (rt.value.equalsIgnoreCase(value)) {
-        return rt;
-      }
-    }
-    return null;
-  }
-
-  public static String getValues() {
-    List<String> values = Lists.newArrayList(Runtime.values()).stream()
-        .map(rt -> rt.value).collect(Collectors.toList());
-    return String.join(",", values);
-  }
-
-  public static class Constants {
-    public static final String TONY = "tony";
-    public static final String YARN_SERVICE = "yarnservice";
-  }
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/TensorFlowRole.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/TensorFlowRole.java
deleted file mode 100644
index 1b10f96bb520..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/TensorFlowRole.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common.api;
-
-/**
- * Enum to represent a TensorFlow Role.
- */
-public enum TensorFlowRole implements Role {
-  PRIMARY_WORKER("master"),
-  WORKER("worker"),
-  PS("ps"),
-  TENSORBOARD("tensorboard");
-
-  private String compName;
-
-  TensorFlowRole(String compName) {
-    this.compName = compName;
-  }
-
-  @Override
-  public String getComponentName() {
-    return compName;
-  }
-
-  @Override
-  public String getName() {
-    return name();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/conf/SubmarineConfiguration.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/conf/SubmarineConfiguration.java
deleted file mode 100644
index 7e70717a81b9..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/conf/SubmarineConfiguration.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common.conf;
-
-import org.apache.hadoop.conf.Configuration;
-
-public class SubmarineConfiguration extends Configuration {
-  private static final String SUBMARINE_CONFIGURATION_FILE = "submarine.xml";
-
-  public static final String SUBMARINE_CONFIGURATION_PREFIX = "submarine.";
-
-  public static final String SUBMARINE_LOCALIZATION_PREFIX =
-      SUBMARINE_CONFIGURATION_PREFIX + "localization.";
-  /**
-   * Limit the size of directory/file to be localized.
-   * To avoid exhausting local disk space,
-   * this limit both remote and local file to be localized
-   */
-  public static final String LOCALIZATION_MAX_ALLOWED_FILE_SIZE_MB =
-      SUBMARINE_LOCALIZATION_PREFIX + "max-allowed-file-size-mb";
-
-  // Default 2GB
-  public static final long DEFAULT_MAX_ALLOWED_REMOTE_URI_SIZE_MB = 2048;
-
-  public SubmarineConfiguration() {
-    this(new Configuration(false), true);
-  }
-
-  public SubmarineConfiguration(Configuration configuration) {
-    this(configuration, false);
-  }
-
-  public SubmarineConfiguration(Configuration configuration,
-      boolean loadLocalConfig) {
-    super(configuration);
-    if (loadLocalConfig) {
-      addResource(SUBMARINE_CONFIGURATION_FILE);
-    }
-  }
-
-  /*
-   * Runtime of submarine
-   */
-
-  private static final String PREFIX = "submarine.";
-
-  public static final String RUNTIME_CLASS = PREFIX + "runtime.class";
-  public static final String DEFAULT_RUNTIME_CLASS =
-      "org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceRuntimeFactory";
-
-  public void setSubmarineRuntimeClass(String runtimeClass) {
-    set(RUNTIME_CLASS, runtimeClass);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/conf/SubmarineLogs.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/conf/SubmarineLogs.java
deleted file mode 100644
index 6bb3248bb5cf..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/conf/SubmarineLogs.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common.conf;
-
-public class SubmarineLogs {
-  private static volatile boolean verbose = false;
-
-  public static boolean isVerbose() {
-    return SubmarineLogs.verbose;
-  }
-
-  public static void verboseOn() {
-    SubmarineLogs.verbose = true;
-  }
-
-  public static void verboseOff() {
-    SubmarineLogs.verbose = false;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/exception/SubmarineException.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/exception/SubmarineException.java
deleted file mode 100644
index b6a39b9fa8ed..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/exception/SubmarineException.java
+++ /dev/null
@@ -1,21 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common.exception;
-
-public class SubmarineException extends Exception {
-  public SubmarineException(String msg) {
-    super(msg);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/exception/SubmarineRuntimeException.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/exception/SubmarineRuntimeException.java
deleted file mode 100644
index 4fb74fd55ace..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/exception/SubmarineRuntimeException.java
+++ /dev/null
@@ -1,25 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common.exception;
-
-public class SubmarineRuntimeException extends RuntimeException {
-  public SubmarineRuntimeException(String s) {
-    super(s);
-  }
-
-  public SubmarineRuntimeException(String message, Throwable cause) {
-    super(message, cause);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/fs/DefaultRemoteDirectoryManager.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/fs/DefaultRemoteDirectoryManager.java
deleted file mode 100644
index bfb20da87665..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/fs/DefaultRemoteDirectoryManager.java
+++ /dev/null
@@ -1,164 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common.fs;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.yarn.submarine.client.cli.CliConstants;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-
-import java.io.File;
-import java.io.IOException;
-import java.net.URI;
-
-/**
- * Manages remote directories for staging, log, etc.
- * TODO, need to properly handle permission / name validation, etc.
- */
-public class DefaultRemoteDirectoryManager implements RemoteDirectoryManager {
-  private FileSystem fs;
-  private Configuration conf;
-
-  public DefaultRemoteDirectoryManager(ClientContext context) {
-    this.conf = context.getYarnConfig();
-    try {
-      this.fs = FileSystem.get(context.getYarnConfig());
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  @Override
-  public Path getJobStagingArea(String jobName, boolean create)
-      throws IOException {
-    Path staging = new Path(getJobRootFolder(jobName), "staging");
-    if (create) {
-      createFolderIfNotExist(staging);
-    }
-
-    // Get a file status to make sure it is a absolute path.
-    FileStatus fStatus = fs.getFileStatus(staging);
-    return fStatus.getPath();
-  }
-
-  @Override
-  public Path getJobCheckpointDir(String jobName, boolean create)
-      throws IOException {
-    Path jobDir = new Path(getJobStagingArea(jobName, create),
-        CliConstants.CHECKPOINT_PATH);
-    if (create) {
-      createFolderIfNotExist(jobDir);
-    }
-    return jobDir;
-  }
-
-  @Override
-  public Path getModelDir(String modelName, boolean create)
-      throws IOException {
-    Path modelDir = new Path(new Path("submarine", "models"), modelName);
-    if (create) {
-      createFolderIfNotExist(modelDir);
-    }
-    return modelDir;
-  }
-
-  @Override
-  public FileSystem getDefaultFileSystem() {
-    return fs;
-  }
-
-  @Override
-  public FileSystem getFileSystemByUri(String uri) throws IOException {
-    return FileSystem.get(URI.create(uri), conf);
-  }
-
-  @Override
-  public Path getUserRootFolder() throws IOException {
-    Path rootPath = new Path("submarine", "jobs");
-    createFolderIfNotExist(rootPath);
-    // Get a file status to make sure it is a absolute path.
-    FileStatus fStatus = fs.getFileStatus(rootPath);
-    return fStatus.getPath();
-  }
-
-  @Override
-  public boolean isDir(String uri) throws IOException {
-    if (isRemote(uri)) {
-      return getFileSystemByUri(uri).getFileStatus(new Path(uri)).isDirectory();
-    }
-    return new File(uri).isDirectory();
-  }
-
-  @Override
-  public boolean isRemote(String uri) {
-    String scheme = new Path(uri).toUri().getScheme();
-    if (null == scheme) {
-      return false;
-    }
-    return !scheme.startsWith("file://");
-  }
-
-  @Override
-  public boolean copyRemoteToLocal(String remoteUri, String localUri)
-      throws IOException {
-    // Delete old to avoid failure in FileUtil.copy
-    File old = new File(localUri);
-    if (old.exists()) {
-      if (!FileUtil.fullyDelete(old)) {
-        throw new IOException("Failed to delete dir:"
-            + old.getAbsolutePath());
-      }
-    }
-    return FileUtil.copy(getFileSystemByUri(remoteUri), new Path(remoteUri),
-        new File(localUri), false,
-        conf);
-  }
-
-  @Override
-  public boolean existsRemoteFile(Path url) throws IOException {
-    return getFileSystemByUri(url.toUri().toString()).exists(url);
-  }
-
-  @Override
-  public FileStatus getRemoteFileStatus(Path url) throws IOException {
-    return getFileSystemByUri(url.toUri().toString()).getFileStatus(url);
-  }
-
-  @Override
-  public long getRemoteFileSize(String uri) throws IOException {
-    return getFileSystemByUri(uri)
-        .getContentSummary(new Path(uri)).getSpaceConsumed();
-  }
-
-  private Path getJobRootFolder(String jobName) throws IOException {
-    Path userRoot = getUserRootFolder();
-    Path jobRootPath = new Path(userRoot, jobName);
-    createFolderIfNotExist(jobRootPath);
-    // Get a file status to make sure it is a absolute path.
-    FileStatus fStatus = fs.getFileStatus(jobRootPath);
-    return fStatus.getPath();
-  }
-
-  private void createFolderIfNotExist(Path path) throws IOException {
-    if (!fs.exists(path)) {
-      if (!fs.mkdirs(path)) {
-        throw new IOException("Failed to create folder=" + path);
-      }
-    }
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/fs/RemoteDirectoryManager.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/fs/RemoteDirectoryManager.java
deleted file mode 100644
index 5f71d19d3755..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/fs/RemoteDirectoryManager.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common.fs;
-
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-import java.io.IOException;
-
-public interface RemoteDirectoryManager {
-  Path getJobStagingArea(String jobName, boolean create) throws IOException;
-
-  Path getJobCheckpointDir(String jobName, boolean create) throws IOException;
-
-  Path getModelDir(String modelName, boolean create) throws IOException;
-
-  FileSystem getDefaultFileSystem() throws IOException;
-
-  FileSystem getFileSystemByUri(String uri) throws IOException;
-
-  Path getUserRootFolder() throws IOException;
-
-  boolean isDir(String uri) throws IOException;
-
-  boolean isRemote(String uri) throws IOException;
-
-  boolean copyRemoteToLocal(String remoteUri, String localUri)
-      throws IOException;
-
-  boolean existsRemoteFile(Path uri) throws IOException;
-
-  FileStatus getRemoteFileStatus(Path uri) throws IOException;
-
-  long getRemoteFileSize(String uri) throws IOException;
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/ResourceUtils.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/ResourceUtils.java
deleted file mode 100644
index 375ae354ad9c..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/ResourceUtils.java
+++ /dev/null
@@ -1,332 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.common.resource;
-
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineRuntimeException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.lang.reflect.InvocationTargetException;
-import java.lang.reflect.Method;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-/**
- * This class implements some methods with the almost the same logic as
- * org.apache.hadoop.yarn.util.resource.ResourceUtils of hadoop 3.3.
- * If the hadoop dependencies are upgraded to 3.3, this class can be refactored
- * with org.apache.hadoop.yarn.util.resource.ResourceUtils.
- */
-public final class ResourceUtils {
-
-  private final static String RES_PATTERN = "^[^=]+=\\d+\\s?\\w*$";
-  private final static String SET_RESOURCE_VALUE_METHOD = "setResourceValue";
-  private final static String SET_MEMORY_SIZE_METHOD = "setMemorySize";
-  private final static String DEPRECATED_SET_MEMORY_SIZE_METHOD =
-      "setMemory";
-  private final static String GET_MEMORY_SIZE_METHOD = "getMemorySize";
-  private final static String DEPRECATED_GET_MEMORY_SIZE_METHOD =
-      "getMemory";
-  private final static String GET_RESOURCE_VALUE_METHOD = "getResourceValue";
-  private final static String GET_RESOURCE_TYPE_METHOD =
-      "getResourcesTypeInfo";
-  private final static String REINITIALIZE_RESOURCES_METHOD =
-      "reinitializeResources";
-  public static final String MEMORY_URI = "memory-mb";
-  public static final String VCORES_URI = "vcores";
-  public static final String GPU_URI = "yarn.io/gpu";
-  public static final String FPGA_URI = "yarn.io/fpga";
-
-  private static final Logger LOG =
-      LoggerFactory.getLogger(ResourceUtils.class);
-
-  private ResourceUtils() {}
-
-  public static Resource createResourceFromString(String resourceStr) {
-    Map<String, Long> typeToValue = parseResourcesString(resourceStr);
-    Resource resource = Resource.newInstance(0, 0);
-    for (Map.Entry<String, Long> entry : typeToValue.entrySet()) {
-      if(entry.getKey().equals(VCORES_URI)) {
-        resource.setVirtualCores(entry.getValue().intValue());
-        continue;
-      } else if (entry.getKey().equals(MEMORY_URI)) {
-        setMemorySize(resource, entry.getValue());
-        continue;
-      }
-      setResource(resource, entry.getKey(), entry.getValue().intValue());
-    }
-    return resource;
-  }
-
-  private static Map<String, Long> parseResourcesString(String resourcesStr) {
-    Map<String, Long> resources = new HashMap<>();
-    String[] pairs = resourcesStr.trim().split(",");
-    for (String resource : pairs) {
-      resource = resource.trim();
-      if (!resource.matches(RES_PATTERN)) {
-        throw new IllegalArgumentException("\"" + resource + "\" is not a "
-            + "valid resource type/amount pair. "
-            + "Please provide key=amount pairs separated by commas.");
-      }
-      String[] splits = resource.split("=");
-      String key = splits[0], value = splits[1];
-      String units = getUnits(value);
-
-      String valueWithoutUnit = value.substring(0,
-          value.length()- units.length()).trim();
-      long resourceValue = Long.parseLong(valueWithoutUnit);
-
-      // Convert commandline unit to standard YARN unit.
-      if (units.equals("M") || units.equals("m")) {
-        units = "Mi";
-      } else if (units.equals("G") || units.equals("g")) {
-        units = "Gi";
-      } else if (!units.isEmpty()){
-        throw new IllegalArgumentException("Acceptable units are M/G or empty");
-      }
-
-      // special handle memory-mb and memory
-      if (key.equals(MEMORY_URI)) {
-        if (!units.isEmpty()) {
-          resourceValue = UnitsConversionUtil.convert(units, "Mi",
-              resourceValue);
-        }
-      }
-
-      if (key.equals("memory")) {
-        key = MEMORY_URI;
-        resourceValue = UnitsConversionUtil.convert(units, "Mi",
-            resourceValue);
-      }
-
-      // special handle gpu
-      if (key.equals("gpu")) {
-        key = GPU_URI;
-      }
-
-      // special handle fpga
-      if (key.equals("fpga")) {
-        key = FPGA_URI;
-      }
-
-      resources.put(key, resourceValue);
-    }
-    return resources;
-  }
-
-  /**
-   * As hadoop 2.9.2 and lower don't support resources except cpu and memory.
-   * Use reflection to set GPU or other resources for compatibility with
-   * hadoop 2.9.2
-   */
-  public static void setResource(Resource resource, String resourceName,
-                                 int resourceValue) {
-    try {
-      Method method = resource.getClass().getMethod(SET_RESOURCE_VALUE_METHOD,
-          String.class, long.class);
-      method.invoke(resource, resourceName, resourceValue);
-    } catch (NoSuchMethodException e) {
-      LOG.error("There is no '" + SET_RESOURCE_VALUE_METHOD + "' API in this" +
-          "version of YARN", e);
-      throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-    } catch (IllegalAccessException | InvocationTargetException e) {
-      LOG.error("Failed to invoke '" + SET_RESOURCE_VALUE_METHOD +
-          "' method to set GPU resources", e);
-      throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-    }
-    return;
-  }
-
-  public static void setMemorySize(Resource resource, Long memorySize) {
-    boolean useWithIntParameter = false;
-    // For hadoop 2.9.2 and above
-    try {
-      Method method = resource.getClass().getMethod(SET_MEMORY_SIZE_METHOD,
-          long.class);
-      method.setAccessible(true);
-      method.invoke(resource, memorySize);
-    } catch (NoSuchMethodException nsme) {
-      LOG.info("There is no '" + SET_MEMORY_SIZE_METHOD + "(long)' API in" +
-          " this version of YARN");
-      useWithIntParameter = true;
-    } catch (IllegalAccessException | InvocationTargetException e) {
-      LOG.error("Failed to invoke '" + SET_MEMORY_SIZE_METHOD +
-          "' method", e);
-      throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-    }
-    // For hadoop 2.7.3
-    if (useWithIntParameter) {
-      try {
-        LOG.info("Trying to use '" + DEPRECATED_SET_MEMORY_SIZE_METHOD +
-            "(int)' API for this version of YARN");
-        Method method = resource.getClass().getMethod(
-            DEPRECATED_SET_MEMORY_SIZE_METHOD, int.class);
-        method.invoke(resource, memorySize.intValue());
-      } catch (NoSuchMethodException e) {
-        LOG.error("There is no '" + DEPRECATED_SET_MEMORY_SIZE_METHOD +
-            "(int)' API in this version of YARN", e);
-        throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-      } catch (IllegalAccessException | InvocationTargetException e) {
-        LOG.error("Failed to invoke '" + DEPRECATED_SET_MEMORY_SIZE_METHOD +
-            "' method", e);
-        throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-      }
-    }
-  }
-
-  public static long getMemorySize(Resource resource) {
-    boolean useWithIntParameter = false;
-    long memory = 0;
-    // For hadoop 2.9.2 and above
-    try {
-      Method method = resource.getClass().getMethod(GET_MEMORY_SIZE_METHOD);
-      method.setAccessible(true);
-      memory = (long) method.invoke(resource);
-    } catch (NoSuchMethodException e) {
-      LOG.info("There is no '" + GET_MEMORY_SIZE_METHOD + "' API in" +
-          " this version of YARN");
-      useWithIntParameter = true;
-    } catch (IllegalAccessException | InvocationTargetException e) {
-      LOG.error("Failed to invoke '" + GET_MEMORY_SIZE_METHOD +
-          "' method", e);
-      throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-    }
-    // For hadoop 2.7.3
-    if (useWithIntParameter) {
-      try {
-        LOG.info("Trying to use '" + DEPRECATED_GET_MEMORY_SIZE_METHOD +
-            "' API for this version of YARN");
-        Method method = resource.getClass().getMethod(
-            DEPRECATED_GET_MEMORY_SIZE_METHOD);
-        method.setAccessible(true);
-        memory = ((Integer) method.invoke(resource)).longValue();
-      } catch (NoSuchMethodException e) {
-        LOG.error("There is no '" + DEPRECATED_GET_MEMORY_SIZE_METHOD +
-                "' API in this version of YARN", e);
-        throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-      } catch (IllegalAccessException | InvocationTargetException e) {
-        LOG.error("Failed to invoke '" + DEPRECATED_GET_MEMORY_SIZE_METHOD +
-            "' method", e);
-        throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-      }
-    }
-    return memory;
-  }
-
-  /**
-   * As hadoop 2.9.2 and lower don't support resources except cpu and memory.
-   * Use reflection to set GPU or other resources for compatibility with
-   * hadoop 2.9.2
-   */
-  public static long getResourceValue(Resource resource, String resourceName) {
-    long resourceValue = 0;
-    try {
-      Method method = resource.getClass().getMethod(GET_RESOURCE_VALUE_METHOD,
-          String.class);
-      Object value = method.invoke(resource, resourceName);
-      resourceValue = (long) value;
-    } catch (NoSuchMethodException e) {
-      LOG.info("There is no '" + GET_RESOURCE_VALUE_METHOD + "' API in this" +
-          " version of YARN");
-    } catch (InvocationTargetException e) {
-      if (e.getTargetException().getClass().getName().equals(
-          "org.apache.hadoop.yarn.exceptions.ResourceNotFoundException")) {
-        LOG.info("Not found resource " + resourceName);
-      } else {
-        LOG.info("Failed to invoke '" + GET_RESOURCE_VALUE_METHOD + "'" +
-            " method to get resource " + resourceName);
-        throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-      }
-    } catch (IllegalAccessException | ClassCastException e) {
-      LOG.error("Failed to invoke '" + GET_RESOURCE_VALUE_METHOD +
-          "' method to get resource " + resourceName, e);
-      throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-    }
-    return resourceValue;
-  }
-
-  /**
-   * As hadoop 2.9.2 and lower don't support resources except cpu and memory.
-   * Use reflection to add GPU or other resources for compatibility with
-   * hadoop 2.9.2
-   */
-  public static void configureResourceType(String resrouceName) {
-    Class resourceTypeInfo;
-    try{
-      resourceTypeInfo = Class.forName(
-          "org.apache.hadoop.yarn.api.records.ResourceTypeInfo");
-      Class resourceUtils = Class.forName(
-          "org.apache.hadoop.yarn.util.resource.ResourceUtils");
-      Method method = resourceUtils.getMethod(GET_RESOURCE_TYPE_METHOD);
-      Object resTypes = method.invoke(null);
-
-      Method resourceTypeInstance = resourceTypeInfo.getMethod("newInstance",
-          String.class, String.class);
-      Object resourceType = resourceTypeInstance.invoke(null, resrouceName, "");
-      ((ArrayList)resTypes).add(resourceType);
-
-      Method reInitialMethod = resourceUtils.getMethod(
-          REINITIALIZE_RESOURCES_METHOD, List.class);
-      reInitialMethod.invoke(null, resTypes);
-
-    } catch (ClassNotFoundException e) {
-      LOG.info("There is no specified class API in this" +
-          " version of YARN");
-      LOG.info(e.getMessage());
-      throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-    } catch (NoSuchMethodException nsme) {
-      LOG.info("There is no '" + GET_RESOURCE_VALUE_METHOD + "' API in this" +
-          " version of YARN");
-    } catch (IllegalAccessException | InvocationTargetException e) {
-      LOG.info("Failed to invoke 'configureResourceType' method ", e);
-      throw new SubmarineRuntimeException(e.getMessage(), e.getCause());
-    }
-  }
-
-  private static String getUnits(String resourceValue) {
-    return parseResourceValue(resourceValue)[0];
-  }
-
-  /**
-   * Extract unit and actual value from resource value.
-   * @param resourceValue Value of the resource
-   * @return Array containing unit and value. [0]=unit, [1]=value
-   * @throws IllegalArgumentException if units contain non alpha characters
-   */
-  private static String[] parseResourceValue(String resourceValue) {
-    String[] resource = new String[2];
-    int i = 0;
-    for (; i < resourceValue.length(); i++) {
-      if (Character.isAlphabetic(resourceValue.charAt(i))) {
-        break;
-      }
-    }
-    String units = resourceValue.substring(i);
-
-    if (StringUtils.isAlpha(units) || units.equals("")) {
-      resource[0] = units;
-      resource[1] = resourceValue.substring(0, i);
-      return resource;
-    } else {
-      throw new IllegalArgumentException("Units '" + units + "'"
-          + " contains non alphabet characters, which is not allowed.");
-    }
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/UnitsConversionUtil.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/UnitsConversionUtil.java
deleted file mode 100644
index 8a8635a180ab..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/UnitsConversionUtil.java
+++ /dev/null
@@ -1,164 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.common.resource;
-
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-/**
- * Almost the same logic as UnitsConversionUtil[YARN-4081]. If the dependencies
- * are upgraded to hadoop 3.*, this class can be replaced.
- */
-public final class UnitsConversionUtil {
-
-  private UnitsConversionUtil() {}
-
-  /**
-   * Helper class for encapsulating conversion values.
-   */
-  public static class Converter {
-    private long numerator;
-    private long denominator;
-
-    Converter(long n, long d) {
-      this.numerator = n;
-      this.denominator = d;
-    }
-  }
-
-  private static final String[] UNITS = {"p", "n", "u", "m", "", "k", "M", "G",
-      "T", "P", "Ki", "Mi", "Gi", "Ti", "Pi"};
-  private static final List<String> SORTED_UNITS = Arrays.asList(UNITS);
-  public static final Set<String> KNOWN_UNITS = createKnownUnitsSet();
-  private static final Converter PICO =
-      new Converter(1L, 1000L * 1000L * 1000L * 1000L);
-  private static final Converter NANO =
-      new Converter(1L, 1000L * 1000L * 1000L);
-  private static final Converter MICRO = new Converter(1L, 1000L * 1000L);
-  private static final Converter MILLI = new Converter(1L, 1000L);
-  private static final Converter BASE = new Converter(1L, 1L);
-  private static final Converter KILO = new Converter(1000L, 1L);
-  private static final Converter MEGA = new Converter(1000L * 1000L, 1L);
-  private static final Converter GIGA =
-      new Converter(1000L * 1000L * 1000L, 1L);
-  private static final Converter TERA =
-      new Converter(1000L * 1000L * 1000L * 1000L, 1L);
-  private static final Converter PETA =
-      new Converter(1000L * 1000L * 1000L * 1000L * 1000L, 1L);
-
-  private static final Converter KILO_BINARY = new Converter(1024L, 1L);
-  private static final Converter MEGA_BINARY = new Converter(1024L * 1024L, 1L);
-  private static final Converter GIGA_BINARY =
-      new Converter(1024L * 1024L * 1024L, 1L);
-  private static final Converter TERA_BINARY =
-      new Converter(1024L * 1024L * 1024L * 1024L, 1L);
-  private static final Converter PETA_BINARY =
-      new Converter(1024L * 1024L * 1024L * 1024L * 1024L, 1L);
-
-  private static Set<String> createKnownUnitsSet() {
-    Set<String> ret = new HashSet<>();
-    ret.addAll(Arrays.asList(UNITS));
-    return ret;
-  }
-
-  private static Converter getConverter(String unit) {
-    switch (unit) {
-    case "p":
-      return PICO;
-    case "n":
-      return NANO;
-    case "u":
-      return MICRO;
-    case "m":
-      return MILLI;
-    case "":
-      return BASE;
-    case "k":
-      return KILO;
-    case "M":
-      return MEGA;
-    case "G":
-      return GIGA;
-    case "T":
-      return TERA;
-    case "P":
-      return PETA;
-    case "Ki":
-      return KILO_BINARY;
-    case "Mi":
-      return MEGA_BINARY;
-    case "Gi":
-      return GIGA_BINARY;
-    case "Ti":
-      return TERA_BINARY;
-    case "Pi":
-      return PETA_BINARY;
-    default:
-      throw new IllegalArgumentException(
-          "Unknown unit '" + unit + "'. Known units are " + KNOWN_UNITS);
-    }
-  }
-
-  /**
-   * Converts a value from one unit to another. Supported units can be obtained
-   * by inspecting the KNOWN_UNITS set.
-   *
-   * @param fromUnit  the unit of the from value
-   * @param toUnit    the target unit
-   * @param fromValue the value you wish to convert
-   * @return the value in toUnit
-   */
-  public static long convert(String fromUnit, String toUnit, long fromValue) {
-    if (toUnit == null || fromUnit == null) {
-      throw new IllegalArgumentException("One or more arguments are null");
-    }
-
-    if (fromUnit.equals(toUnit)) {
-      return fromValue;
-    }
-    Converter fc = getConverter(fromUnit);
-    Converter tc = getConverter(toUnit);
-    long numerator = fc.numerator * tc.denominator;
-    long denominator = fc.denominator * tc.numerator;
-    long numeratorMultiplierLimit = Long.MAX_VALUE / numerator;
-    if (numerator < denominator) {
-      if (numeratorMultiplierLimit < fromValue) {
-        String overflowMsg =
-            "Converting " + fromValue + " from '" + fromUnit + "' to '" + toUnit
-                + "' will result in an overflow of Long";
-        throw new IllegalArgumentException(overflowMsg);
-      }
-      return (fromValue * numerator) / denominator;
-    }
-    if (numeratorMultiplierLimit > fromValue) {
-      return (numerator * fromValue) / denominator;
-    }
-    long tmp = numerator / denominator;
-    if ((Long.MAX_VALUE / tmp) < fromValue) {
-      String overflowMsg =
-          "Converting " + fromValue + " from '" + fromUnit + "' to '" + toUnit
-              + "' will result in an overflow of Long";
-      throw new IllegalArgumentException(overflowMsg);
-    }
-    return fromValue * tmp;
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/package-info.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/package-info.java
deleted file mode 100644
index 144316b12f7b..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/package-info.java
+++ /dev/null
@@ -1,19 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains resource utility classes.
- */
-package org.apache.hadoop.yarn.submarine.common.resource;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/RuntimeFactory.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/RuntimeFactory.java
deleted file mode 100644
index 476cded75f9f..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/RuntimeFactory.java
+++ /dev/null
@@ -1,103 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes;
-
-import com.google.common.annotations.VisibleForTesting;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineConfiguration;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineRuntimeException;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobMonitor;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.common.SubmarineStorage;
-
-import java.lang.reflect.InvocationTargetException;
-
-public abstract class RuntimeFactory {
-  protected ClientContext clientContext;
-  private JobSubmitter jobSubmitter;
-  private JobMonitor jobMonitor;
-  private SubmarineStorage submarineStorage;
-
-  public RuntimeFactory(ClientContext clientContext) {
-    this.clientContext = clientContext;
-  }
-
-  public static RuntimeFactory getRuntimeFactory(
-      ClientContext clientContext) {
-    Configuration submarineConfiguration =
-        clientContext.getSubmarineConfig();
-    String runtimeClass = submarineConfiguration.get(
-        SubmarineConfiguration.RUNTIME_CLASS,
-        SubmarineConfiguration.DEFAULT_RUNTIME_CLASS);
-
-    try {
-      Class<?> runtimeClazz = Class.forName(runtimeClass);
-      if (RuntimeFactory.class.isAssignableFrom(runtimeClazz)) {
-        return (RuntimeFactory) runtimeClazz.getConstructor(ClientContext.class).newInstance(clientContext);
-      } else {
-        throw new SubmarineRuntimeException("Class: " + runtimeClass
-            + " not instance of " + RuntimeFactory.class.getCanonicalName());
-      }
-    } catch (ClassNotFoundException | IllegalAccessException |
-             InstantiationException | NoSuchMethodException |
-             InvocationTargetException e) {
-      throw new SubmarineRuntimeException(
-          "Could not instantiate RuntimeFactory: " + runtimeClass, e);
-    }
-  }
-
-  protected abstract JobSubmitter internalCreateJobSubmitter();
-
-  protected abstract JobMonitor internalCreateJobMonitor();
-
-  protected abstract SubmarineStorage internalCreateSubmarineStorage();
-
-  public synchronized JobSubmitter getJobSubmitterInstance() {
-    if (jobSubmitter == null) {
-      jobSubmitter = internalCreateJobSubmitter();
-    }
-    return jobSubmitter;
-  }
-
-  public synchronized JobMonitor getJobMonitorInstance() {
-    if (jobMonitor == null) {
-      jobMonitor = internalCreateJobMonitor();
-    }
-    return jobMonitor;
-  }
-
-  public synchronized SubmarineStorage getSubmarineStorage() {
-    if (submarineStorage == null) {
-      submarineStorage = internalCreateSubmarineStorage();
-    }
-    return submarineStorage;
-  }
-
-  @VisibleForTesting
-  public synchronized void setJobSubmitterInstance(JobSubmitter jobSubmitter) {
-    this.jobSubmitter = jobSubmitter;
-  }
-
-  @VisibleForTesting
-  public synchronized void setJobMonitorInstance(JobMonitor jobMonitor) {
-    this.jobMonitor = jobMonitor;
-  }
-
-  @VisibleForTesting
-  public synchronized void setSubmarineStorage(SubmarineStorage storage) {
-    this.submarineStorage = storage;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/FSBasedSubmarineStorageImpl.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/FSBasedSubmarineStorageImpl.java
deleted file mode 100644
index fb7c12f38a77..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/FSBasedSubmarineStorageImpl.java
+++ /dev/null
@@ -1,102 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-
-package org.apache.hadoop.yarn.submarine.runtimes.common;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-
-import java.io.IOException;
-import java.io.ObjectInput;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutput;
-import java.io.ObjectOutputStream;
-import java.util.Map;
-
-/**
- * A super naive FS-based storage.
- */
-public class FSBasedSubmarineStorageImpl extends SubmarineStorage {
-  RemoteDirectoryManager rdm;
-
-  public FSBasedSubmarineStorageImpl(ClientContext clientContext) {
-    rdm = clientContext.getRemoteDirectoryManager();
-  }
-
-  @Override
-  public void addNewJob(String jobName, Map<String, String> jobInfo)
-      throws IOException {
-    Path jobInfoPath = getJobInfoPath(jobName, true);
-    FSDataOutputStream fos = rdm.getDefaultFileSystem().create(jobInfoPath);
-    serializeMap(fos, jobInfo);
-  }
-
-  @Override
-  public Map<String, String> getJobInfoByName(String jobName)
-      throws IOException {
-    Path jobInfoPath = getJobInfoPath(jobName, false);
-    FSDataInputStream fis = rdm.getDefaultFileSystem().open(jobInfoPath);
-    return deserializeMap(fis);
-  }
-
-  @Override
-  public void addNewModel(String modelName, String version,
-      Map<String, String> modelInfo) throws IOException {
-    Path modelInfoPath = getModelInfoPath(modelName, version, true);
-    FSDataOutputStream fos = rdm.getDefaultFileSystem().create(modelInfoPath);
-    serializeMap(fos, modelInfo);
-  }
-
-  @Override
-  public Map<String, String> getModelInfoByName(String modelName,
-      String version) throws IOException {
-    Path modelInfoPath = getModelInfoPath(modelName, version, false);
-    FSDataInputStream fis = rdm.getDefaultFileSystem().open(modelInfoPath);
-    return deserializeMap(fis);
-  }
-
-  private Path getModelInfoPath(String modelName, String version, boolean create)
-      throws IOException {
-    Path modelDir = rdm.getModelDir(modelName, create);
-    return new Path(modelDir, version + ".info");
-  }
-
-  private void serializeMap(FSDataOutputStream fos, Map<String, String> map)
-      throws IOException {
-    ObjectOutput oo = new ObjectOutputStream(fos);
-    oo.writeObject(map);
-    oo.close();
-  }
-
-  private Map<String, String> deserializeMap(FSDataInputStream fis)
-      throws IOException {
-    ObjectInput oi = new ObjectInputStream(fis);
-    Map<String, String> newMap;
-    try {
-      newMap = (Map<String, String>) oi.readObject();
-    } catch (ClassNotFoundException e) {
-      throw new IOException(e);
-    }
-    return newMap;
-  }
-
-  private Path getJobInfoPath(String jobName, boolean create) throws IOException {
-    Path path = rdm.getJobStagingArea(jobName, create);
-    return new Path(path, "job.info");
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/JobMonitor.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/JobMonitor.java
deleted file mode 100644
index 35e21fc8d370..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/JobMonitor.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.common;
-
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.JobState;
-import org.apache.hadoop.yarn.submarine.common.api.JobStatus;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-
-/**
- * Monitor status of job(s)
- */
-public abstract class JobMonitor {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(JobMonitor.class);
-  protected ClientContext clientContext;
-
-  public JobMonitor(ClientContext clientContext) {
-    this.clientContext = clientContext;
-  }
-
-  /**
-   * Returns status of training job.
-   *
-   * @param jobName name of job
-   * @return job status
-   * @throws IOException anything else happens
-   * @throws YarnException anything related to YARN happens
-   */
-  public abstract JobStatus getTrainingJobStatus(String jobName)
-      throws IOException, YarnException;
-
-  /**
-   * Cleanup AppAdminClient, etc.
-   */
-  public void cleanup() throws IOException {}
-
-  /**
-   * Continue wait and print status if job goes to ready or final state.
-   * @param jobName
-   * @throws IOException
-   * @throws YarnException
-   * @throws SubmarineException
-   */
-  public void waitTrainingFinal(String jobName)
-      throws IOException, YarnException, SubmarineException {
-    // Wait 5 sec between each fetch.
-    int waitIntervalSec = 5;
-    JobStatus js;
-    while (true) {
-      js = getTrainingJobStatus(jobName);
-      JobState jobState = js.getState();
-      js.nicePrint(System.err);
-
-      if (JobState.isFinal(jobState)) {
-        if (jobState.equals(JobState.FAILED)) {
-          throw new SubmarineException("Job failed");
-        } else if (jobState.equals(JobState.KILLED)) {
-          throw new SubmarineException("Job killed");
-        }
-        LOG.info("Job exited with state=" + jobState);
-        break;
-      }
-
-      try {
-        Thread.sleep(waitIntervalSec * 1000);
-      } catch (InterruptedException e) {
-        throw new IOException(e);
-      }
-    }
-    cleanup();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/JobSubmitter.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/JobSubmitter.java
deleted file mode 100644
index 3806c6877c4c..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/JobSubmitter.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.common;
-
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-
-import java.io.IOException;
-
-/**
- * Submit job to cluster master.
- */
-public interface JobSubmitter {
-  /**
-   * Submit a job to cluster.
-   * @param parameters run job parameters
-   * @return applicationId when successfully submitted
-   * @throws YarnException for issues while contacting YARN daemons
-   * @throws IOException for other issues.
-   */
-  ApplicationId submitJob(ParametersHolder parameters)
-      throws IOException, YarnException;
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/StorageKeyConstants.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/StorageKeyConstants.java
deleted file mode 100644
index 1fbbe7ae4653..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/StorageKeyConstants.java
+++ /dev/null
@@ -1,24 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.common;
-
-public class StorageKeyConstants {
-  public static final String JOB_NAME = "JOB_NAME";
-  public static final String JOB_RUN_ARGS = "JOB_RUN_ARGS";
-  public static final String APPLICATION_ID = "APPLICATION_ID";
-  public static final String CHECKPOINT_PATH = "CHECKPOINT_PATH";
-  public static final String INPUT_PATH = "INPUT_PATH";
-  public static final String SAVED_MODEL_PATH = "SAVED_MODEL_PATH";
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/SubmarineStorage.java b/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/SubmarineStorage.java
deleted file mode 100644
index 9c2004fa9690..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/SubmarineStorage.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.common;
-
-import java.io.IOException;
-import java.util.Map;
-
-/**
- * Persistent job/model, etc.
- */
-public abstract class SubmarineStorage {
-  /**
-   * Add a new job by name
-   * @param jobName name of job.
-   * @param jobInfo info of the job.
-   */
-  public abstract void addNewJob(String jobName, Map<String, String> jobInfo)
-      throws IOException;
-
-  /**
-   * Get job info by job name.
-   * @param jobName name of job
-   * @return info of the job.
-   */
-  public abstract Map<String, String> getJobInfoByName(String jobName)
-      throws IOException;
-
-  /**
-   * Add a new model
-   * @param modelName name of model
-   * @param version version of the model, when null is specified, it will be
-   *                "default"
-   * @param modelInfo info of the model.
-   */
-  public abstract void addNewModel(String modelName, String version,
-      Map<String, String> modelInfo) throws IOException;
-
-  /**
-   * Get model info by name and version.
-   *  @param modelName name of model.
-   * @param version version of the model, when null is specifed, it will be
-   */
-  public abstract Map<String, String> getModelInfoByName(String modelName, String version)
-      throws IOException;
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/Examples.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/Examples.md
deleted file mode 100644
index fd61e833e077..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/Examples.md
+++ /dev/null
@@ -1,21 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-# Examples
-
-Here are some examples about how to use Submarine:
-
-[Running Distributed CIFAR 10 Tensorflow Job](RunningDistributedCifar10TFJobs.html)
-
-[Running Standalone CIFAR 10 PyTorch Job](RunningSingleNodeCifar10PTJobs.html)
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/HowToInstall.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/HowToInstall.md
deleted file mode 100644
index af96d6d6fed6..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/HowToInstall.md
+++ /dev/null
@@ -1,36 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-# How to Install Dependencies
-
-Submarine project uses YARN Service, Docker container and GPU.
-GPU could only be used if a GPU hardware is available and properly configured.
-
-As an administrator, you have to properly setup YARN Service related dependencies, including:
-- YARN Registry DNS
-- Docker related dependencies, including:
-  - Docker binary with expected versions
-  - Docker network that allows Docker containers to talk to each other across different nodes
-
-If you would like to use GPU, you need to set up:
-- GPU Driver
-- Nvidia-docker
-
-For your convenience, we provided some installation documents to help you setup your environment. You can always choose to have them installed in your own way.
-
-Use Submarine installer to install dependencies: [EN](https://github.com/hadoopsubmarine/hadoop-submarine-ecosystem/tree/master/submarine-installer) [CN](https://github.com/hadoopsubmarine/hadoop-submarine-ecosystem/blob/master/submarine-installer/README-CN.md)
-
-Alternatively, you can follow this guide to manually install dependencies: [EN](InstallationGuide.html) [CN](InstallationGuideChineseVersion.html)
-
-Once you have installed all the dependencies, please follow this guide: [TestAndTroubleshooting](TestAndTroubleshooting.html).
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/Index.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/Index.md
deleted file mode 100644
index e2c7979fb2ff..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/Index.md
+++ /dev/null
@@ -1,47 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-Submarine is a project which allows infra engineer / data scientist to run
-*unmodified* Tensorflow or PyTorch programs on YARN or Kubernetes.
-
-Goals of Submarine:
-
-- It allows jobs for easy access to data/models in HDFS and other storages.
-
-- Can launch services to serve Tensorflow/MXNet models.
-
-- Supports running distributed Tensorflow jobs with simple configs.
-
-- Supports running standalone PyTorch jobs with simple configs.
-
-- Supports running user-specified Docker images.
-
-- Supports specifying GPU and other resources.
-
-- Supports launching Tensorboard for training jobs (optional, if specified).
-
-- Supports customized DNS name for roles (like tensorboard.$user.$domain:6006)
-
-
-If you want to deep-dive, please check these resources:
-
-- [QuickStart Guide](QuickStart.html)
-
-- [Examples](Examples.html)
-
-- [How to write Dockerfile for Submarine TensorFlow jobs](WriteDockerfileTF.html)
-
-- [How to write Dockerfile for Submarine PyTorch jobs](WriteDockerfilePT.html)
-
-- [Installation guides](HowToInstall.html)
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/InstallationGuide.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/InstallationGuide.md
deleted file mode 100644
index 73bb9bdc4386..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/InstallationGuide.md
+++ /dev/null
@@ -1,594 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-# Submarine Installation Guide
-
-## Prerequisites
-
-Please note that the following prerequisites are just an example for you to install Submarine.
-
-You can always choose to install your own version of kernel, different users, different drivers, etc.
-
-### Operating System
-
-The operating system and kernel versions we have tested against are shown in the following table.
-The versions in the table are the recommended minimum required versions.
-
-| Environment | Version |
-| ------ | ------ |
-| Operating System | centos-release-7-5.1804.el7.centos.x86_64 |
-| Kernel | 3.10.0-862.el7.x86_64 |
-
-### User & Group
-
-There are specific users and groups recommended to be created to install Hadoop with Docker.
-
-Please create these users if they do not exist.
-
-```
-adduser hdfs
-adduser mapred
-adduser yarn
-addgroup hadoop
-usermod -aG hdfs,hadoop hdfs
-usermod -aG mapred,hadoop mapred
-usermod -aG yarn,hadoop yarn
-usermod -aG hdfs,hadoop hadoop
-groupadd docker
-usermod -aG docker yarn
-usermod -aG docker hadoop
-```
-
-### GCC Version
-
-Check the version of GCC tool (to compile kernel).
-
-```bash
-gcc --version
-gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)
-# install if needed
-yum install gcc make g++
-```
-
-### Kernel header & Kernel devel
-
-```bash
-# Approach 1：
-yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r)
-# Approach 2：
-wget http://vault.centos.org/7.3.1611/os/x86_64/Packages/kernel-headers-3.10.0-862.el7.x86_64.rpm
-rpm -ivh kernel-headers-3.10.0-862.el7.x86_64.rpm
-```
-
-### GPU Servers (Only for Nvidia GPU equipped nodes)
-
-```
-lspci | grep -i nvidia
-
-# If the server has gpus, you can get info like this：
-04:00.0 3D controller: NVIDIA Corporation Device 1b38 (rev a1)
-82:00.0 3D controller: NVIDIA Corporation Device 1b38 (rev a1)
-```
-
-
-
-### Nvidia Driver Installation (Only for Nvidia GPU equipped nodes)
-
-To make a clean installation, if you have requirements to upgrade GPU drivers.
-
-If nvidia driver / CUDA has been installed before, they should be uninstalled as a first step.
-
-```
-# uninstall cuda：
-sudo /usr/local/cuda-10.0/bin/uninstall_cuda_10.0.pl
-
-# uninstall nvidia-driver：
-sudo /usr/bin/nvidia-uninstall
-```
-
-To check GPU version, install nvidia-detect:
-
-```
-yum install nvidia-detect
-# run 'nvidia-detect -v' to get reqired nvidia driver version：
-nvidia-detect -v
-Probing for supported NVIDIA devices...
-[10de:13bb] NVIDIA Corporation GM107GL [Quadro K620]
-This device requires the current xyz.nm NVIDIA driver kmod-nvidia
-[8086:1912] Intel Corporation HD Graphics 530
-An Intel display controller was also detected
-```
-
-Pay attention to `This device requires the current xyz.nm NVIDIA driver kmod-nvidia`.
-Download the installer like [NVIDIA-Linux-x86_64-390.87.run](https://www.nvidia.com/object/linux-amd64-display-archive.html).
-
-
-Some preparatory work for Nvidia driver installation.
-
-The steps below are for Nvidia GPU driver installation, just pasted here for your convenience.
-
-```
-# It may take a while to update
-yum -y update
-yum -y install kernel-devel
-
-yum -y install epel-release
-yum -y install dkms
-
-# Disable nouveau
-vim /etc/default/grub
-# Add the following configuration in “GRUB_CMDLINE_LINUX” part
-rd.driver.blacklist=nouveau nouveau.modeset=0
-
-# Generate configuration
-grub2-mkconfig -o /boot/grub2/grub.cfg
-
-vim /etc/modprobe.d/blacklist.conf
-# Add confiuration:
-blacklist nouveau
-
-mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.img
-dracut /boot/initramfs-$(uname -r).img $(uname -r)
-reboot
-```
-
-Check whether nouveau is disabled
-
-```
-lsmod | grep nouveau  # return null
-
-# install nvidia driver
-sh NVIDIA-Linux-x86_64-390.87.run
-```
-
-Some options during the installation
-
-```
-Install NVIDIA's 32-bit compatibility libraries (Yes)
-centos Install NVIDIA's 32-bit compatibility libraries (Yes)
-Would you like to run the nvidia-xconfig utility to automatically update your X configuration file... (NO)
-```
-
-
-Check Nvidia driver installation
-
-```
-nvidia-smi
-```
-
-Reference：
-https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html
-
-
-
-### Docker Installation
-
-The following steps show you how to install docker 18.06.1.ce. You can choose other approaches to install Docker.
-
-```
-# Remove old version docker
-sudo yum remove docker \
-                docker-client \
-                docker-client-latest \
-                docker-common \
-                docker-latest \
-                docker-latest-logrotate \
-                docker-logrotate \
-                docker-engine
-
-# Docker version
-export DOCKER_VERSION="18.06.1.ce"
-# Setup the repository
-sudo yum install -y yum-utils \
-  device-mapper-persistent-data \
-  lvm2
-sudo yum-config-manager \
-    --add-repo \
-    https://download.docker.com/linux/centos/docker-ce.repo
-
-# Check docker version
-yum list docker-ce --showduplicates | sort -r
-
-# Install docker with specified DOCKER_VERSION
-sudo yum install -y docker-ce-${DOCKER_VERSION} docker-ce-cli-${DOCKER_VERSION} containerd.io
-
-# Start docker
-systemctl start docker
-
-chown hadoop:netease /var/run/docker.sock
-chown hadoop:netease /usr/bin/docker
-```
-
-Reference：https://docs.docker.com/install/linux/docker-ce/centos/
-
-### Docker Configuration
-
-Add a file, named daemon.json, under the path of /etc/docker/.
-
-Please replace the variables of image_registry_ip, etcd_host_ip, localhost_ip, yarn_dns_registry_host_ip, dns_host_ip with specific IPs according to your environment.
-
-```
-{
-    "insecure-registries": ["${image_registry_ip}:5000"],
-    "cluster-store":"etcd://${etcd_host_ip1}:2379,${etcd_host_ip2}:2379,${etcd_host_ip3}:2379",
-    "cluster-advertise":"{localhost_ip}:2375",
-    "dns": ["${yarn_dns_registry_host_ip}", "${dns_host_ip1}"],
-    "hosts": ["tcp://{localhost_ip}:2375", "unix:///var/run/docker.sock"]
-}
-```
-
-Restart docker daemon：
-
-```
-sudo systemctl restart docker
-```
-
-
-
-### Check docker version
-
-```bash
-$ docker version
-
-Client:
- Version:      18.06.1-ce
- API version:  1.38
- Go version:   go1.10.3
- Git commit:   e68fc7a
- Built:        Tue Aug 21 17:23:03 2018
- OS/Arch:      linux/amd64
- Experimental: false
-
-Server:
- Version:      18.06.1-ce
- API version:  1.38 (minimum version 1.12)
- Go version:   go1.10.3
- Git commit:   e68fc7a
- Built:        Tue Aug 21 17:23:03 2018
- OS/Arch:      linux/amd64
- Experimental: false
-```
-
-### Nvidia-docker Installation (Only for Nvidia GPU equipped nodes)
-
-Submarine has already supported nvidia-docker V2
-
-```
-# Add the package repositories
-distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
-curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | \
-  sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo
-sudo yum install -y nvidia-docker2-2.0.3-1.docker18.06.1.ce
-```
-
-According to `nvidia-driver` version, add folders under the path of  `/var/lib/nvidia-docker/volumes/nvidia_driver/`
-
-```
-mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87
-# 390.8 is nvidia driver version
-
-mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/bin
-mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64
-
-cp /usr/bin/nvidia* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/bin
-cp /usr/lib64/libcuda* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64
-cp /usr/lib64/libnvidia* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64
-
-# Test with nvidia-smi
-nvidia-docker run --rm nvidia/cuda:10.0-devel nvidia-smi
-```
-
-Test docker, nvidia-docker, nvidia-driver installation
-
-```
-# Test 1
-nvidia-docker run -rm nvidia/cuda nvidia-smi
-```
-
-```
-# Test 2
-nvidia-docker run -it tensorflow/tensorflow:1.9.0-gpu bash
-# In docker container
-python
-import tensorflow as tf
-tf.test.is_gpu_available()
-```
-
-If you want to uninstall nvidia-docker V2:
-```
-sudo yum remove -y nvidia-docker2-2.0.3-1.docker18.06.1.ce
-```
-
-Reference:
-https://github.com/NVIDIA/nvidia-docker
-
-### Tensorflow Image
-
-There is no need to install CUDNN and CUDA on the servers, because CUDNN and CUDA can be added in the docker images.
-
-We can get or build basic docker images by referring to [Write Dockerfile](WriteDockerfileTF.html).
-
-### Test tensorflow in a docker container
-
-After docker image is built, we can check
-Tensorflow environments before submitting a Submarine job.
-
-```shell
-$ docker run -it ${docker_image_name} /bin/bash
-# >>> In the docker container
-$ python
-$ python >> import tensorflow as tf
-$ python >> tf.__version__
-```
-
-If there are some errors, we could check the following configuration.
-
-1. LD_LIBRARY_PATH environment variable
-
-   ```
-   echo $LD_LIBRARY_PATH
-   /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
-   ```
-
-2. The location of libcuda.so.1, libcuda.so
-
-   ```
-   ls -l /usr/local/nvidia/lib64 | grep libcuda.so
-   ```
-
-### Etcd Installation
-
-etcd is a distributed, reliable key-value store for the most critical data of a distributed system, Registration and discovery of services used in containers.
-You can also choose alternatives like ZooKeeper, Consul or others.
-
-To install Etcd on specified servers, we can run Submarine-installer/install.sh
-
-```shell
-$ ./Submarine-installer/install.sh
-# Etcd status
-systemctl status Etcd.service
-```
-
-Check Etcd cluster health
-
-```shell
-$ etcdctl cluster-health
-member 3adf2673436aa824 is healthy: got healthy result from http://${etcd_host_ip1}:2379
-member 85ffe9aafb7745cc is healthy: got healthy result from http://${etcd_host_ip2}:2379
-member b3d05464c356441a is healthy: got healthy result from http://${etcd_host_ip3}:2379
-cluster is healthy
-
-$ etcdctl member list
-3adf2673436aa824: name=etcdnode3 peerURLs=http://${etcd_host_ip1}:2380 clientURLs=http://${etcd_host_ip1}:2379 isLeader=false
-85ffe9aafb7745cc: name=etcdnode2 peerURLs=http://${etcd_host_ip2}:2380 clientURLs=http://${etcd_host_ip2}:2379 isLeader=false
-b3d05464c356441a: name=etcdnode1 peerURLs=http://${etcd_host_ip3}:2380 clientURLs=http://${etcd_host_ip3}:2379 isLeader=true
-```
-
-
-
-### Calico Installation
-
-Calico creates and manages a flat three-tier network, and each container is assigned a routable IP address.
-
-We are listing the steps here for your convenience.
-You can also choose alternatives like Flannel, OVS or others.
-
-To install Calico on specified servers, we can run Submarine-installer/install.sh
-
-```
-systemctl start calico-node.service
-systemctl status calico-node.service
-```
-
-#### Check Calico Network
-
-```shell
-# Run the following command to show all host status in the cluster except localhost.
-$ calicoctl node status
-Calico process is running.
-
-IPv4 BGP status
-+---------------+-------------------+-------+------------+-------------+
-| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |
-+---------------+-------------------+-------+------------+-------------+
-| ${host_ip1} | node-to-node mesh | up    | 2018-09-21 | Established |
-| ${host_ip2} | node-to-node mesh | up    | 2018-09-21 | Established |
-| ${host_ip3} | node-to-node mesh | up    | 2018-09-21 | Established |
-+---------------+-------------------+-------+------------+-------------+
-
-IPv6 BGP status
-No IPv6 peers found.
-```
-
-Create containers to validate calico network
-
-```
-docker network create --driver calico --ipam-driver calico-ipam calico-network
-docker run --net calico-network --name workload-A -tid busybox
-docker run --net calico-network --name workload-B -tid busybox
-docker exec workload-A ping workload-B
-```
-
-
-## Hadoop Installation
-
-### Get Hadoop Release
-You can either get Hadoop release binary or compile from source code. Please follow the https://hadoop.apache.org/ guides.
-
-
-### Start YARN service
-
-```
-YARN_LOGFILE=resourcemanager.log ./sbin/yarn-daemon.sh start resourcemanager
-YARN_LOGFILE=nodemanager.log ./sbin/yarn-daemon.sh start nodemanager
-YARN_LOGFILE=timeline.log ./sbin/yarn-daemon.sh start timelineserver
-YARN_LOGFILE=mr-historyserver.log ./sbin/mr-jobhistory-daemon.sh start historyserver
-```
-
-### Start YARN registry DNS service
-
-```
-sudo YARN_LOGFILE=registrydns.log ./yarn-daemon.sh start registrydns
-```
-
-### Test with a MR wordcount job
-
-```
-./bin/hadoop jar /home/hadoop/hadoop-current/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0-SNAPSHOT.jar wordcount /tmp/wordcount.txt /tmp/wordcount-output4
-```
-
-
-
-## Tensorflow Job with CPU
-
-### Standalone Mode
-
-#### Clean up apps with the same name
-
-Suppose we want to submit a TensorFlow job named standalone-tf, destroy any application with the same name and clean up historical job directories.
-
-```bash
-./bin/yarn app -destroy standalone-tf
-./bin/hdfs dfs -rmr hdfs://${dfs_name_service}/tmp/cifar-10-jobdir
-```
-where ${dfs_name_service} is the HDFS name service you use
-
-#### Run a standalone tensorflow job
-
-```bash
-./bin/yarn jar /home/hadoop/hadoop-current/share/hadoop/yarn/hadoop-yarn-submarine-3.2.0-SNAPSHOT.jar job run \
- --env DOCKER_JAVA_HOME=/opt/java \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name standalone-tf \
- --docker_image tf-1.13.1-cpu:0.0.1 \
- --input_path hdfs://${dfs_name_service}/tmp/cifar-10-data \
- --checkpoint_path hdfs://${dfs_name_service}/user/hadoop/tf-checkpoint \
- --worker_resources memory=4G,vcores=2 --verbose \
- --worker_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --num-gpus=0"
-```
-
-### Distributed Mode
-
-#### Clean up apps with the same name
-
-```bash
-./bin/yarn app -destroy distributed-tf
-./bin/hdfs dfs -rmr hdfs://${dfs_name_service}/tmp/cifar-10-jobdir
-```
-
-#### Run a distributed TensorFlow job
-
-```bash
-./bin/yarn jar /home/hadoop/hadoop-current/share/hadoop/yarn/hadoop-yarn-submarine-3.2.0-SNAPSHOT.jar job run \
- --env DOCKER_JAVA_HOME=/opt/java \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name distributed-tf \
- --env YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_NETWORK=calico-network \
- --docker_image tf-1.13.1-cpu:0.0.1 \
- --input_path hdfs://${dfs_name_service}/tmp/cifar-10-data \
- --checkpoint_path hdfs://${dfs_name_service}/user/hadoop/tf-distributed-checkpoint \
- --worker_resources memory=4G,vcores=2 --verbose \
- --num_ps 1 \
- --ps_resources memory=4G,vcores=2 \
- --ps_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=0" \
- --num_workers 4 \
- --worker_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --sync --num-gpus=0"
-```
-
-
-## TensorFlow Job with GPU
-
-### GPU configurations for both ResourceManager and NodeManager
-
-Add the YARN resource configuration file, named resource-types.xml
-
-   ```
-   <configuration>
-     <property>
-       <name>yarn.resource-types</name>
-       <value>yarn.io/gpu</value>
-     </property>
-   </configuration>
-   ```
-
-#### GPU configurations for ResourceManager
-
-The scheduler used by ResourceManager must be the capacity scheduler, and yarn.scheduler.capacity.resource-calculator in capacity-scheduler.xml should be DominantResourceCalculator
-
-   ```
-   <configuration>
-     <property>
-       <name>yarn.scheduler.capacity.resource-calculator</name>
-       <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>
-     </property>
-   </configuration>
-   ```
-
-#### GPU configurations for NodeManager
-
-Add configurations in yarn-site.xml
-
-   ```
-   <configuration>
-     <property>
-       <name>yarn.nodemanager.resource-plugins</name>
-       <value>yarn.io/gpu</value>
-     </property>
-     <!--Use nvidia docker v2-->
-     <property>
-        <name>yarn.nodemanager.resource-plugins.gpu.docker-plugin</name>
-        <value>nvidia-docker-v2</value>
-     </property>
-   </configuration>
-   ```
-
-Add configurations to container-executor.cfg
-
-   ```
-   [docker]
-   ...
-   # Add configurations in `[docker]` part：
-   # /usr/bin/nvidia-docker is the path of nvidia-docker command
-   # nvidia_driver_375.26 means that nvidia driver version is <version>. nvidia-smi command can be used to check the version
-   docker.allowed.volume-drivers=/usr/bin/nvidia-docker
-   docker.allowed.devices=/dev/nvidiactl,/dev/nvidia-uvm,/dev/nvidia-uvm-tools,/dev/nvidia1,/dev/nvidia0
-   docker.allowed.ro-mounts=nvidia_driver_<version>
-   # Use nvidia docker v2
-   docker.allowed.runtimes=nvidia
-
-   [gpu]
-   module.enabled=true
-
-   [cgroups]
-   # /sys/fs/cgroup is the cgroup mount destination
-   # /hadoop-yarn is the path yarn creates by default
-   root=/sys/fs/cgroup
-   yarn-hierarchy=/hadoop-yarn
-   ```
-
-### Run a distributed TensorFlow GPU job
-
-```bash
- ./yarn jar /home/hadoop/hadoop-current/share/hadoop/yarn/hadoop-yarn-submarine-3.2.0-SNAPSHOT.jar job run \
- --env DOCKER_JAVA_HOME=/opt/java \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name distributed-tf-gpu \
- --env YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_NETWORK=calico-network \
- --docker_image tf-1.13.1-gpu:0.0.1 \
- --input_path hdfs://${dfs_name_service}/tmp/cifar-10-data \
- --checkpoint_path hdfs://${dfs_name_service}/user/hadoop/tf-distributed-checkpoint \
- --num_ps 0 \
- --ps_resources memory=4G,vcores=2,gpu=0 \
- --ps_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=0" \
- --worker_resources memory=4G,vcores=2,gpu=1 --verbose \
- --num_workers 1 \
- --worker_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --sync --num-gpus=1"
-```
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/InstallationGuideChineseVersion.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/InstallationGuideChineseVersion.md
deleted file mode 100644
index 7667c1c10b1b..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/InstallationGuideChineseVersion.md
+++ /dev/null
@@ -1,704 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-# Submarine 安装说明
-
-## Prerequisites
-
-### 操作系统
-
-我们使用的操作系统版本是 centos-release-7-5.1804.el7.centos.x86_64, 内核版本是 3.10.0-862.el7.x86_64。
-
-| Enviroment | Verion |
-| ------ | ------ |
-| Operating System | centos-release-7-5.1804.el7.centos.x86_64 |
-| Kernal | 3.10.0-862.el7.x86_64 |
-
-### User & Group
-
-如果操作系统中没有这些用户组和用户，必须添加。一部分用户是 hadoop 运行需要，一部分用户是 docker 运行需要。
-
-```
-adduser hdfs
-adduser mapred
-adduser yarn
-addgroup hadoop
-usermod -aG hdfs,hadoop hdfs
-usermod -aG mapred,hadoop mapred
-usermod -aG yarn,hadoop yarn
-usermod -aG hdfs,hadoop hadoop
-groupadd docker
-usermod -aG docker yarn
-usermod -aG docker hadoop
-```
-
-### GCC 版本
-
-```bash
-gcc --version
-gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)
-# 如果没有安装请执行以下命令进行安装
-yum install gcc make g++
-```
-
-### Kernel header & devel
-
-```bash
-# 方法一：
-yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r)
-# 方法二：
-wget http://vault.centos.org/7.3.1611/os/x86_64/Packages/kernel-headers-3.10.0-862.el7.x86_64.rpm
-rpm -ivh kernel-headers-3.10.0-862.el7.x86_64.rpm
-```
-
-### 检查 GPU 版本
-
-```
-lspci | grep -i nvidia
-
-# 如果什么都没输出，就说明显卡不对，以下是我的输出：
-# 04:00.0 3D controller: NVIDIA Corporation Device 1b38 (rev a1)
-# 82:00.0 3D controller: NVIDIA Corporation Device 1b38 (rev a1)
-```
-
-
-
-### 安装 nvidia 驱动
-
-安装nvidia driver/cuda要确保已安装的nvidia driver/cuda已被清理
-
-```
-# 卸载cuda：
-sudo /usr/local/cuda-10.0/bin/uninstall_cuda_10.0.pl
-
-# 卸载nvidia-driver：
-sudo /usr/bin/nvidia-uninstall
-```
-
-安装nvidia-detect，用于检查显卡版本
-
-```
-yum install nvidia-detect
-# 运行命令 nvidia-detect -v 返回结果：
-nvidia-detect -v
-Probing for supported NVIDIA devices...
-[10de:13bb] NVIDIA Corporation GM107GL [Quadro K620]
-This device requires the current 390.87 NVIDIA driver kmod-nvidia
-[8086:1912] Intel Corporation HD Graphics 530
-An Intel display controller was also detected
-```
-
-注意这里的信息 [Quadro K620] 和390.87。
-下载 [NVIDIA-Linux-x86_64-390.87.run](https://www.nvidia.com/object/linux-amd64-display-archive.html)
-
-
-安装前的一系列准备工作
-
-```
-# 若系统很久没更新，这句可能耗时较长
-yum -y update
-yum -y install kernel-devel
-
-yum -y install epel-release
-yum -y install dkms
-
-# 禁用nouveau
-vim /etc/default/grub  #在“GRUB_CMDLINE_LINUX”中添加内容 rd.driver.blacklist=nouveau nouveau.modeset=0
-grub2-mkconfig -o /boot/grub2/grub.cfg # 生成配置
-vim /etc/modprobe.d/blacklist.conf # 打开（新建）文件，添加内容blacklist nouveau
-
-mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.img
-dracut /boot/initramfs-$(uname -r).img $(uname -r)   # 更新配置，并重启
-reboot
-```
-
-开机后确认是否禁用
-
-```
-lsmod | grep nouveau  # 应该返回空
-
-# 开始安装
-sh NVIDIA-Linux-x86_64-390.87.run
-```
-
-安装过程中，会遇到一些选项：
-
-```
-Install NVIDIA's 32-bit compatibility libraries (Yes)
-centos Install NVIDIA's 32-bit compatibility libraries (Yes)
-Would you like to run the nvidia-xconfig utility to automatically update your X configuration file... (NO)
-```
-
-
-最后查看 nvidia gpu 状态
-
-```
-nvidia-smi
-```
-
-reference：
-https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html
-
-
-
-### 安装 Docker
-
-```
-# Remove old version docker
-sudo yum remove docker \
-                docker-client \
-                docker-client-latest \
-                docker-common \
-                docker-latest \
-                docker-latest-logrotate \
-                docker-logrotate \
-                docker-engine
-
-# Docker version
-export DOCKER_VERSION="18.06.1.ce"
-# Setup the repository
-sudo yum install -y yum-utils \
-  device-mapper-persistent-data \
-  lvm2
-sudo yum-config-manager \
-    --add-repo \
-    https://download.docker.com/linux/centos/docker-ce.repo
-
-# Check docker version
-yum list docker-ce --showduplicates | sort -r
-
-# Install docker with specified DOCKER_VERSION
-sudo yum install -y docker-ce-${DOCKER_VERSION} docker-ce-cli-${DOCKER_VERSION} containerd.io
-
-# Start docker
-systemctl start docker
-
-chown hadoop:netease /var/run/docker.sock
-chown hadoop:netease /usr/bin/docker
-```
-
-Reference：https://docs.docker.com/install/linux/docker-ce/centos/
-
-### 配置 Docker
-
-在 `/etc/docker/` 目录下，创建`daemon.json`文件, 添加以下配置，变量如image_registry_ip, etcd_host_ip, localhost_ip, yarn_dns_registry_host_ip, dns_host_ip需要根据具体环境，进行修改
-
-```
-{
-    "insecure-registries": ["${image_registry_ip}:5000"],
-    "cluster-store":"etcd://${etcd_host_ip1}:2379,${etcd_host_ip2}:2379,${etcd_host_ip3}:2379",
-    "cluster-advertise":"{localhost_ip}:2375",
-    "dns": ["${yarn_dns_registry_host_ip}", "${dns_host_ip1}"],
-    "hosts": ["tcp://{localhost_ip}:2375", "unix:///var/run/docker.sock"]
-}
-```
-
-重启 docker daemon：
-
-```
-sudo systemctl restart docker
-```
-
-
-
-### 检查 Docker version
-
-```bash
-$ docker version
-
-Client:
- Version:      18.06.1-ce
- API version:  1.38
- Go version:   go1.10.3
- Git commit:   e68fc7a
- Built:        Tue Aug 21 17:23:03 2018
- OS/Arch:      linux/amd64
- Experimental: false
-
-Server:
- Version:      18.06.1-ce
- API version:  1.38 (minimum version 1.12)
- Go version:   go1.10.3
- Git commit:   e68fc7a
- Built:        Tue Aug 21 17:23:03 2018
- OS/Arch:      linux/amd64
- Experimental: false
-```
-
-### 安装 nvidia-docker
-
-Hadoop-3.2 的 submarine 已支持 V2 版本的 nvidia-docker
-
-```
-# Add the package repositories
-distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
-curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | \
-  sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo
-sudo yum install -y nvidia-docker2-2.0.3-1.docker18.06.1.ce
-```
-
-在 `/var/lib/nvidia-docker/volumes/nvidia_driver/` 路径下，根据 `nvidia-driver` 的版本创建文件夹：
-
-```
-mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87
-# 其中390.87是nvidia driver的版本号
-
-mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/bin
-mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64
-
-cp /usr/bin/nvidia* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/bin
-cp /usr/lib64/libcuda* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64
-cp /usr/lib64/libnvidia* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64
-
-# Test nvidia-smi
-nvidia-docker run --rm nvidia/cuda:10.0-devel nvidia-smi
-```
-
-测试 docker, nvidia-docker, nvidia-driver 安装
-
-```
-# 测试一
-nvidia-docker run -rm nvidia/cuda nvidia-smi
-```
-
-```
-# 测试二
-nvidia-docker run -it tensorflow/tensorflow:1.9.0-gpu bash
-# 在docker中执行
-python
-import tensorflow as tf
-tf.test.is_gpu_available()
-```
-
-卸载 nvidia-docker V2 的方法：
-```
-sudo yum remove -y nvidia-docker2-2.0.3-1.docker18.06.1.ce
-```
-
-reference:
-https://github.com/NVIDIA/nvidia-docker
-
-
-
-### Tensorflow Image
-
-CUDNN 和 CUDA 其实不需要在物理机上安装，因为 Submarine 中提供了已经包含了CUDNN 和 CUDA 的镜像文件，基础的Dockfile可参见WriteDockerfileTF.md
-
-### 测试 TF 环境
-
-创建好 docker 镜像后，需要先手动检查 TensorFlow 是否可以正常使用，避免通过 YARN 调度后出现问题，可以执行以下命令
-
-```shell
-$ docker run -it ${docker_image_name} /bin/bash
-# >>> 进入容器
-$ python
-$ python >> import tensorflow as tf
-$ python >> tf.__version__
-```
-
-如果出现问题，可以按照以下路径进行排查
-
-1. 环境变量是否设置正确
-
-   ```
-   echo $LD_LIBRARY_PATH
-   /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
-   ```
-
-2. libcuda.so.1,libcuda.so是否在LD_LIBRARY_PATH指定的路径中
-
-   ```
-   ls -l /usr/local/nvidia/lib64 | grep libcuda.so
-   ```
-
-### 安装 Etcd
-
-运行 Submarine/install.sh 脚本，就可以在指定服务器中安装 Etcd 组件和服务自启动脚本。
-
-```shell
-$ ./Submarine/install.sh
-# 通过如下命令查看 Etcd 服务状态
-systemctl status Etcd.service
-```
-
-检查 Etcd 服务状态
-
-```shell
-$ etcdctl cluster-health
-member 3adf2673436aa824 is healthy: got healthy result from http://${etcd_host_ip1}:2379
-member 85ffe9aafb7745cc is healthy: got healthy result from http://${etcd_host_ip2}:2379
-member b3d05464c356441a is healthy: got healthy result from http://${etcd_host_ip3}:2379
-cluster is healthy
-
-$ etcdctl member list
-3adf2673436aa824: name=etcdnode3 peerURLs=http://${etcd_host_ip1}:2380 clientURLs=http://${etcd_host_ip1}:2379 isLeader=false
-85ffe9aafb7745cc: name=etcdnode2 peerURLs=http://${etcd_host_ip2}:2380 clientURLs=http://${etcd_host_ip2}:2379 isLeader=false
-b3d05464c356441a: name=etcdnode1 peerURLs=http://${etcd_host_ip3}:2380 clientURLs=http://${etcd_host_ip3}:2379 isLeader=true
-```
-其中，${etcd_host_ip*} 是etcd服务器的ip
-
-
-### 安装 Calico
-
-运行 Submarine/install.sh 脚本，就可以在指定服务器中安装 Calico 组件和服务自启动脚本。
-
-```
-systemctl start calico-node.service
-systemctl status calico-node.service
-```
-
-#### 检查 Calico 网络
-
-```shell
-# 执行如下命令，注意：不会显示本服务器的状态，只显示其他的服务器状态
-$ calicoctl node status
-Calico process is running.
-
-IPv4 BGP status
-+---------------+-------------------+-------+------------+-------------+
-| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |
-+---------------+-------------------+-------+------------+-------------+
-| ${host_ip1} | node-to-node mesh | up    | 2018-09-21 | Established |
-| ${host_ip2} | node-to-node mesh | up    | 2018-09-21 | Established |
-| ${host_ip3} | node-to-node mesh | up    | 2018-09-21 | Established |
-+---------------+-------------------+-------+------------+-------------+
-
-IPv6 BGP status
-No IPv6 peers found.
-```
-
-创建docker container，验证calico网络
-
-```
-docker network create --driver calico --ipam-driver calico-ipam calico-network
-docker run --net calico-network --name workload-A -tid busybox
-docker run --net calico-network --name workload-B -tid busybox
-docker exec workload-A ping workload-B
-```
-
-
-## 安装 Hadoop
-
-### 编译 Hadoop
-
-```
-mvn package -Pdist -DskipTests -Dtar
-```
-
-
-
-### 启动 YARN服务
-
-```
-YARN_LOGFILE=resourcemanager.log ./sbin/yarn-daemon.sh start resourcemanager
-YARN_LOGFILE=nodemanager.log ./sbin/yarn-daemon.sh start nodemanager
-YARN_LOGFILE=timeline.log ./sbin/yarn-daemon.sh start timelineserver
-YARN_LOGFILE=mr-historyserver.log ./sbin/mr-jobhistory-daemon.sh start historyserver
-```
-
-### 启动 registery dns 服务
-
-```
-sudo YARN_LOGFILE=registrydns.log ./yarn-daemon.sh start registrydns
-```
-
-
-
-### 测试 wordcount
-
-通过测试最简单的 wordcount ，检查 YARN 是否正确安装
-
-```
-./bin/hadoop jar /home/hadoop/hadoop-current/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0-SNAPSHOT.jar wordcount /tmp/wordcount.txt /tmp/wordcount-output4
-```
-
-
-
-## 使用CUP的Tensorflow任务
-
-### 单机模式
-
-#### 清理重名程序
-
-```bash
-# 每次提交前需要执行：
-./bin/yarn app -destroy standalone-tf
-# 并删除hdfs路径：
-./bin/hdfs dfs -rmr hdfs://${dfs_name_service}/tmp/cifar-10-jobdir
-# 确保之前的任务已经结束
-```
-其中，变量${dfs_name_service}请根据环境，用你的hdfs name service名称替换
-
-#### 执行单机模式的tensorflow任务
-
-```bash
-./bin/yarn jar /home/hadoop/hadoop-current/share/hadoop/yarn/hadoop-yarn-submarine-3.2.0-SNAPSHOT.jar job run \
- --env DOCKER_JAVA_HOME=/opt/java \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name standalone-tf \
- --docker_image tf-1.13.1-cpu:0.0.1 \
- --input_path hdfs://${dfs_name_service}/tmp/cifar-10-data \
- --checkpoint_path hdfs://${dfs_name_service}/user/hadoop/tf-checkpoint \
- --worker_resources memory=4G,vcores=2 --verbose \
- --worker_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --num-gpus=0"
-```
-
-
-### 分布式模式
-
-#### 清理重名程序
-
-```bash
-# 每次提交前需要执行：
-./bin/yarn app -destroy distributed-tf
-# 并删除hdfs路径：
-./bin/hdfs dfs -rmr hdfs://${dfs_name_service}/tmp/cifar-10-jobdir
-# 确保之前的任务已经结束
-```
-
-#### 提交分布式模式 tensorflow 任务
-
-```bash
-./bin/yarn jar /home/hadoop/hadoop-current/share/hadoop/yarn/hadoop-yarn-submarine-3.2.0-SNAPSHOT.jar job run \
- --env DOCKER_JAVA_HOME=/opt/java \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name distributed-tf \
- --env YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_NETWORK=calico-network \
- --docker_image tf-1.13.1-cpu:0.0.1 \
- --input_path hdfs://${dfs_name_service}/tmp/cifar-10-data \
- --checkpoint_path hdfs://${dfs_name_service}/user/hadoop/tf-distributed-checkpoint \
- --worker_resources memory=4G,vcores=2 --verbose \
- --num_ps 1 \
- --ps_resources memory=4G,vcores=2 \
- --ps_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=0" \
- --num_workers 4 \
- --worker_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --sync --num-gpus=0"
-```
-
-
-## 使用GPU的Tensorflow任务
-
-### Resourcemanager, Nodemanager 中添加GPU支持
-
-在 yarn 配置文件夹(conf或etc/hadoop)中创建 resource-types.xml，添加：
-
-   ```
-   <configuration>
-     <property>
-       <name>yarn.resource-types</name>
-       <value>yarn.io/gpu</value>
-     </property>
-   </configuration>
-   ```
-
-### Resourcemanager 的 GPU 配置
-
-resourcemanager 使用的 scheduler 必须是 capacity scheduler，在 capacity-scheduler.xml 中修改属性：
-
-   ```
-   <configuration>
-     <property>
-       <name>yarn.scheduler.capacity.resource-calculator</name>
-       <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>
-     </property>
-   </configuration>
-   ```
-
-### Nodemanager 的 GPU 配置
-
-在 nodemanager 的 yarn-site.xml 中添加配置：
-
-   ```
-   <configuration>
-     <property>
-       <name>yarn.nodemanager.resource-plugins</name>
-       <value>yarn.io/gpu</value>
-     </property>
-     <!--Use nvidia docker v2-->
-     <property>
-       <name>yarn.nodemanager.resource-plugins.gpu.docker-plugin</name>
-       <value>nvidia-docker-v2</value>
-     </property>
-   </configuration>
-   ```
-
-在 container-executor.cfg 中添加配置：
-
-   ```
-   [docker]
-   ...
-   # 在[docker]已有配置中，添加以下内容：
-   # /usr/bin/nvidia-docker是nvidia-docker路径
-   # nvidia_driver_375.26的版本号375.26，可以使用nvidia-smi查看
-   docker.allowed.volume-drivers=/usr/bin/nvidia-docker
-   docker.allowed.devices=/dev/nvidiactl,/dev/nvidia-uvm,/dev/nvidia-uvm-tools,/dev/nvidia1,/dev/nvidia0
-   docker.allowed.ro-mounts=nvidia_driver_375.26
-   # Use nvidia docker v2
-   docker.allowed.runtimes=nvidia
-
-   [gpu]
-   module.enabled=true
-
-   [cgroups]
-   # /sys/fs/cgroup是cgroup的mount路径
-   # /hadoop-yarn是yarn在cgroup路径下默认创建的path
-   root=/sys/fs/cgroup
-   yarn-hierarchy=/hadoop-yarn
-   ```
-
-### 提交验证
-
-Distributed-shell + GPU + cgroup
-
-```bash
- ./yarn jar /home/hadoop/hadoop-current/share/hadoop/yarn/hadoop-yarn-submarine-3.2.0-SNAPSHOT.jar job run \
- --env DOCKER_JAVA_HOME=/opt/java \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name distributed-tf-gpu \
- --env YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_NETWORK=calico-network \
- --docker_image tf-1.13.1-gpu:0.0.1 \
- --input_path hdfs://${dfs_name_service}/tmp/cifar-10-data \
- --checkpoint_path hdfs://${dfs_name_service}/user/hadoop/tf-distributed-checkpoint \
- --num_ps 0 \
- --ps_resources memory=4G,vcores=2,gpu=0 \
- --ps_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=hdfs://${dfs_name_service}/tmp/cifar-10-data --job-dir=hdfs://${dfs_name_service}/tmp/cifar-10-jobdir --num-gpus=0" \
- --worker_resources memory=4G,vcores=2,gpu=1 --verbose \
- --num_workers 1 \
- --worker_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=hdfs://${dfs_name_service}/tmp/cifar-10-data --job-dir=hdfs://${dfs_name_service}/tmp/cifar-10-jobdir --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --sync --num-gpus=1"
-```
-
-
-
-## 问题
-
-### 问题一: 操作系统重启导致 nodemanager 启动失败
-
-```
-2018-09-20 18:54:39,785 ERROR org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Failed to bootstrap configured resource subsystems!
-org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Unexpected: Cannot create yarn cgroup Subsystem:cpu Mount points:/proc/mounts User:yarn Path:/sys/fs/cgroup/cpu,cpuacct/hadoop-yarn
-  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.initializePreMountedCGroupController(CGroupsHandlerImpl.java:425)
-  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.initializeCGroupController(CGroupsHandlerImpl.java:377)
-  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.bootstrap(CGroupsCpuResourceHandlerImpl.java:98)
-  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.bootstrap(CGroupsCpuResourceHandlerImpl.java:87)
-  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.bootstrap(ResourceHandlerChain.java:58)
-  at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:320)
-  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:389)
-  at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
-  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:929)
-  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:997)
-2018-09-20 18:54:39,789 INFO org.apache.hadoop.service.AbstractService: Service NodeManager failed in state INITED
-```
-
-解决方法：使用 `root` 账号给 `yarn` 用户修改 `/sys/fs/cgroup/cpu,cpuacct` 的权限
-
-```
-chown :yarn -R /sys/fs/cgroup/cpu,cpuacct
-chmod g+rwx -R /sys/fs/cgroup/cpu,cpuacct
-```
-
-在支持gpu时，还需cgroup devices路径权限
-
-```
-chown :yarn -R /sys/fs/cgroup/devices
-chmod g+rwx -R /sys/fs/cgroup/devices
-```
-
-
-### 问题二：container-executor 权限问题
-
-```
-2018-09-21 09:36:26,102 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor: IOException executing command:
-java.io.IOException: Cannot run program "/etc/yarn/sbin/Linux-amd64-64/container-executor": error=13, Permission denied
-        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
-        at org.apache.hadoop.util.Shell.runCommand(Shell.java:938)
-        at org.apache.hadoop.util.Shell.run(Shell.java:901)
-        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)
-```
-
-`/etc/yarn/sbin/Linux-amd64-64/container-executor` 该文件的权限应为6050
-
-### 问题三：查看系统服务启动日志
-
-```
-journalctl -u docker
-```
-
-### 问题四：Docker 无法删除容器的问题 `device or resource busy`
-
-```bash
-$ docker rm 0bfafa146431
-Error response from daemon: Unable to remove filesystem for 0bfafa146431771f6024dcb9775ef47f170edb2f1852f71916ba44209ca6120a: remove /app/docker/containers/0bfafa146431771f6024dcb9775ef47f170edb2f152f71916ba44209ca6120a/shm: device or resource busy
-```
-
-编写 `find-busy-mnt.sh` 脚本，检查 `device or resource busy` 状态的容器挂载文件
-
-```bash
-#!/bin/bash
-
-# A simple script to get information about mount points and pids and their
-# mount namespaces.
-
-if [ $# -ne 1 ];then
-echo "Usage: $0 <devicemapper-device-id>"
-exit 1
-fi
-
-ID=$1
-
-MOUNTS=`find /proc/*/mounts | xargs grep $ID 2>/dev/null`
-
-[ -z "$MOUNTS" ] &&  echo "No pids found" && exit 0
-
-printf "PID\tNAME\t\tMNTNS\n"
-echo "$MOUNTS" | while read LINE; do
-PID=`echo $LINE | cut -d ":" -f1 | cut -d "/" -f3`
-# Ignore self and thread-self
-if [ "$PID" == "self" ] || [ "$PID" == "thread-self" ]; then
-  continue
-fi
-NAME=`ps -q $PID -o comm=`
-MNTNS=`readlink /proc/$PID/ns/mnt`
-printf "%s\t%s\t\t%s\n" "$PID" "$NAME" "$MNTNS"
-done
-```
-
-查找占用目录的进程
-
-```bash
-$ chmod +x find-busy-mnt.sh
-./find-busy-mnt.sh 0bfafa146431771f6024dcb9775ef47f170edb2f152f71916ba44209ca6120a
-# PID   NAME            MNTNS
-# 5007  ntpd            mnt:[4026533598]
-$ kill -9 5007
-```
-
-
-### 问题五：命令sudo nvidia-docker run 报错
-
-```
-docker: Error response from daemon: create nvidia_driver_361.42: VolumeDriver.Create: internal error, check logs for details.
-See 'docker run --help'.
-```
-
-解决方法：
-
-```
-#查看nvidia-docker状态，是不是启动有问题，可以使用
-$ systemctl status nvidia-docker
-$ journalctl -n -u nvidia-docker
-#重启下nvidia-docker
-systemctl stop nvidia-docker
-systemctl start nvidia-docker
-```
-
-### 问题六：YARN 启动容器失败
-
-如果你创建的容器数（PS+Work>GPU显卡总数），可能会出现容器创建失败，那是因为在一台服务器上同时创建了超过本机显卡总数的容器。
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/QuickStart.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/QuickStart.md
deleted file mode 100644
index 37991d789946..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/QuickStart.md
+++ /dev/null
@@ -1,322 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-# Quick Start Guide
-
-## Prerequisite
-
-Must:
-
-- Apache Hadoop version newer than 2.7.3
-
-Optional:
-
-- Enable YARN DNS. (Only when YARN Service runtime is required)
-- Enable GPU on YARN support. (When GPU-based training is required)
-- Docker images for Submarine jobs. (When docker container is required)
-```
-  # Get prebuilt docker images (No liability)
-  docker pull hadoopsubmarine/tf-1.13.1-gpu:0.0.1
-  # Or build your own docker images
-  docker build . -f Dockerfile.gpu.tf_1.13.1 -t tf-1.13.1-gpu-base:0.0.1
-```
-For more details, please refer to:
-
-- [How to write Dockerfile for Submarine TensorFlow jobs](WriteDockerfileTF.html)
-
-- [How to write Dockerfile for Submarine PyTorch jobs](WriteDockerfilePT.html)
-
-## Submarine runtimes
-After submarine 0.2.0, it supports two runtimes which are YARN native service
- runtime and Linkedin's TonY runtime. Each runtime can support both Tensorflow
- and Pytorch framework. And the user don't need to worry about the usage
- because the two runtime implements the same interface.
-
-To use the TonY runtime, please set below value in the submarine configuration.
-
-|Configuration Name | Description |
-|:---- |:---- |
-| `submarine.runtime.class` | org.apache.hadoop.yarn.submarine.runtimes.tony.TonyRuntimeFactory |
-
-For more details of TonY runtime, please check [TonY runtime guide](TonYRuntimeGuide.html)
-
-## Run jobs
-
-### Commandline options
-
-```$xslt
-usage: job run
-
- -framework <arg>             Framework to use.
-                              Valid values are: tensorflow, pytorch.
-                              The default framework is Tensorflow.
- -checkpoint_path <arg>       Training output directory of the job, could
-                              be local or other FS directory. This
-                              typically includes checkpoint files and
-                              exported model
- -docker_image <arg>          Docker image name/tag
- -env <arg>                   Common environment variable of worker/ps
- -input_path <arg>            Input of the job, could be local or other FS
-                              directory
- -name <arg>                  Name of the job
- -num_ps <arg>                Number of PS tasks of the job, by default
-                              it's 0
- -num_workers <arg>           Numnber of worker tasks of the job, by
-                              default it's 1
- -ps_docker_image <arg>       Specify docker image for PS, when this is
-                              not specified, PS uses --docker_image as
-                              default.
- -ps_launch_cmd <arg>         Commandline of worker, arguments will be
-                              directly used to launch the PS
- -ps_resources <arg>          Resource of each PS, for example
-                              memory-mb=2048,vcores=2,yarn.io/gpu=2
- -queue <arg>                 Name of queue to run the job, by default it
-                              uses default queue
- -saved_model_path <arg>      Model exported path (savedmodel) of the job,
-                              which is needed when exported model is not
-                              placed under ${checkpoint_path}could be
-                              local or other FS directory. This will be
-                              used to serve.
- -tensorboard <arg>           Should we run TensorBoard for this job? By
-                              default it's true
- -verbose                     Print verbose log for troubleshooting
- -wait_job_finish             Specified when user want to wait the job
-                              finish
- -worker_docker_image <arg>   Specify docker image for WORKER, when this
-                              is not specified, WORKER uses --docker_image
-                              as default.
- -worker_launch_cmd <arg>     Commandline of worker, arguments will be
-                              directly used to launch the worker
- -worker_resources <arg>      Resource of each worker, for example
-                              memory-mb=2048,vcores=2,yarn.io/gpu=2
- -localization <arg>          Specify localization to remote/local
-                              file/directory available to all container(Docker).
-                              Argument format is "RemoteUri:LocalFilePath[:rw]"
-                              (ro permission is not supported yet).
-                              The RemoteUri can be a file or directory in local
-                              or HDFS or s3 or abfs or http .etc.
-                              The LocalFilePath can be absolute or relative.
-                              If relative, it'll be under container's implied
-                              working directory.
-                              This option can be set mutiple times.
-                              Examples are
-                              -localization "hdfs:///user/yarn/mydir2:/opt/data"
-                              -localization "s3a:///a/b/myfile1:./"
-                              -localization "https:///a/b/myfile2:./myfile"
-                              -localization "/user/yarn/mydir3:/opt/mydir3"
-                              -localization "./mydir1:."
-```
-
-#### Notes:
-When using `localization` option to make a collection of dependency Python
-scripts available to entry python script in the container, you may also need to
-set the `PYTHONPATH` environment variable as below to avoid module import errors
-reported from `entry_script.py`.
-
-```
-... job run
-  # the entry point
-  --localization entry_script.py:<path>/entry_script.py
-  # the dependency Python scripts of the entry point
-  --localization other_scripts_dir:<path>/other_scripts_dir
-  # the PYTHONPATH env to make dependency available to entry script
-  --env PYTHONPATH="<path>/other_scripts_dir"
-  --worker_launch_cmd "python <path>/entry_script.py ..."
-```
-
-### Submarine Configuration
-
-For Submarine internal configuration, please create a `submarine.xml` file which should be placed under `$HADOOP_CONF_DIR`.
-
-|Configuration Name | Description |
-|:---- |:---- |
-| `submarine.runtime.class` | Optional. Full qualified class name for your runtime factory. |
-| `submarine.localization.max-allowed-file-size-mb` | Optional. This sets a size limit to the file/directory to be localized in "-localization" CLI option. 2GB by default. |
-
-
-
-### Launch Standalone Tensorflow Application:
-
-#### Commandline
-```
-yarn jar path-to/hadoop-yarn-applications-submarine-3.2.0-SNAPSHOT.jar job run \
-  --framework tensorflow \
-  --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/ \
-  --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name tf-job-001 \
-  --docker_image <your-docker-image> \
-  --input_path hdfs://default/dataset/cifar-10-data  \
-  --checkpoint_path hdfs://default/tmp/cifar-10-jobdir \
-  --worker_resources memory=4G,vcores=2,gpu=2 \
-  --worker_launch_cmd "python ... (Your training application cmd)" \
-  --tensorboard # this will launch a companion tensorboard container for monitoring
-```
-
-#### Notes:
-
-1) `DOCKER_JAVA_HOME` points to JAVA_HOME inside Docker image.
-
-2) `DOCKER_HADOOP_HDFS_HOME` points to HADOOP_HDFS_HOME inside Docker image.
-
-3) `--worker_resources` can include GPU when you need GPU to train your task.
-
-4) When `--tensorboard` is specified, you can go to YARN new UI, go to services -> `<you specified service>` -> Click `...` to access Tensorboard.
-
-This will launch Tensorboard to monitor *all your jobs*.
-By access the YARN UI (new UI), you can go to the Services page, then go to the `tensorboard-service`, click quick links (`Tensorboard`)
-This will lead you to Tensorboard.
-
-See below screenshot:
-
-![alt text](./images/tensorboard-service.png "Tensorboard service")
-
-After v0.2.0, if there is no hadoop client, we can also use the java command
-and the uber jar, hadoop-submarine-all-*.jar, to submit the job.
-
-```
-java -cp /path-to/hadoop-conf:/path-to/hadoop-submarine-all-*.jar \
-  org.apache.hadoop.yarn.submarine.client.cli.Cli job run \
-  --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/ \
-  --env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.0 --name tf-job-001 \
-  --docker_image <your-docker-image> \
-  --input_path hdfs://default/dataset/cifar-10-data  \
-  --checkpoint_path hdfs://default/tmp/cifar-10-jobdir \
-  --worker_resources memory=4G,vcores=2,gpu=2  \
-  --worker_launch_cmd "python ... (Your training application cmd)" \
-  --tensorboard # this will launch a companion tensorboard container for monitoring
-```
-
-
-### Launch Distributed Tensorflow Application:
-
-#### Commandline
-
-```
-yarn jar hadoop-yarn-applications-submarine-<version>.jar job run \
- --name tf-job-001 --docker_image <your-docker-image> \
- --framework tensorflow \
- --input_path hdfs://default/dataset/cifar-10-data \
- --checkpoint_path hdfs://default/tmp/cifar-10-jobdir \
- --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/ \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current \
- --num_workers 2 \
- --worker_resources memory=8G,vcores=2,gpu=1 --worker_launch_cmd "cmd for worker ..." \
- --num_ps 2 \
- --ps_resources memory=4G,vcores=2,gpu=0 --ps_launch_cmd "cmd for ps" \
-```
-Or
-```
-java -cp /path-to/hadoop-conf:/path-to/hadoop-submarine-all-*.jar \
- org.apache.hadoop.yarn.submarine.client.cli.Cli job run \
- --name tf-job-001 --docker_image <your docker image> \
- --input_path hdfs://default/dataset/cifar-10-data \
- --checkpoint_path hdfs://default/tmp/cifar-10-jobdir \
- --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/ \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.0 \
- --num_workers 2 \
- --worker_resources memory=8G,vcores=2,gpu=1 --worker_launch_cmd "cmd for worker ..." \
- --num_ps 2 \
- --ps_resources memory=4G,vcores=2,gpu=0 --ps_launch_cmd "cmd for ps" \
-```
-
-#### Notes:
-
-1) Very similar to standalone TF application, but you need to specify number of workers / PS processes.
-
-2) Different resources can be specified for worker and PS.
-
-3) `TF_CONFIG` environment will be auto generated and set before executing user's launch command.
-
-## Get job history / logs
-
-### Get Job Status from CLI
-
-```
-yarn jar hadoop-yarn-applications-submarine-3.2.0-SNAPSHOT.jar job show --name tf-job-001
-```
-Or
-```
-java -cp /path-to/hadoop-conf:/path-to/hadoop-submarine-all-*.jar \
- org.apache.hadoop.yarn.submarine.client.cli.Cli job show --name tf-job-001
-```
-Output looks like:
-```
-Job Meta Info:
-  Application Id: application_1532131617202_0005
-  Input Path: hdfs://default/dataset/cifar-10-data
-  Checkpoint Path: hdfs://default/tmp/cifar-10-jobdir
-  Run Parameters: --name tf-job-001 --docker_image <your-docker-image>
-                  (... all your commandline before run the job)
-```
-
-After that, you can run ```tensorboard --logdir=<checkpoint-path>``` to view Tensorboard of the job.
-
-### Run tensorboard to monitor your jobs
-
-```
-# Cleanup previous service if needed
-yarn app -destroy tensorboard-service; \
-yarn jar /tmp/hadoop-yarn-applications-submarine-3.2.0-SNAPSHOT.jar \
-  job run --name tensorboard-service --verbose --docker_image <your-docker-image> \
-  --framework tensorflow \
-  --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/ \
-  --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current \
-  --num_workers 0 --tensorboard
-```
-Or
-```
-# Cleanup previous service if needed
-yarn app -destroy tensorboard-service; \
-java -cp /path-to/hadoop-conf:/path-to/hadoop-submarine-all-*.jar \
-  org.apache.hadoop.yarn.submarine.client.cli.Cli job run \
-  --name tensorboard-service --verbose --docker_image wtan/tf-1.8.0-cpu:0.0.3 \
-  --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/ \
-  --env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.0 \
-  --num_workers 0 --tensorboard
-```
-
-You can view multiple job training history from the `Tensorboard` link:
-
-![alt text](./images/multiple-tensorboard-jobs.png "Tensorboard for multiple jobs")
-
-
-### Get component logs from a training job
-
-There are two ways to get the logs of a training job.
-First, from YARN UI (new or old):
-
-![alt text](./images/job-logs-ui.png "Job logs UI")
-
-Alternatively, you can use `yarn logs -applicationId <applicationId>` to get logs from CLI.
-
-## Build from source code
-
-If you want to build the Submarine project by yourself, you should follow these steps:
-
-- Run 'mvn install -DskipTests' from Hadoop source top level once.
-
-- Navigate to hadoop-submarine folder and run 'mvn clean package'.
-
-    - By Default, hadoop-submarine is built based on hadoop 3.1.2 dependencies.
-      Both yarn service runtime and tony runtime are built.
-      You can also add a parameter of "-Phadoop-3.2" to specify the dependencies
-      to hadoop 3.2.0.
-
-    - Hadoop-submarine can support hadoop 2.9.2 and hadoop 2.7.4 as well.
-      You can add "-Phadoop-2.9" to build submarine based on hadoop 2.9.2.
-      For example:
-      ```
-      mvn clean package -Phadoop-2.9
-      ```
-      As yarn service is based on hadoop 3.*, so only tony runtime is built
-      in this case.
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/RunningDistributedCifar10TFJobs.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/RunningDistributedCifar10TFJobs.md
deleted file mode 100644
index 3495c6966aa6..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/RunningDistributedCifar10TFJobs.md
+++ /dev/null
@@ -1,164 +0,0 @@
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-# Tutorial: Running Distributed Cifar10 Tensorflow Estimator Example.
-
-## Prepare data for training
-
-CIFAR-10 is a common benchmark in machine learning for image recognition. The example below is based on CIFAR-10 dataset.
-
-1) Checkout https://github.com/tensorflow/models/:
-```
-git clone https://github.com/tensorflow/models/
-```
-
-2) Go to `models/tutorials/image/cifar10_estimator`
-
-3) Generate data by using following command: (required Tensorflow installed)
-
-```
-python generate_cifar10_tfrecords.py --data-dir=cifar-10-data
-```
-
-4) Upload data to HDFS
-
-```
-hadoop fs -put cifar-10-data/ /dataset/cifar-10-data
-```
-
-**Warning:**
-
-Please note that YARN service does not allow multiple services with the same name, so please run following command
-```
-yarn application -destroy <service-name>
-```
-to delete services if you want to reuse the same service name.
-
-## Prepare Docker images
-
-Refer to [Write Dockerfile](WriteDockerfileTF.html) to build a Docker image or use prebuilt one.
-
-## Run Tensorflow jobs
-
-### Run standalone training
-
-```
-yarn jar path/to/hadoop-yarn-applications-submarine-3.2.0-SNAPSHOT.jar \
-   job run --name tf-job-001 --verbose --docker_image tf-1.13.1-gpu:0.0.1 \
-   --input_path hdfs://default/dataset/cifar-10-data \
-   --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/ \
-   --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current \
-   --num_workers 1 --worker_resources memory=8G,vcores=2,gpu=1 \
-   --worker_launch_cmd "cd /test/models/tutorials/image/cifar10_estimator && python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=10000 --eval-batch-size=16 --train-batch-size=16 --num-gpus=2 --sync" \
-   --tensorboard --tensorboard_docker_image tf-1.13.1-cpu:0.0.1
-```
-
-Explanations:
-
-- When access of HDFS is required, the two environments are required to indicate: JAVA_HOME and HDFS_HOME to access libhdfs libraries *inside Docker image*. We will try to eliminate specifying this in the future.
-- Docker image for worker and tensorboard can be specified separately. For this case, Tensorboard does not need GPU, so we will use the CPU Docker image for Tensorboard. (Same for parameter-server in the distributed example below).
-
-### Run distributed training
-
-```
-yarn jar path/to/hadoop-yarn-applications-submarine-3.2.0-SNAPSHOT.jar \
-   job run --name tf-job-001 --verbose --docker_image tf-1.13.1-gpu:0.0.1 \
-   --input_path hdfs://default/dataset/cifar-10-data \
-   --env(s) (same as standalone) \
-   --num_workers 2 \
-   --worker_resources memory=8G,vcores=2,gpu=1 \
-   --worker_launch_cmd "cd /test/models/tutorials/image/cifar10_estimator && python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=10000 --eval-batch-size=16 --train-batch-size=16 --num-gpus=2 --sync"  \
-   --ps_docker_image tf-1.13.1-cpu:0.0.1 \
-   --num_ps 1 --ps_resources memory=4G,vcores=2,gpu=0  \
-   --ps_launch_cmd "cd /test/models/tutorials/image/cifar10_estimator && python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=0" \
-   --tensorboard --tensorboard_docker_image tf-1.13.1-cpu:0.0.1
-```
-
-Explanations:
-
-- `>1` num_workers indicates it is a distributed training.
-- Parameters / resources / Docker image of parameter server can be specified separately. For many cases, parameter server does not require GPU.
-
-For the meaning of the individual parameters, see the [QuickStart](QuickStart.html) page!
-
-*Outputs of distributed training*
-
-Sample output of master:
-```
-...
-allow_soft_placement: true
-, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe77cb15050>, '_tf_config': gpu_options {
-  per_process_gpu_memory_fraction: 1.0
-}
-...
-2018-05-06 22:29:14.656022: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> localhost:8000}
-2018-05-06 22:29:14.656097: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-0.distributed-tf.root.tensorflow.site:8000}
-2018-05-06 22:29:14.656112: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-0.distributed-tf.root.tensorflow.site:8000}
-2018-05-06 22:29:14.659359: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:8000
-...
-INFO:tensorflow:Restoring parameters from hdfs://default/tmp/cifar-10-jobdir/model.ckpt-0
-INFO:tensorflow:Evaluation [1/625]
-INFO:tensorflow:Evaluation [2/625]
-INFO:tensorflow:Evaluation [3/625]
-INFO:tensorflow:Evaluation [4/625]
-INFO:tensorflow:Evaluation [5/625]
-INFO:tensorflow:Evaluation [6/625]
-...
-INFO:tensorflow:Validation (step 1): loss = 1220.6445, global_step = 1, accuracy = 0.1
-INFO:tensorflow:loss = 6.3980675, step = 0
-INFO:tensorflow:loss = 6.3980675, learning_rate = 0.1
-INFO:tensorflow:global_step/sec: 2.34092
-INFO:tensorflow:Average examples/sec: 1931.22 (1931.22), step = 100
-INFO:tensorflow:Average examples/sec: 354.236 (38.6479), step = 110
-INFO:tensorflow:Average examples/sec: 211.096 (38.7693), step = 120
-INFO:tensorflow:Average examples/sec: 156.533 (38.1633), step = 130
-INFO:tensorflow:Average examples/sec: 128.6 (38.7372), step = 140
-INFO:tensorflow:Average examples/sec: 111.533 (39.0239), step = 150
-```
-
-Sample output of worker:
-```
-, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc2a490b050>, '_tf_config': gpu_options {
-  per_process_gpu_memory_fraction: 1.0
-}
-...
-2018-05-06 22:28:45.807936: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-0.distributed-tf.root.tensorflow.site:8000}
-2018-05-06 22:28:45.808040: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-0.distributed-tf.root.tensorflow.site:8000}
-2018-05-06 22:28:45.808064: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8000}
-2018-05-06 22:28:45.809919: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:8000
-...
-INFO:tensorflow:loss = 5.319096, step = 0
-INFO:tensorflow:loss = 5.319096, learning_rate = 0.1
-INFO:tensorflow:Average examples/sec: 49.2338 (49.2338), step = 10
-INFO:tensorflow:Average examples/sec: 52.117 (55.3589), step = 20
-INFO:tensorflow:Average examples/sec: 53.2754 (55.7541), step = 30
-INFO:tensorflow:Average examples/sec: 53.8388 (55.6028), step = 40
-INFO:tensorflow:Average examples/sec: 54.1082 (55.2134), step = 50
-INFO:tensorflow:Average examples/sec: 54.3141 (55.3676), step = 60
-```
-
-Sample output of PS:
-```
-...
-, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4be54dff90>, '_tf_config': gpu_options {
-  per_process_gpu_memory_fraction: 1.0
-}
-...
-2018-05-06 22:28:42.562316: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-0.distributed-tf.root.tensorflow.site:8000}
-2018-05-06 22:28:42.562408: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:8000}
-2018-05-06 22:28:42.562433: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-0.distributed-tf.root.tensorflow.site:8000}
-2018-05-06 22:28:42.564242: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:8000
-```
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/RunningSingleNodeCifar10PTJobs.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/RunningSingleNodeCifar10PTJobs.md
deleted file mode 100644
index ca77c829ac03..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/RunningSingleNodeCifar10PTJobs.md
+++ /dev/null
@@ -1,62 +0,0 @@
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-# Tutorial: Running a standalone Cifar10 PyTorch Estimator Example.
-
-Currently, PyTorch integration with Submarine only supports PyTorch in standalone (non-distributed mode).
-Please also note that HDFS as a data source is not yet supported by PyTorch.
-
-## What is CIFAR-10?
-CIFAR-10 is a common benchmark in machine learning for image recognition. Below example is based on CIFAR-10 dataset.
-
-**Warning:**
-
-Please note that YARN service doesn't allow multiple services with the same name, so please run following command
-```
-yarn application -destroy <service-name>
-```
-to delete services if you want to reuse the same service name.
-
-## Prepare Docker images
-
-Refer to [Write Dockerfile](WriteDockerfilePT.html) to build a Docker image or use prebuilt one.
-
-## Running PyTorch jobs
-
-### Run standalone training
-
-```
-export HADOOP_CLASSPATH="/home/systest/hadoop-submarine-score-yarnservice-runtime-0.2.0-SNAPSHOT.jar:/home/systest/hadoop-submarine-core-0.2.0-SNAPSHOT.jar"
-/opt/hadoop/bin/yarn jar /home/systest/hadoop-submarine-core-0.2.0-SNAPSHOT.jar job run \
---name pytorch-job-001 \
---verbose \
---framework pytorch \
---wait_job_finish \
---docker_image pytorch-latest-gpu:0.0.1 \
---input_path hdfs://unused \
---env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre \
---env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.2 \
---env YARN_CONTAINER_RUNTIME_DOCKER_DELAYED_REMOVAL=true \
---num_workers 1 \
---worker_resources memory=5G,vcores=2 \
---worker_launch_cmd "cd /test/ && python cifar10_tutorial.py"
-
-```
-
-For the meaning of the individual parameters, see the [QuickStart](QuickStart.html) page!
-
-**Remarks:**
-Please note that the input path parameter is mandatory, but not yet used by the PyTorch docker container.
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/TestAndTroubleshooting.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/TestAndTroubleshooting.md
deleted file mode 100644
index 3231aafa6189..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/TestAndTroubleshooting.md
+++ /dev/null
@@ -1,146 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-#### Test with a tensorflow job
-
-Distributed-shell + GPU + cgroup
-
-```bash
- ./yarn jar /home/hadoop/hadoop-current/share/hadoop/yarn/hadoop-yarn-submarine-3.2.0-SNAPSHOT.jar job run \
- --env DOCKER_JAVA_HOME=/opt/java \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name distributed-tf-gpu \
- --env YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_NETWORK=calico-network \
- --worker_docker_image tf-1.13.1-gpu:0.0.1 \
- --ps_docker_image tf-1.13.1-cpu:0.0.1 \
- --input_path hdfs://${dfs_name_service}/tmp/cifar-10-data \
- --checkpoint_path hdfs://${dfs_name_service}/user/hadoop/tf-distributed-checkpoint \
- --num_ps 0 \
- --ps_resources memory=4G,vcores=2,gpu=0 \
- --ps_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=hdfs://${dfs_name_service}/tmp/cifar-10-data --job-dir=hdfs://${dfs_name_service}/tmp/cifar-10-jobdir --num-gpus=0" \
- --worker_resources memory=4G,vcores=2,gpu=1 --verbose \
- --num_workers 1 \
- --worker_launch_cmd "python /test/cifar10_estimator/cifar10_main.py --data-dir=hdfs://${dfs_name_service}/tmp/cifar-10-data --job-dir=hdfs://${dfs_name_service}/tmp/cifar-10-jobdir --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --sync --num-gpus=1"
-```
-
-
-
-## Issues:
-
-### Issue 1: Fail to start NodeManager after system reboot
-
-```
-2018-09-20 18:54:39,785 ERROR org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Failed to bootstrap configured resource subsystems!
-org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Unexpected: Cannot create yarn cgroup Subsystem:cpu Mount points:/proc/mounts User:yarn Path:/sys/fs/cgroup/cpu,cpuacct/hadoop-yarn
-  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.initializePreMountedCGroupController(CGroupsHandlerImpl.java:425)
-  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.initializeCGroupController(CGroupsHandlerImpl.java:377)
-  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.bootstrap(CGroupsCpuResourceHandlerImpl.java:98)
-  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.bootstrap(CGroupsCpuResourceHandlerImpl.java:87)
-  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.bootstrap(ResourceHandlerChain.java:58)
-  at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:320)
-  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:389)
-  at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
-  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:929)
-  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:997)
-2018-09-20 18:54:39,789 INFO org.apache.hadoop.service.AbstractService: Service NodeManager failed in state INITED
-```
-
-Solution: Grant user yarn the access to  `/sys/fs/cgroup/cpu,cpuacct`, which is the subfolder of cgroup mount destination.
-
-```
-chown :yarn -R /sys/fs/cgroup/cpu,cpuacct
-chmod g+rwx -R /sys/fs/cgroup/cpu,cpuacct
-```
-
-If GPUs are used, access to cgroup devices folder is required as well.
-
-```
-chown :yarn -R /sys/fs/cgroup/devices
-chmod g+rwx -R /sys/fs/cgroup/devices
-```
-
-
-### Issue 2: container-executor permission denied
-
-```
-2018-09-21 09:36:26,102 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor: IOException executing command:
-java.io.IOException: Cannot run program "/etc/yarn/sbin/Linux-amd64-64/container-executor": error=13, Permission denied
-        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
-        at org.apache.hadoop.util.Shell.runCommand(Shell.java:938)
-        at org.apache.hadoop.util.Shell.run(Shell.java:901)
-        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)
-```
-
-Solution: The permission of `/etc/yarn/sbin/Linux-amd64-64/container-executor` should be 6050
-
-### Issue 3：How to get docker service log
-
-Solution: we can get docker log with the following command
-
-```
-journalctl -u docker
-```
-
-### Issue 4：docker can't remove containers with errors like `device or resource busy`
-
-```bash
-$ docker rm 0bfafa146431
-Error response from daemon: Unable to remove filesystem for 0bfafa146431771f6024dcb9775ef47f170edb2f1852f71916ba44209ca6120a: remove /app/docker/containers/0bfafa146431771f6024dcb9775ef47f170edb2f152f71916ba44209ca6120a/shm: device or resource busy
-```
-
-Solution: to find which process leads to a `device or resource busy`, we can add a shell script, named `find-busy-mnt.sh`
-
-```bash
-#!/bin/bash
-
-# A simple script to get information about mount points and pids and their
-# mount namespaces.
-
-if [ $# -ne 1 ];then
-echo "Usage: $0 <devicemapper-device-id>"
-exit 1
-fi
-
-ID=$1
-
-MOUNTS=`find /proc/*/mounts | xargs grep $ID 2>/dev/null`
-
-[ -z "$MOUNTS" ] &&  echo "No pids found" && exit 0
-
-printf "PID\tNAME\t\tMNTNS\n"
-echo "$MOUNTS" | while read LINE; do
-PID=`echo $LINE | cut -d ":" -f1 | cut -d "/" -f3`
-# Ignore self and thread-self
-if [ "$PID" == "self" ] || [ "$PID" == "thread-self" ]; then
-  continue
-fi
-NAME=`ps -q $PID -o comm=`
-MNTNS=`readlink /proc/$PID/ns/mnt`
-printf "%s\t%s\t\t%s\n" "$PID" "$NAME" "$MNTNS"
-done
-```
-
-Kill the process by pid, which is found by the script
-
-```bash
-$ chmod +x find-busy-mnt.sh
-./find-busy-mnt.sh 0bfafa146431771f6024dcb9775ef47f170edb2f152f71916ba44209ca6120a
-# PID   NAME            MNTNS
-# 5007  ntpd            mnt:[4026533598]
-$ kill -9 5007
-```
-
-### Issue 5：YARN fails to start containers
-
-If the number of GPUs required by an application is greater than the number of GPUs in the cluster, some container will not be created.
-
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/TonYRuntimeGuide.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/TonYRuntimeGuide.md
deleted file mode 100644
index 72e2d0ff1a10..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/TonYRuntimeGuide.md
+++ /dev/null
@@ -1,309 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-# Quick Start Guide
-
-## Prerequisite
-
-Must:
-
-- Apache Hadoop 2.7 or above.
-- TonY library 0.3.2 or above. You could download latest TonY jar from
-https://github.com/linkedin/TonY/releases.
-
-Optional:
-
-- Enable GPU on YARN support (when GPU-based training is required, Hadoop 3.1 and above).
-- Enable Docker support on Hadoop (Hadoop 2.9 and above).
-
-## Run jobs
-
-### Commandline options
-
-```$xslt
-usage:
- -docker_image <arg>          Docker image name/tag
- -env <arg>                   Common environment variable of worker/ps
- -name <arg>                  Name of the job
- -num_ps <arg>                Number of PS tasks of the job, by default
-                              it's 0
- -num_workers <arg>           Numnber of worker tasks of the job, by
-                              default it's 1
- -ps_docker_image <arg>       Specify docker image for PS, when this is
-                              not specified, PS uses --docker_image as
-                              default.
- -ps_launch_cmd <arg>         Commandline of worker, arguments will be
-                              directly used to launch the PS
- -ps_resources <arg>          Resource of each PS, for example
-                              memory-mb=2048,vcores=2,yarn.io/gpu=2
- -queue <arg>                 Name of queue to run the job, by default it
-                              uses default queue
- -saved_model_path <arg>      Model exported path (savedmodel) of the job,
-                              which is needed when exported model is not
-                              placed under ${checkpoint_path}could be
-                              local or other FS directory. This will be
-                              used to serve.
- -tensorboard <arg>           Should we run TensorBoard for this job? By
-                              default it's true
- -verbose                     Print verbose log for troubleshooting
- -wait_job_finish             Specified when user want to wait the job
-                              finish
- -worker_docker_image <arg>   Specify docker image for WORKER, when this
-                              is not specified, WORKER uses --docker_image
-                              as default.
- -worker_launch_cmd <arg>     Commandline of worker, arguments will be
-                              directly used to launch the worker
- -worker_resources <arg>      Resource of each worker, for example
-                              memory-mb=2048,vcores=2,yarn.io/gpu=2
- -localization <arg>          Specify localization to remote/local
-                              file/directory available to all container(Docker).
-                              Argument format is "RemoteUri:LocalFileName"
-                              The LocalFilePath is the local file or folder name.
-                              You should access it with relative path to working directory.
-                              This option can be set mutiple times.
-                              Examples are
-                              -localization "hdfs:///user/yarn/mydir2:data"
-                              -localization "s3a:///a/b/myfile1:file1"
-                              -localization "https:///a/b/myfile2:myfile"
-                              -localization "/user/yarn/mydir3:mydir3"
-                              -localization "./mydir1:mydir1"
- -insecure                    Whether running in an insecure cluster
- -conf                        Override configurations via commandline
-```
-
-> Note: all --localization files will be localized to working directory. You should access them use
-relative path. Alternatively, you could use `--conf tony.containers.resources
-=src_file::dest_file_name,src_file2::dest_file_name2`. It accepts a list of resources to be localized to all containers,
-delimited by comma. If a resource has no scheme like `hdfs://` or `s3://`, the file is considered a local file. You
-could add #archive annotation, if an entry has `#archive`, the file will be automatically unzipped when localized to the
-containers, folder name is the same as the file name. For example: `/user/khu/abc.zip#archive` would be inferred as a
-local file and will be unarchived in containers. You would anticipate an abc.zip/ folder in your container's working
-directory. Annotation `::` is added since TonY 0.3.3. If you use `PATH/TO/abc.txt::def.txt`, the `abc.txt` file
-would be localized as `def.txt` in the container working directory.
-Details: [tony configurations](https://github.com/linkedin/TonY/wiki/TonY-Configurations)
-
-### Submarine Configuration
-
-For submarine internal configuration, please create a `submarine.xml` which should be placed under `$HADOOP_CONF_DIR`.
-Make sure you set `submarine.runtime.class` to `org.apache.hadoop.yarn.submarine.runtimes.tony.TonyRuntimeFactory`
-
-|Configuration Name | Description |
-|:---- |:---- |
-| `submarine.runtime.class` | org.apache.hadoop.yarn.submarine.runtimes.tony.TonyRuntimeFactory
-| `submarine.localization.max-allowed-file-size-mb` | Optional. This sets a size limit to the file/directory to be localized in "-localization" CLI option. 2GB by default. |
-
-
-
-### Launch TensorFlow Application:
-
-#### Commandline
-
-### Without Docker
-
-You need:
-* Build a Python virtual environment with TensorFlow 1.13.1 installed
-* A cluster with Hadoop 2.7 or above.
-
-### Building a Python virtual environment with TensorFlow
-
-TonY requires a Python virtual environment zip with TensorFlow and any needed Python libraries already installed.
-
-```
-wget https://files.pythonhosted.org/packages/33/bc/fa0b5347139cd9564f0d44ebd2b147ac97c36b2403943dbee8a25fd74012/virtualenv-16.0.0.tar.gz
-tar xf virtualenv-16.0.0.tar.gz
-
-# Make sure to install using Python 3, as TensorFlow only provides Python 3 artifacts
-python virtualenv-16.0.0/virtualenv.py venv
-. venv/bin/activate
-pip install tensorflow==1.13.1
-zip -r venv.zip venv
-```
-
-### TensorFlow version
-
- - Version 1.13.1
-
-**Note:** If you require a past version of TensorFlow and TensorBoard, take a look at [this](https://github.com/linkedin/TonY/issues/42) issue.
-
-
-### Installing Hadoop
-
-TonY only requires YARN, not HDFS. Please see the [open-source documentation](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html) on how to set YARN up.
-
-### Get the training examples
-
-Get mnist_distributed.py from https://github.com/linkedin/TonY/tree/master/tony-examples/mnist-tensorflow
-
-
-```
-CLASSPATH=$(hadoop classpath --glob): \
-./hadoop-submarine-core/target/hadoop-submarine-core-0.2.0-SNAPSHOT.jar: \
-./hadoop-submarine-yarnservice-runtime/target/hadoop-submarine-score-yarnservice-runtime-0.2.0-SNAPSHOT.jar: \
-./hadoop-submarine-tony-runtime/target/hadoop-submarine-tony-runtime-0.2.0-SNAPSHOT.jar: \
-/home/pi/hadoop/TonY/tony-cli/build/libs/tony-cli-0.3.11-all.jar \
-
-java org.apache.hadoop.yarn.submarine.client.cli.Cli job run --name tf-job-001 \
- --framework tensorflow \
- --num_workers 2 \
- --worker_resources memory=3G,vcores=2 \
- --num_ps 2 \
- --ps_resources memory=3G,vcores=2 \
- --worker_launch_cmd "venv.zip/venv/bin/python mnist_distributed.py --steps 1000 --data_dir /tmp/data --working_dir /tmp/mode" \
- --ps_launch_cmd "venv.zip/venv/bin/python mnist_distributed.py --steps 1000 --data_dir /tmp/data --working_dir /tmp/mode" \
- --insecure
- --conf tony.containers.resources=PATH_TO_VENV_YOU_CREATED/venv.zip#archive,PATH_TO_MNIST_EXAMPLE/mnist_distributed.py, \
-PATH_TO_TONY_CLI_JAR/tony-cli-0.3.11-all.jar
-
-```
-
-You should then be able to see links and status of the jobs from command line:
-
-```
-2019-04-22 20:30:42,611 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi status: RUNNING
-2019-04-22 20:30:42,612 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 1 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi status: RUNNING
-2019-04-22 20:30:42,612 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: ps index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi status: RUNNING
-2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for ps 0 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi
-2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for worker 0 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi
-2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for worker 1 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi
-2019-04-22 20:30:44,625 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: ps index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi status: FINISHED
-2019-04-22 20:30:44,625 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi status: FINISHED
-2019-04-22 20:30:44,626 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 1 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi status: FINISHED
-
-```
-
-### With Docker
-
-```
-CLASSPATH=$(hadoop classpath --glob): \
-./hadoop-submarine-core/target/hadoop-submarine-core-0.2.0-SNAPSHOT.jar: \
-./hadoop-submarine-yarnservice-runtime/target/hadoop-submarine-score-yarnservice-runtime-0.2.0-SNAPSHOT.jar: \
-./hadoop-submarine-tony-runtime/target/hadoop-submarine-tony-runtime-0.2.0-SNAPSHOT.jar: \
-/home/pi/hadoop/TonY/tony-cli/build/libs/tony-cli-0.3.11-all.jar \
-
-java org.apache.hadoop.yarn.submarine.client.cli.Cli job run --name tf-job-001 \
- --framework tensorflow \
- --docker_image hadoopsubmarine/tf-1.8.0-cpu:0.0.3 \
- --input_path hdfs://pi-aw:9000/dataset/cifar-10-data \
- --worker_resources memory=3G,vcores=2 \
- --worker_launch_cmd "export CLASSPATH=\$(/hadoop-3.1.0/bin/hadoop classpath --glob) && cd /test/models/tutorials/image/cifar10_estimator && python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=10000 --eval-batch-size=16 --train-batch-size=16 --variable-strategy=CPU --num-gpus=0 --sync" \
- --env JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.0 \
- --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
- --env HADOOP_HOME=/hadoop-3.1.0 \
- --env HADOOP_YARN_HOME=/hadoop-3.1.0 \
- --env HADOOP_COMMON_HOME=/hadoop-3.1.0 \
- --env HADOOP_HDFS_HOME=/hadoop-3.1.0 \
- --env HADOOP_CONF_DIR=/hadoop-3.1.0/etc/hadoop \
- --conf tony.containers.resources=/home/pi/hadoop/TonY/tony-cli/build/libs/tony-cli-0.3.11-all.jar
-```
-
-
-### Launch PyToch Application:
-
-#### Commandline
-
-### Without Docker
-
-You need:
-* Build a Python virtual environment with PyTorch 0.4.* installed
-* A cluster with Hadoop 2.7 or above.
-
-### Building a Python virtual environment with PyTorch
-
-TonY requires a Python virtual environment zip with PyTorch and any needed Python libraries already installed.
-
-```
-wget https://files.pythonhosted.org/packages/33/bc/fa0b5347139cd9564f0d44ebd2b147ac97c36b2403943dbee8a25fd74012/virtualenv-16.0.0.tar.gz
-tar xf virtualenv-16.0.0.tar.gz
-
-python virtualenv-16.0.0/virtualenv.py venv
-. venv/bin/activate
-pip install pytorch==0.4.0
-zip -r venv.zip venv
-```
-
-### PyTorch version
-
- - Version 0.4.0+
-
-
-### Installing Hadoop
-
-TonY only requires YARN, not HDFS. Please see the [open-source documentation](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html) on how to set YARN up.
-
-### Get the training examples
-
-Get mnist_distributed.py from https://github.com/linkedin/TonY/tree/master/tony-examples/mnist-pytorch
-
-
-```
-CLASSPATH=$(hadoop classpath --glob): \
-./hadoop-submarine-core/target/hadoop-submarine-core-0.2.0-SNAPSHOT.jar: \
-./hadoop-submarine-yarnservice-runtime/target/hadoop-submarine-score-yarnservice-runtime-0.2.0-SNAPSHOT.jar: \
-./hadoop-submarine-tony-runtime/target/hadoop-submarine-tony-runtime-0.2.0-SNAPSHOT.jar: \
-/home/pi/hadoop/TonY/tony-cli/build/libs/tony-cli-0.3.11-all.jar \
-
-java org.apache.hadoop.yarn.submarine.client.cli.Cli job run --name tf-job-001 \
- --num_workers 2 \
- --worker_resources memory=3G,vcores=2 \
- --num_ps 2 \
- --ps_resources memory=3G,vcores=2 \
- --worker_launch_cmd "venv.zip/venv/bin/python mnist_distributed.py" \
- --ps_launch_cmd "venv.zip/venv/bin/python mnist_distributed.py" \
- --insecure \
- --conf tony.containers.resources=PATH_TO_VENV_YOU_CREATED/venv.zip#archive,PATH_TO_MNIST_EXAMPLE/mnist_distributed.py, \
-PATH_TO_TONY_CLI_JAR/tony-cli-0.3.11-all.jar \
---conf tony.application.framework=pytorch
-
-```
-You should then be able to see links and status of the jobs from command line:
-
-```
-2019-04-22 20:30:42,611 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi status: RUNNING
-2019-04-22 20:30:42,612 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 1 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi status: RUNNING
-2019-04-22 20:30:42,612 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: ps index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi status: RUNNING
-2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for ps 0 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi
-2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for worker 0 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi
-2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for worker 1 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi
-2019-04-22 20:30:44,625 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: ps index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi status: FINISHED
-2019-04-22 20:30:44,625 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi status: FINISHED
-2019-04-22 20:30:44,626 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 1 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi status: FINISHED
-
-```
-
-### With Docker
-
-```
-CLASSPATH=$(hadoop classpath --glob): \
-./hadoop-submarine-core/target/hadoop-submarine-core-0.2.0-SNAPSHOT.jar: \
-./hadoop-submarine-yarnservice-runtime/target/hadoop-submarine-score-yarnservice-runtime-0.2.0-SNAPSHOT.jar: \
-./hadoop-submarine-tony-runtime/target/hadoop-submarine-tony-runtime-0.2.0-SNAPSHOT.jar: \
-/home/pi/hadoop/TonY/tony-cli/build/libs/tony-cli-0.3.11-all.jar \
-
-java org.apache.hadoop.yarn.submarine.client.cli.Cli job run --name tf-job-001 \
- --docker_image hadoopsubmarine/tf-1.8.0-cpu:0.0.3 \
- --input_path hdfs://pi-aw:9000/dataset/cifar-10-data \
- --worker_resources memory=3G,vcores=2 \
- --worker_launch_cmd "export CLASSPATH=\$(/hadoop-3.1.0/bin/hadoop classpath --glob) && cd /test/models/tutorials/image/cifar10_estimator && python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=10000 --eval-batch-size=16 --train-batch-size=16 --variable-strategy=CPU --num-gpus=0 --sync" \
- --env JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
- --env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.0 \
- --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
- --env HADOOP_HOME=/hadoop-3.1.0 \
- --env HADOOP_YARN_HOME=/hadoop-3.1.0 \
- --env HADOOP_COMMON_HOME=/hadoop-3.1.0 \
- --env HADOOP_HDFS_HOME=/hadoop-3.1.0 \
- --env HADOOP_CONF_DIR=/hadoop-3.1.0/etc/hadoop \
- --conf tony.containers.resources=PATH_TO_TONY_CLI_JAR/tony-cli-0.3.11-all.jar \
- --conf tony.application.framework=pytorch
-```
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/WriteDockerfilePT.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/WriteDockerfilePT.md
deleted file mode 100644
index 84ca479978eb..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/WriteDockerfilePT.md
+++ /dev/null
@@ -1,114 +0,0 @@
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-# Creating Docker Images for Running PyTorch on YARN
-
-## How to create docker images to run PyTorch on YARN
-
-Dockerfile to run PyTorch on YARN needs two parts:
-
-**Base libraries which PyTorch depends on**
-
-1) OS base image, for example ```ubuntu:16.04```
-
-2) PyTorch dependent libraries and packages. For example ```python```, ```scipy```. For GPU support, you also need ```cuda```, ```cudnn```, etc.
-
-3) PyTorch package.
-
-**Libraries to access HDFS**
-
-1) JDK
-
-2) Hadoop
-
-Here's an example of a base image (with GPU support) to install PyTorch:
-```
-FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04
-ARG PYTHON_VERSION=3.6
-RUN apt-get update && apt-get install -y --no-install-recommends \
-         build-essential \
-         cmake \
-         git \
-         curl \
-         vim \
-         ca-certificates \
-         libjpeg-dev \
-         libpng-dev \
-         wget &&\
-     rm -rf /var/lib/apt/lists/*
-
-
-RUN curl -o ~/miniconda.sh -O  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh  && \
-     chmod +x ~/miniconda.sh && \
-     ~/miniconda.sh -b -p /opt/conda && \
-     rm ~/miniconda.sh && \
-     /opt/conda/bin/conda install -y python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl mkl-include cython typing && \
-     /opt/conda/bin/conda install -y -c pytorch magma-cuda100 && \
-     /opt/conda/bin/conda clean -ya
-ENV PATH /opt/conda/bin:$PATH
-RUN pip install ninja
-# This must be done before pip so that requirements.txt is available
-WORKDIR /opt/pytorch
-RUN git clone https://github.com/pytorch/pytorch.git
-WORKDIR pytorch
-RUN git submodule update --init
-RUN TORCH_CUDA_ARCH_LIST="3.5 5.2 6.0 6.1 7.0+PTX" TORCH_NVCC_FLAGS="-Xfatbin -compress-all" \
-    CMAKE_PREFIX_PATH="$(dirname $(which conda))/../" \
-    pip install -v .
-
-WORKDIR /opt/pytorch
-RUN git clone https://github.com/pytorch/vision.git && cd vision && pip install -v .
-
-```
-
-On top of above image, add files, install packages to access HDFS
-```
-RUN apt-get update && apt-get install -y openjdk-8-jdk wget
-# Install hadoop
-ENV HADOOP_VERSION="3.1.2"
-RUN wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
-RUN tar zxf hadoop-${HADOOP_VERSION}.tar.gz
-RUN ln -s hadoop-${HADOOP_VERSION} hadoop-current
-RUN rm hadoop-${HADOOP_VERSION}.tar.gz
-```
-
-Build and push to your own docker registry: Use ```docker build ... ``` and ```docker push ...``` to finish this step.
-
-## Use examples to build your own PyTorch docker images
-
-We provided some example Dockerfiles for you to build your own PyTorch docker images.
-
-For latest PyTorch
-
-- *docker/pytorch/base/ubuntu-16.04/Dockerfile.gpu.pytorch_latest*: Latest Pytorch that supports GPU, which is prebuilt to CUDA10.
-- *docker/pytorch/with-cifar10-models/ubuntu-16.04/Dockerfile.gpu.pytorch_latest*: Latest Pytorch that GPU, which is prebuilt to CUDA10, with models.
-
-## Build Docker images
-
-### Manually build Docker image:
-
-Under `docker/pytorch` directory, run `build-all.sh` to build all Docker images. This command will build the following Docker images:
-
-- `pytorch-latest-gpu-base:0.0.1` for base Docker image which includes Hadoop, PyTorch, GPU base libraries.
-- `pytorch-latest-gpu:0.0.1` which includes cifar10 model as well
-
-### Use prebuilt images
-
-(No liability)
-You can also use prebuilt images for convenience:
-
-- hadoopsubmarine/pytorch-latest-gpu-base:0.0.1
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/WriteDockerfileTF.md b/hadoop-submarine/hadoop-submarine-core/src/site/markdown/WriteDockerfileTF.md
deleted file mode 100644
index 5dc565d06830..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/markdown/WriteDockerfileTF.md
+++ /dev/null
@@ -1,123 +0,0 @@
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-# Creating Docker Images for Running Tensorflow on YARN
-
-## How to create docker images to run Tensorflow on YARN
-
-Dockerfile to run Tensorflow on YARN need two part:
-
-**Base libraries which Tensorflow depends on**
-
-1) OS base image, for example ```ubuntu:16.04```
-
-2) Tensorflow depended libraries and packages. For example ```python```, ```scipy```. For GPU support, need ```cuda```, ```cudnn```, etc.
-
-3) Tensorflow package.
-
-**Libraries to access HDFS**
-
-1) JDK
-
-2) Hadoop
-
-Here's an example of a base image (w/o GPU support) to install Tensorflow:
-```
-FROM ubuntu:16.04
-
-# Pick up some TF dependencies
-RUN apt-get update && apt-get install -y --no-install-recommends \
-        build-essential \
-        curl \
-        libfreetype6-dev \
-        libpng12-dev \
-        libzmq3-dev \
-        pkg-config \
-        python \
-        python-dev \
-        rsync \
-        software-properties-common \
-        unzip \
-        && \
-    apt-get clean && \
-    rm -rf /var/lib/apt/lists/*
-
-RUN export DEBIAN_FRONTEND=noninteractive && apt-get update && apt-get install -yq krb5-user libpam-krb5 && apt-get clean
-
-RUN curl -O https://bootstrap.pypa.io/get-pip.py && \
-    python get-pip.py && \
-    rm get-pip.py
-
-RUN pip --no-cache-dir install \
-        Pillow \
-        h5py \
-        ipykernel \
-        jupyter \
-        matplotlib \
-        numpy \
-        pandas \
-        scipy \
-        sklearn \
-        && \
-    python -m ipykernel.kernelspec
-
-RUN pip --no-cache-dir install \
-    http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.13.1-cp27-none-linux_x86_64.whl
-```
-
-On top of above image, add files, install packages to access HDFS
-```
-RUN apt-get update && apt-get install -y openjdk-8-jdk wget
-# Install hadoop
-ENV HADOOP_VERSION="3.1.2"
-RUN wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
-RUN tar zxf hadoop-${HADOOP_VERSION}.tar.gz
-RUN ln -s hadoop-${HADOOP_VERSION} hadoop-current
-RUN rm hadoop-${HADOOP_VERSION}.tar.gz
-```
-
-Build and push to your own docker registry: Use ```docker build ... ``` and ```docker push ...``` to finish this step.
-
-## Use examples to build your own Tensorflow docker images
-
-We provided following examples for you to build tensorflow docker images.
-
-For Tensorflow 1.13.1 (Precompiled to CUDA 10.x)
-
-- *docker/tensorflow/base/ubuntu-16.04/Dockerfile.cpu.tf_1.13.1*: Tensorflow 1.13.1 supports CPU only.
-- *docker/tensorflow/with-cifar10-models/ubuntu-16.04/Dockerfile.cpu.tf_1.13.1*: Tensorflow 1.13.1 supports CPU only, and included models
-- *docker/tensorflow/base/ubuntu-16.04/Dockerfile.gpu.tf_1.13.1*: Tensorflow 1.13.1 supports GPU, which is prebuilt to CUDA10.
-- *docker/tensorflow/with-cifar10-models/ubuntu-16.04/Dockerfile.gpu.tf_1.13.1*: Tensorflow 1.13.1 supports GPU, which is prebuilt to CUDA10, with models.
-
-## Build Docker images
-
-### Manually build Docker image:
-
-Under `docker/` directory, run `build-all.sh` to build Docker images. It will build following images:
-
-- `tf-1.13.1-gpu-base:0.0.1` for base Docker image which includes Hadoop, Tensorflow, GPU base libraries.
-- `tf-1.13.1-gpu-base:0.0.1` for base Docker image which includes Hadoop. Tensorflow.
-- `tf-1.13.1-gpu:0.0.1` which includes cifar10 model
-- `tf-1.13.1-cpu:0.0.1` which inclues cifar10 model (cpu only).
-
-### Use prebuilt images
-
-(No liability)
-You can also use prebuilt images for convenience:
-
-- hadoopsubmarine/tf-1.13.1-gpu:0.0.1
-- hadoopsubmarine/tf-1.13.1-cpu:0.0.1
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/resources/css/site.css b/hadoop-submarine/hadoop-submarine-core/src/site/resources/css/site.css
deleted file mode 100644
index 7315db31e53c..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/resources/css/site.css
+++ /dev/null
@@ -1,29 +0,0 @@
-/*
-* Licensed to the Apache Software Foundation (ASF) under one or more
-* contributor license agreements.  See the NOTICE file distributed with
-* this work for additional information regarding copyright ownership.
-* The ASF licenses this file to You under the Apache License, Version 2.0
-* (the "License"); you may not use this file except in compliance with
-* the License.  You may obtain a copy of the License at
-*
-*     http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing, software
-* distributed under the License is distributed on an "AS IS" BASIS,
-* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-* See the License for the specific language governing permissions and
-* limitations under the License.
-*/
-#banner {
-  height: 93px;
-  background: none;
-}
-
-#bannerLeft img {
-  margin-left: 30px;
-  margin-top: 10px;
-}
-
-#bannerRight img {
-  margin: 17px;
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/job-logs-ui.png b/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/job-logs-ui.png
deleted file mode 100644
index 18b9e17f1076..000000000000
Binary files a/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/job-logs-ui.png and /dev/null differ
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/multiple-tensorboard-jobs.png b/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/multiple-tensorboard-jobs.png
deleted file mode 100644
index 8e3db797269a..000000000000
Binary files a/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/multiple-tensorboard-jobs.png and /dev/null differ
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/submarine-installer.gif b/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/submarine-installer.gif
deleted file mode 100644
index 56b3b690f0ad..000000000000
Binary files a/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/submarine-installer.gif and /dev/null differ
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/tensorboard-service.png b/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/tensorboard-service.png
deleted file mode 100644
index 3251d74531bf..000000000000
Binary files a/hadoop-submarine/hadoop-submarine-core/src/site/resources/images/tensorboard-service.png and /dev/null differ
diff --git a/hadoop-submarine/hadoop-submarine-core/src/site/site.xml b/hadoop-submarine/hadoop-submarine-core/src/site/site.xml
deleted file mode 100644
index 5feae9a879b8..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/site/site.xml
+++ /dev/null
@@ -1,28 +0,0 @@
-<!--
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License. See accompanying LICENSE file.
--->
-<project name="Apache Hadoop ${project.version}">
-
-  <skin>
-    <groupId>org.apache.maven.skins</groupId>
-    <artifactId>maven-stylus-skin</artifactId>
-    <version>${maven-stylus-skin.version}</version>
-  </skin>
-
-  <body>
-    <links>
-      <item name="Apache Hadoop" href="http://hadoop.apache.org/"/>
-    </links>
-  </body>
-
-</project>
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/TestKillJobCliParsing.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/TestKillJobCliParsing.java
deleted file mode 100644
index db654dbf3d73..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/TestKillJobCliParsing.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli;
-
-import java.io.IOException;
-
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.param.KillJobParameters;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineException;
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestKillJobCliParsing {
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @Test
-  public void testPrintHelp() {
-    MockClientContext mockClientContext = new MockClientContext();
-    KillJobCli killJobCli = new KillJobCli(mockClientContext);
-    killJobCli.printUsages();
-  }
-
-  @Test
-  public void testKillJob()
-      throws InterruptedException, SubmarineException, YarnException,
-      ParseException, IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-    KillJobCli killJobCli = new KillJobCli(mockClientContext) {
-      @Override
-      protected boolean killJob() {
-        // do nothing
-        return false;
-      }
-    };
-    killJobCli.run(new String[] { "--name", "my-job" });
-    KillJobParameters parameters = killJobCli.getParameters();
-    Assert.assertEquals(parameters.getName(), "my-job");
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/TestShowJobCliParsing.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/TestShowJobCliParsing.java
deleted file mode 100644
index 9c0d872d623c..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/TestShowJobCliParsing.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.hadoop.yarn.submarine.client.cli;
-
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ShowJobParameters;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineException;
-import org.apache.hadoop.yarn.submarine.runtimes.RuntimeFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.common.MemorySubmarineStorage;
-import org.apache.hadoop.yarn.submarine.runtimes.common.StorageKeyConstants;
-import org.apache.hadoop.yarn.submarine.runtimes.common.SubmarineStorage;
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-public class TestShowJobCliParsing {
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @Test
-  public void testPrintHelp() {
-    MockClientContext mockClientContext = new MockClientContext();
-    ShowJobCli showJobCli = new ShowJobCli(mockClientContext);
-    showJobCli.printUsages();
-  }
-
-  @Test
-  public void testShowJob()
-      throws InterruptedException, SubmarineException, YarnException,
-      ParseException, IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-    ShowJobCli showJobCli = new ShowJobCli(mockClientContext) {
-      @Override
-      protected void getAndPrintJobInfo() {
-        // do nothing
-      }
-    };
-    showJobCli.run(new String[] { "--name", "my-job" });
-    ShowJobParameters parameters = showJobCli.getParameters();
-    Assert.assertEquals(parameters.getName(), "my-job");
-  }
-
-  private Map<String, String> getMockJobInfo(String jobName) {
-    Map<String, String> map = new HashMap<>();
-    map.put(StorageKeyConstants.APPLICATION_ID,
-        ApplicationId.newInstance(1234L, 1).toString());
-    map.put(StorageKeyConstants.JOB_RUN_ARGS, "job run 123456");
-    map.put(StorageKeyConstants.INPUT_PATH, "hdfs://" + jobName);
-    return map;
-  }
-
-  @Test
-  public void testSimpleShowJob()
-      throws InterruptedException, SubmarineException, YarnException,
-      ParseException, IOException {
-    SubmarineStorage storage = new MemorySubmarineStorage();
-    MockClientContext mockClientContext = new MockClientContext();
-    RuntimeFactory runtimeFactory = mock(RuntimeFactory.class);
-    when(runtimeFactory.getSubmarineStorage()).thenReturn(storage);
-    mockClientContext.setRuntimeFactory(runtimeFactory);
-
-    ShowJobCli showJobCli = new ShowJobCli(mockClientContext);
-
-    try {
-      showJobCli.run(new String[] { "--name", "my-job" });
-    } catch (IOException e) {
-      // expected
-    }
-
-
-    storage.addNewJob("my-job", getMockJobInfo("my-job"));
-    showJobCli.run(new String[] { "--name", "my-job" });
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/YamlConfigTestUtils.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/YamlConfigTestUtils.java
deleted file mode 100644
index 6ac201ffe0fe..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/YamlConfigTestUtils.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters.UnderscoreConverterPropertyUtils;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.YamlConfigFile;
-import org.yaml.snakeyaml.Yaml;
-import org.yaml.snakeyaml.constructor.Constructor;
-
-import java.io.File;
-import java.io.IOException;
-import java.io.InputStream;
-
-/**
- * Test utility class for test code that deals with YAML configuration parsing.
- */
-public final class YamlConfigTestUtils {
-
-  private YamlConfigTestUtils() {}
-
-  public static void deleteFile(File file) {
-    if (file != null) {
-      file.delete();
-    }
-  }
-
-  public static YamlConfigFile readYamlConfigFile(String filename) {
-    Constructor constructor = new Constructor(YamlConfigFile.class);
-    constructor.setPropertyUtils(new UnderscoreConverterPropertyUtils());
-    Yaml yaml = new Yaml(constructor);
-    InputStream inputStream = YamlConfigTestUtils.class
-        .getClassLoader()
-        .getResourceAsStream(filename);
-    return yaml.loadAs(inputStream, YamlConfigFile.class);
-  }
-
-  public static File createTempFileWithContents(String filename)
-      throws IOException {
-    InputStream inputStream = YamlConfigTestUtils.class
-        .getClassLoader()
-        .getResourceAsStream(filename);
-    File targetFile = File.createTempFile("test", ".yaml");
-    FileUtils.copyInputStreamToFile(inputStream, targetFile);
-    return targetFile;
-  }
-
-  public static File createEmptyTempFile() throws IOException {
-    return File.createTempFile("test", ".yaml");
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingCommon.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingCommon.java
deleted file mode 100644
index 1dbf1f1dbd27..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingCommon.java
+++ /dev/null
@@ -1,129 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob;
-
-import org.apache.commons.cli.MissingArgumentException;
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.runtimes.RuntimeFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobMonitor;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.common.SubmarineStorage;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-import java.io.IOException;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-import static org.mockito.ArgumentMatchers.any;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-/**
- * This class contains some test methods to test common functionality
- * (including TF / PyTorch) of the run job Submarine command.
- */
-public class TestRunJobCliParsingCommon {
-
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @Rule
-  public ExpectedException expectedException = ExpectedException.none();
-
-  public static MockClientContext getMockClientContext()
-      throws IOException, YarnException {
-    MockClientContext mockClientContext = new MockClientContext();
-    JobSubmitter mockJobSubmitter = mock(JobSubmitter.class);
-    when(mockJobSubmitter.submitJob(any(ParametersHolder.class)))
-        .thenReturn(ApplicationId.newInstance(1235L, 1));
-
-    JobMonitor mockJobMonitor = mock(JobMonitor.class);
-    SubmarineStorage storage = mock(SubmarineStorage.class);
-    RuntimeFactory rtFactory = mock(RuntimeFactory.class);
-
-    when(rtFactory.getJobSubmitterInstance()).thenReturn(mockJobSubmitter);
-    when(rtFactory.getJobMonitorInstance()).thenReturn(mockJobMonitor);
-    when(rtFactory.getSubmarineStorage()).thenReturn(storage);
-
-    mockClientContext.setRuntimeFactory(rtFactory);
-    return mockClientContext;
-  }
-
-  @Test
-  public void testAbsentFrameworkFallsBackToTensorFlow() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    runJobCli.run(
-        new String[]{"--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input", "--checkpoint_path",
-            "hdfs://output",
-            "--num_workers", "1", "--worker_launch_cmd", "python run-job.py",
-            "--worker_resources", "memory=4g,vcores=2", "--tensorboard",
-            "true", "--verbose", "--wait_job_finish"});
-    RunJobParameters runJobParameters = runJobCli.getRunJobParameters();
-    assertTrue("Default Framework should be TensorFlow!",
-        runJobParameters instanceof TensorFlowRunJobParameters);
-  }
-
-  @Test
-  public void testEmptyFrameworkOption() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    expectedException.expect(MissingArgumentException.class);
-    expectedException.expectMessage("Missing argument for option: framework");
-
-    runJobCli.run(
-        new String[]{"--framework", "--name", "my-job",
-            "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input", "--checkpoint_path",
-            "hdfs://output",
-            "--num_workers", "1", "--worker_launch_cmd", "python run-job.py",
-            "--worker_resources", "memory=4g,vcores=2", "--tensorboard",
-            "true", "--verbose", "--wait_job_finish"});
-  }
-
-  @Test
-  public void testInvalidFrameworkOption() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    expectedException.expect(ParseException.class);
-    expectedException.expectMessage("Failed to parse Framework type");
-
-    runJobCli.run(
-        new String[]{"--framework", "bla", "--name", "my-job",
-            "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input", "--checkpoint_path",
-            "hdfs://output",
-            "--num_workers", "1", "--worker_launch_cmd", "python run-job.py",
-            "--worker_resources", "memory=4g,vcores=2", "--tensorboard",
-            "true", "--verbose", "--wait_job_finish"});
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingCommonYaml.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingCommonYaml.java
deleted file mode 100644
index f174f04cf75e..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingCommonYaml.java
+++ /dev/null
@@ -1,255 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob;
-
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.YamlConfigTestUtils;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.YamlParseException;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineRuntimeException;
-import org.apache.hadoop.yarn.submarine.common.resource.ResourceUtils;
-import org.junit.After;
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-
-import static org.apache.hadoop.yarn.submarine.client.cli.runjob.TestRunJobCliParsingCommon.getMockClientContext;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-/**
- * This class contains some test methods to test common YAML parsing
- * functionality (including TF / PyTorch) of the run job Submarine command.
- */
-public class TestRunJobCliParsingCommonYaml {
-  private static final String DIR_NAME = "runjob-common-yaml";
-  private static final String TF_DIR = "runjob-pytorch-yaml";
-  private File yamlConfig;
-  private static Logger LOG = LoggerFactory.getLogger(
-      TestRunJobCliParsingCommonYaml.class);
-
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @After
-  public void after() {
-    YamlConfigTestUtils.deleteFile(yamlConfig);
-  }
-
-  @BeforeClass
-  public static void configureResourceTypes() {
-    try {
-      ResourceUtils.configureResourceType(ResourceUtils.GPU_URI);
-    } catch (SubmarineRuntimeException e) {
-      LOG.info("The hadoop dependency doesn't support gpu resource, " +
-          "so just skip this test case.");
-    }
-  }
-
-  @Rule
-  public ExpectedException exception = ExpectedException.none();
-
-  @Test
-  public void testYamlAndCliOptionIsDefinedIsInvalid() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        TF_DIR + "/valid-config.yaml");
-    String[] args = new String[] {"--name", "my-job",
-        "--docker_image", "tf-docker:1.1.0",
-        "-f", yamlConfig.getAbsolutePath() };
-
-    exception.expect(YarnException.class);
-    exception.expectMessage("defined both with YAML config and with " +
-        "CLI argument");
-
-    runJobCli.run(args);
-  }
-
-  @Test
-  public void testYamlAndCliOptionIsDefinedIsInvalidWithListOption()
-      throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        TF_DIR + "/valid-config.yaml");
-    String[] args = new String[] {"--name", "my-job",
-        "--quicklink", "AAA=http://master-0:8321",
-        "--quicklink", "BBB=http://worker-0:1234",
-        "-f", yamlConfig.getAbsolutePath()};
-
-    exception.expect(YarnException.class);
-    exception.expectMessage("defined both with YAML config and with " +
-        "CLI argument");
-
-    runJobCli.run(args);
-  }
-
-  @Test
-  public void testFalseValuesForBooleanFields() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/test-false-values.yaml");
-    runJobCli.run(
-        new String[] {"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-
-    assertTrue(RunJobParameters.class + " must be an instance of " +
-            TensorFlowRunJobParameters.class,
-        jobRunParameters instanceof TensorFlowRunJobParameters);
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) jobRunParameters;
-
-    assertFalse(jobRunParameters.isDistributeKeytab());
-    assertFalse(jobRunParameters.isWaitJobFinish());
-    assertFalse(tensorFlowParams.isTensorboardEnabled());
-  }
-
-  @Test
-  public void testWrongIndentation() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/wrong-indentation.yaml");
-
-    exception.expect(YamlParseException.class);
-    exception.expectMessage("Failed to parse YAML config, details:");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-  }
-
-  @Test
-  public void testWrongFilename() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    exception.expect(YamlParseException.class);
-    runJobCli.run(
-        new String[]{"-f", "not-existing", "--verbose"});
-  }
-
-  @Test
-  public void testEmptyFile() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createEmptyTempFile();
-
-    exception.expect(YamlParseException.class);
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-  }
-
-  @Test
-  public void testNotExistingFile() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    exception.expect(YamlParseException.class);
-    exception.expectMessage("file does not exist");
-    runJobCli.run(
-        new String[]{"-f", "blabla", "--verbose"});
-  }
-
-  @Test
-  public void testWrongPropertyName() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/wrong-property-name.yaml");
-
-    exception.expect(YamlParseException.class);
-    exception.expectMessage("Failed to parse YAML config, details:");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-  }
-
-  @Test
-  public void testMissingConfigsSection() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/missing-configs.yaml");
-
-    exception.expect(YamlParseException.class);
-    exception.expectMessage("config section should be defined, " +
-        "but it cannot be found");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-  }
-
-  @Test
-  public void testMissingSectionsShouldParsed() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/some-sections-missing.yaml");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-  }
-
-
-  @Test
-  public void testAbsentFramework() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/missing-framework.yaml");
-
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-  }
-
-  @Test
-  public void testEmptyFramework() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/empty-framework.yaml");
-
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-  }
-
-  @Test
-  public void testInvalidFramework() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/invalid-framework.yaml");
-
-    exception.expect(YamlParseException.class);
-    exception.expectMessage("framework should is defined, " +
-        "but it has an invalid value");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingParameterized.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingParameterized.java
deleted file mode 100644
index 884b1463828e..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingParameterized.java
+++ /dev/null
@@ -1,192 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob;
-import com.google.common.collect.Lists;
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.submarine.client.cli.CliConstants;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobMonitor;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-import org.junit.runner.RunWith;
-import org.junit.runners.Parameterized;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-
-import static org.apache.hadoop.yarn.submarine.client.cli.runjob.TestRunJobCliParsingCommon.getMockClientContext;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-import static org.mockito.Mockito.mock;
-
-/**
- * This class contains some test methods to test common CLI parsing
- * functionality (including TF / PyTorch) of the run job Submarine command.
- */
-@RunWith(Parameterized.class)
-public class TestRunJobCliParsingParameterized {
-
-  private final Framework framework;
-
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @Rule
-  public ExpectedException expectedException = ExpectedException.none();
-
-  @Parameterized.Parameters
-  public static Collection<Object[]> data() {
-    Collection<Object[]> params = new ArrayList<>();
-    params.add(new Object[]{Framework.TENSORFLOW });
-    params.add(new Object[]{Framework.PYTORCH });
-    return params;
-  }
-
-  public TestRunJobCliParsingParameterized(Framework framework) {
-    this.framework = framework;
-  }
-
-  private String getFrameworkName() {
-    return framework.getValue();
-  }
-
-  @Test
-  public void testPrintHelp() {
-    MockClientContext mockClientContext = new MockClientContext();
-    JobSubmitter mockJobSubmitter = mock(JobSubmitter.class);
-    JobMonitor mockJobMonitor = mock(JobMonitor.class);
-    RunJobCli runJobCli = new RunJobCli(mockClientContext, mockJobSubmitter,
-        mockJobMonitor);
-    runJobCli.printUsages();
-  }
-
-  @Test
-  public void testNoInputPathOptionSpecified() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    String expectedErrorMessage = "\"--" + CliConstants.INPUT_PATH + "\"" +
-        " is absent";
-    String actualMessage = "";
-    try {
-      runJobCli.run(
-          new String[]{"--framework", getFrameworkName(),
-              "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-              "--checkpoint_path", "hdfs://output",
-              "--num_workers", "1", "--worker_launch_cmd", "python run-job.py",
-              "--worker_resources", "memory=4g,vcores=2", "--verbose",
-              "--wait_job_finish"});
-    } catch (ParseException e) {
-      actualMessage = e.getMessage();
-      e.printStackTrace();
-    }
-    assertEquals(expectedErrorMessage, actualMessage);
-  }
-
-  @Test
-  public void testJobWithoutName() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    String expectedErrorMessage =
-        "--" + CliConstants.NAME + " is absent";
-    String actualMessage = "";
-    try {
-      runJobCli.run(
-          new String[]{"--framework", getFrameworkName(),
-              "--docker_image", "tf-docker:1.1.0",
-              "--num_workers", "0", "--verbose"});
-    } catch (ParseException e) {
-      actualMessage = e.getMessage();
-      e.printStackTrace();
-    }
-    assertEquals(expectedErrorMessage, actualMessage);
-  }
-
-  @Test
-  public void testLaunchCommandPatternReplace() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    List<String> parameters = Lists.newArrayList("--framework",
-        getFrameworkName(),
-        "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-        "--input_path", "hdfs://input", "--checkpoint_path",
-        "hdfs://output",
-        "--num_workers", "3",
-        "--worker_launch_cmd", "python run-job.py --input=%input_path% " +
-            "--model_dir=%checkpoint_path% " +
-            "--export_dir=%saved_model_path%/savedmodel",
-        "--worker_resources", "memory=2048,vcores=2");
-
-    if (framework == Framework.TENSORFLOW) {
-      parameters.addAll(Lists.newArrayList(
-          "--ps_resources", "memory=4096,vcores=4",
-          "--ps_launch_cmd", "python run-ps.py --input=%input_path% " +
-              "--model_dir=%checkpoint_path%/model",
-          "--verbose"));
-    }
-
-    runJobCli.run(parameters.toArray(new String[0]));
-
-    RunJobParameters runJobParameters = checkExpectedFrameworkParams(runJobCli);
-
-    if (framework == Framework.TENSORFLOW) {
-      TensorFlowRunJobParameters tensorFlowParams =
-          (TensorFlowRunJobParameters) runJobParameters;
-      assertEquals(
-          "python run-job.py --input=hdfs://input --model_dir=hdfs://output "
-              + "--export_dir=hdfs://output/savedmodel",
-          tensorFlowParams.getWorkerLaunchCmd());
-      assertEquals(
-          "python run-ps.py --input=hdfs://input " +
-              "--model_dir=hdfs://output/model",
-          tensorFlowParams.getPSLaunchCmd());
-    } else if (framework == Framework.PYTORCH) {
-      PyTorchRunJobParameters pyTorchParameters =
-          (PyTorchRunJobParameters) runJobParameters;
-      assertEquals(
-          "python run-job.py --input=hdfs://input --model_dir=hdfs://output "
-              + "--export_dir=hdfs://output/savedmodel",
-          pyTorchParameters.getWorkerLaunchCmd());
-    }
-  }
-
-  private RunJobParameters checkExpectedFrameworkParams(RunJobCli runJobCli) {
-    RunJobParameters runJobParameters = runJobCli.getRunJobParameters();
-
-    if (framework == Framework.TENSORFLOW) {
-      assertTrue(RunJobParameters.class + " must be an instance of " +
-              TensorFlowRunJobParameters.class,
-          runJobParameters instanceof TensorFlowRunJobParameters);
-    } else if (framework == Framework.PYTORCH) {
-      assertTrue(RunJobParameters.class + " must be an instance of " +
-              PyTorchRunJobParameters.class,
-          runJobParameters instanceof PyTorchRunJobParameters);
-    }
-    return runJobParameters;
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/pytorch/TestRunJobCliParsingPyTorch.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/pytorch/TestRunJobCliParsingPyTorch.java
deleted file mode 100644
index edb194356a71..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/pytorch/TestRunJobCliParsingPyTorch.java
+++ /dev/null
@@ -1,209 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob.pytorch;
-
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.RunJobCli;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.util.resource.Resources;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-
-import static org.apache.hadoop.yarn.submarine.client.cli.runjob.TestRunJobCliParsingCommon.getMockClientContext;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-/**
- * Test class that verifies the correctness of PyTorch
- * CLI configuration parsing.
- */
-public class TestRunJobCliParsingPyTorch {
-
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @Rule
-  public ExpectedException expectedException = ExpectedException.none();
-
-  @Test
-  public void testBasicRunJobForSingleNodeTraining() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    runJobCli.run(
-        new String[] {"--framework", "pytorch",
-            "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input", "--checkpoint_path",
-            "hdfs://output",
-            "--num_workers", "1", "--worker_launch_cmd", "python run-job.py",
-            "--worker_resources", "memory=4g,vcores=2", "--verbose",
-            "--wait_job_finish" });
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    assertTrue(RunJobParameters.class +
-            " must be an instance of " +
-            PyTorchRunJobParameters.class,
-        jobRunParameters instanceof PyTorchRunJobParameters);
-    PyTorchRunJobParameters pyTorchParams =
-        (PyTorchRunJobParameters) jobRunParameters;
-
-    assertEquals(jobRunParameters.getInputPath(), "hdfs://input");
-    assertEquals(jobRunParameters.getCheckpointPath(), "hdfs://output");
-    assertEquals(pyTorchParams.getNumWorkers(), 1);
-    assertEquals(pyTorchParams.getWorkerLaunchCmd(),
-        "python run-job.py");
-    assertEquals(Resources.createResource(4096, 2),
-        pyTorchParams.getWorkerResource());
-    assertTrue(SubmarineLogs.isVerbose());
-    assertTrue(jobRunParameters.isWaitJobFinish());
-  }
-
-  @Test
-  public void testNumPSCannotBeDefined() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    expectedException.expect(ParseException.class);
-    expectedException.expectMessage("cannot be defined for PyTorch jobs");
-    runJobCli.run(
-        new String[] {"--framework", "pytorch",
-            "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input",
-            "--checkpoint_path","hdfs://output",
-            "--num_workers", "3",
-            "--worker_launch_cmd",
-            "python run-job.py", "--worker_resources", "memory=2048M,vcores=2",
-            "--num_ps", "2" });
-  }
-
-  @Test
-  public void testPSResourcesCannotBeDefined() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    expectedException.expect(ParseException.class);
-    expectedException.expectMessage("cannot be defined for PyTorch jobs");
-    runJobCli.run(
-        new String[] {"--framework", "pytorch",
-            "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input",
-            "--checkpoint_path", "hdfs://output",
-            "--num_workers", "3",
-            "--worker_launch_cmd",
-            "python run-job.py", "--worker_resources", "memory=2048M,vcores=2",
-            "--ps_resources", "memory=2048M,vcores=2" });
-  }
-
-  @Test
-  public void testPSDockerImageCannotBeDefined() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    expectedException.expect(ParseException.class);
-    expectedException.expectMessage("cannot be defined for PyTorch jobs");
-    runJobCli.run(
-        new String[] {"--framework", "pytorch",
-            "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input",
-            "--checkpoint_path", "hdfs://output",
-            "--num_workers", "3",
-            "--worker_launch_cmd",
-            "python run-job.py", "--worker_resources", "memory=2048M,vcores=2",
-            "--ps_docker_image", "psDockerImage" });
-  }
-
-  @Test
-  public void testPSLaunchCommandCannotBeDefined() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    expectedException.expect(ParseException.class);
-    expectedException.expectMessage("cannot be defined for PyTorch jobs");
-    runJobCli.run(
-        new String[] {"--framework", "pytorch",
-            "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input",
-            "--checkpoint_path", "hdfs://output",
-            "--num_workers", "3",
-            "--worker_launch_cmd",
-            "python run-job.py", "--worker_resources", "memory=2048M,vcores=2",
-            "--ps_launch_cmd", "psLaunchCommand" });
-  }
-
-  @Test
-  public void testTensorboardCannotBeDefined() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    expectedException.expect(ParseException.class);
-    expectedException.expectMessage("cannot be defined for PyTorch jobs");
-    runJobCli.run(
-        new String[] {"--framework", "pytorch",
-            "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input",
-            "--checkpoint_path", "hdfs://output",
-            "--num_workers", "3",
-            "--worker_launch_cmd",
-            "python run-job.py", "--worker_resources", "memory=2048M,vcores=2",
-            "--tensorboard" });
-  }
-
-  @Test
-  public void testTensorboardResourcesCannotBeDefined() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    expectedException.expect(ParseException.class);
-    expectedException.expectMessage("cannot be defined for PyTorch jobs");
-    runJobCli.run(
-        new String[] {"--framework", "pytorch",
-            "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input",
-            "--checkpoint_path", "hdfs://output",
-            "--num_workers", "3",
-            "--worker_launch_cmd",
-            "python run-job.py", "--worker_resources", "memory=2048M,vcores=2",
-            "--tensorboard_resources", "memory=2048M,vcores=2" });
-  }
-
-  @Test
-  public void testTensorboardDockerImageCannotBeDefined() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    expectedException.expect(ParseException.class);
-    expectedException.expectMessage("cannot be defined for PyTorch jobs");
-    runJobCli.run(
-        new String[] {"--framework", "pytorch",
-            "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input",
-            "--checkpoint_path", "hdfs://output",
-            "--num_workers", "3",
-            "--worker_launch_cmd",
-            "python run-job.py", "--worker_resources", "memory=2048M,vcores=2",
-            "--tensorboard_docker_image", "TBDockerImage" });
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/pytorch/TestRunJobCliParsingPyTorchYaml.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/pytorch/TestRunJobCliParsingPyTorchYaml.java
deleted file mode 100644
index bf94edcf0914..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/pytorch/TestRunJobCliParsingPyTorchYaml.java
+++ /dev/null
@@ -1,257 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob.pytorch;
-
-import static org.apache.hadoop.yarn.submarine.client.cli.runjob.TestRunJobCliParsingCommon.getMockClientContext;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
-import static org.junit.Assert.assertTrue;
-
-import java.io.File;
-import java.util.List;
-
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.submarine.client.cli.YamlConfigTestUtils;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.YamlParseException;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.RunJobCli;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineRuntimeException;
-import org.apache.hadoop.yarn.submarine.common.resource.ResourceUtils;
-import org.apache.hadoop.yarn.util.resource.Resources;
-import org.junit.After;
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-
-import com.google.common.collect.ImmutableList;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Test class that verifies the correctness of PyTorch
- * YAML configuration parsing.
- */
-public class TestRunJobCliParsingPyTorchYaml {
-  private static final String OVERRIDDEN_PREFIX = "overridden_";
-  private static final String DIR_NAME = "runjob-pytorch-yaml";
-  private File yamlConfig;
-  private static Logger LOG = LoggerFactory.getLogger(
-      TestRunJobCliParsingPyTorchYaml.class);
-
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @After
-  public void after() {
-    YamlConfigTestUtils.deleteFile(yamlConfig);
-  }
-
-  @Rule
-  public ExpectedException exception = ExpectedException.none();
-
-  private void verifyBasicConfigValues(RunJobParameters jobRunParameters) {
-    verifyBasicConfigValues(jobRunParameters,
-        ImmutableList.of("env1=env1Value", "env2=env2Value"));
-  }
-
-  private void verifyBasicConfigValues(RunJobParameters jobRunParameters,
-      List<String> expectedEnvs) {
-    assertEquals("testInputPath", jobRunParameters.getInputPath());
-    assertEquals("testCheckpointPath", jobRunParameters.getCheckpointPath());
-    assertEquals("testDockerImage", jobRunParameters.getDockerImageName());
-
-    assertNotNull(jobRunParameters.getLocalizations());
-    assertEquals(2, jobRunParameters.getLocalizations().size());
-
-    assertNotNull(jobRunParameters.getQuicklinks());
-    assertEquals(2, jobRunParameters.getQuicklinks().size());
-
-    assertTrue(SubmarineLogs.isVerbose());
-    assertTrue(jobRunParameters.isWaitJobFinish());
-
-    for (String env : expectedEnvs) {
-      assertTrue(String.format(
-          "%s should be in env list of jobRunParameters!", env),
-          jobRunParameters.getEnvars().contains(env));
-    }
-  }
-
-  private PyTorchRunJobParameters verifyWorkerCommonValues(RunJobParameters
-      jobRunParameters, String prefix) {
-    assertTrue(RunJobParameters.class + " must be an instance of " +
-            PyTorchRunJobParameters.class,
-        jobRunParameters instanceof PyTorchRunJobParameters);
-    PyTorchRunJobParameters pyTorchParams =
-        (PyTorchRunJobParameters) jobRunParameters;
-
-    assertEquals(3, pyTorchParams.getNumWorkers());
-    assertEquals(prefix + "testLaunchCmdWorker",
-        pyTorchParams.getWorkerLaunchCmd());
-    assertEquals(prefix + "testDockerImageWorker",
-        pyTorchParams.getWorkerDockerImage());
-    return pyTorchParams;
-  }
-
-  private void verifyWorkerValues(RunJobParameters jobRunParameters,
-      String prefix) {
-    PyTorchRunJobParameters pyTorchParams = verifyWorkerCommonValues
-        (jobRunParameters, prefix);
-    assertEquals(Resources.createResource(20480, 32),
-        pyTorchParams.getWorkerResource());
-  }
-
-  private void verifyWorkerValuesWithGpu(RunJobParameters jobRunParameters,
-      String prefix) {
-
-    PyTorchRunJobParameters pyTorchParams = verifyWorkerCommonValues
-        (jobRunParameters, prefix);
-    Resource workResource = Resources.createResource(20480, 32);
-    ResourceUtils.setResource(workResource, ResourceUtils.GPU_URI, 2);
-    assertEquals(workResource, pyTorchParams.getWorkerResource());
-  }
-
-  private void verifySecurityValues(RunJobParameters jobRunParameters) {
-    assertEquals("keytabPath", jobRunParameters.getKeytab());
-    assertEquals("testPrincipal", jobRunParameters.getPrincipal());
-    assertTrue(jobRunParameters.isDistributeKeytab());
-  }
-
-  @Test
-  public void testValidYamlParsing() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/valid-config.yaml");
-    runJobCli.run(
-        new String[] {"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    verifyBasicConfigValues(jobRunParameters);
-    verifyWorkerValues(jobRunParameters, "");
-    verifySecurityValues(jobRunParameters);
-  }
-
-  @Test
-  public void testValidGpuYamlParsing() throws Exception {
-    try {
-      ResourceUtils.configureResourceType(ResourceUtils.GPU_URI);
-    } catch (SubmarineRuntimeException e) {
-      LOG.info("The hadoop dependency doesn't support gpu resource, " +
-          "so just skip this test case.");
-      return;
-    }
-
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/valid-gpu-config.yaml");
-    runJobCli.run(
-        new String[] {"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    verifyBasicConfigValues(jobRunParameters);
-    verifyWorkerValuesWithGpu(jobRunParameters, "");
-    verifySecurityValues(jobRunParameters);
-  }
-
-  @Test
-  public void testRoleOverrides() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/valid-config-with-overrides.yaml");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    verifyBasicConfigValues(jobRunParameters);
-    verifyWorkerValues(jobRunParameters, OVERRIDDEN_PREFIX);
-    verifySecurityValues(jobRunParameters);
-  }
-
-  @Test
-  public void testMissingPrincipalUnderSecuritySection() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/security-principal-is-missing.yaml");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    verifyBasicConfigValues(jobRunParameters);
-    verifyWorkerValues(jobRunParameters, "");
-
-    //Verify security values
-    assertEquals("keytabPath", jobRunParameters.getKeytab());
-    assertNull("Principal should be null!", jobRunParameters.getPrincipal());
-    assertTrue(jobRunParameters.isDistributeKeytab());
-  }
-
-  @Test
-  public void testMissingEnvs() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/envs-are-missing.yaml");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    verifyBasicConfigValues(jobRunParameters, ImmutableList.of());
-    verifyWorkerValues(jobRunParameters, "");
-    verifySecurityValues(jobRunParameters);
-  }
-
-  @Test
-  public void testInvalidConfigPsSectionIsDefined() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    exception.expect(YamlParseException.class);
-    exception.expectMessage("PS section should not be defined " +
-        "when PyTorch is the selected framework");
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/invalid-config-ps-section.yaml");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-  }
-
-  @Test
-  public void testInvalidConfigTensorboardSectionIsDefined() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    exception.expect(YamlParseException.class);
-    exception.expectMessage("TensorBoard section should not be defined " +
-        "when PyTorch is the selected framework");
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/invalid-config-tensorboard-section.yaml");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlow.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlow.java
deleted file mode 100644
index d0812244fdfe..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlow.java
+++ /dev/null
@@ -1,170 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob.tensorflow;
-
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.yarn.submarine.client.cli.CliConstants;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.RunJobCli;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.util.resource.Resources;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-
-import static org.apache.hadoop.yarn.submarine.client.cli.runjob.TestRunJobCliParsingCommon.getMockClientContext;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-/**
- * Test class that verifies the correctness of TensorFlow
- * CLI configuration parsing.
- */
-public class TestRunJobCliParsingTensorFlow {
-
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @Rule
-  public ExpectedException expectedException = ExpectedException.none();
-
-  @Test
-  public void testNoInputPathOptionSpecified() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    String expectedErrorMessage = "\"--" + CliConstants.INPUT_PATH +
-        "\" is absent";
-    String actualMessage = "";
-    try {
-      runJobCli.run(
-          new String[]{"--framework", "tensorflow",
-              "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-              "--checkpoint_path", "hdfs://output",
-              "--num_workers", "1", "--worker_launch_cmd", "python run-job.py",
-              "--worker_resources", "memory=4g,vcores=2", "--tensorboard",
-              "true", "--verbose", "--wait_job_finish"});
-    } catch (ParseException e) {
-      actualMessage = e.getMessage();
-      e.printStackTrace();
-    }
-    assertEquals(expectedErrorMessage, actualMessage);
-  }
-
-  @Test
-  public void testBasicRunJobForDistributedTraining() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    assertFalse(SubmarineLogs.isVerbose());
-
-    runJobCli.run(
-        new String[] { "--framework", "tensorflow",
-            "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input",
-            "--checkpoint_path", "hdfs://output",
-            "--num_workers", "3", "--num_ps", "2", "--worker_launch_cmd",
-            "python run-job.py", "--worker_resources", "memory=2048M,vcores=2",
-            "--ps_resources", "memory=4G,vcores=4", "--tensorboard", "true",
-            "--ps_launch_cmd", "python run-ps.py", "--keytab", "/keytab/path",
-            "--principal", "user/_HOST@domain.com", "--distribute_keytab",
-            "--verbose" });
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    assertTrue(RunJobParameters.class +
-        " must be an instance of " +
-        TensorFlowRunJobParameters.class,
-        jobRunParameters instanceof TensorFlowRunJobParameters);
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) jobRunParameters;
-
-    assertEquals(jobRunParameters.getInputPath(), "hdfs://input");
-    assertEquals(jobRunParameters.getCheckpointPath(), "hdfs://output");
-    assertEquals(tensorFlowParams.getNumPS(), 2);
-    assertEquals(tensorFlowParams.getPSLaunchCmd(), "python run-ps.py");
-    assertEquals(Resources.createResource(4096, 4),
-        tensorFlowParams.getPsResource());
-    assertEquals(tensorFlowParams.getWorkerLaunchCmd(),
-        "python run-job.py");
-    assertEquals(Resources.createResource(2048, 2),
-        tensorFlowParams.getWorkerResource());
-    assertEquals(jobRunParameters.getDockerImageName(),
-        "tf-docker:1.1.0");
-    assertEquals(jobRunParameters.getKeytab(),
-        "/keytab/path");
-    assertEquals(jobRunParameters.getPrincipal(),
-        "user/_HOST@domain.com");
-    assertTrue(jobRunParameters.isDistributeKeytab());
-    assertTrue(SubmarineLogs.isVerbose());
-  }
-
-  @Test
-  public void testBasicRunJobForSingleNodeTraining() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    assertFalse(SubmarineLogs.isVerbose());
-
-    runJobCli.run(
-        new String[] { "--framework", "tensorflow",
-            "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input", "--checkpoint_path",
-            "hdfs://output",
-            "--num_workers", "1", "--worker_launch_cmd", "python run-job.py",
-            "--worker_resources", "memory=4g,vcores=2", "--tensorboard",
-            "true", "--verbose", "--wait_job_finish" });
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    assertTrue(RunJobParameters.class +
-            " must be an instance of " +
-            TensorFlowRunJobParameters.class,
-        jobRunParameters instanceof TensorFlowRunJobParameters);
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) jobRunParameters;
-
-    assertEquals(jobRunParameters.getInputPath(), "hdfs://input");
-    assertEquals(jobRunParameters.getCheckpointPath(), "hdfs://output");
-    assertEquals(tensorFlowParams.getNumWorkers(), 1);
-    assertEquals(tensorFlowParams.getWorkerLaunchCmd(),
-        "python run-job.py");
-    assertEquals(Resources.createResource(4096, 2),
-        tensorFlowParams.getWorkerResource());
-    assertTrue(SubmarineLogs.isVerbose());
-    assertTrue(jobRunParameters.isWaitJobFinish());
-  }
-
-  /**
-   * when only run tensorboard, input_path is not needed
-   * */
-  @Test
-  public void testNoInputPathOptionButOnlyRunTensorboard() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    boolean success = true;
-    try {
-      runJobCli.run(
-          new String[]{"--framework", "tensorflow",
-              "--name", "my-job", "--docker_image", "tf-docker:1.1.0",
-              "--num_workers", "0", "--tensorboard", "--verbose",
-              "--tensorboard_resources", "memory=2G,vcores=2",
-              "--tensorboard_docker_image", "tb_docker_image:001"});
-    } catch (ParseException e) {
-      success = false;
-    }
-    assertTrue(success);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlowYaml.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlowYaml.java
deleted file mode 100644
index 9c69720e5b7e..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlowYaml.java
+++ /dev/null
@@ -1,293 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob.tensorflow;
-
-import com.google.common.collect.ImmutableList;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.submarine.client.cli.YamlConfigTestUtils;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.RunJobCli;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.common.exception.SubmarineRuntimeException;
-import org.apache.hadoop.yarn.submarine.common.resource.ResourceUtils;
-import org.apache.hadoop.yarn.util.resource.Resources;
-import org.junit.After;
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.util.List;
-
-import static org.apache.hadoop.yarn.submarine.client.cli.runjob.TestRunJobCliParsingCommon.getMockClientContext;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
-import static org.junit.Assert.assertTrue;
-
-/**
- * Test class that verifies the correctness of TF YAML configuration parsing.
- */
-public class TestRunJobCliParsingTensorFlowYaml {
-  private static final String OVERRIDDEN_PREFIX = "overridden_";
-  private static final String DIR_NAME = "runjob-tensorflow-yaml";
-  private File yamlConfig;
-  private static Logger LOG = LoggerFactory.getLogger(
-      TestRunJobCliParsingTensorFlowYaml.class);
-
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @After
-  public void after() {
-    YamlConfigTestUtils.deleteFile(yamlConfig);
-  }
-
-  @Rule
-  public ExpectedException exception = ExpectedException.none();
-
-  private void verifyBasicConfigValues(RunJobParameters jobRunParameters) {
-    verifyBasicConfigValues(jobRunParameters,
-        ImmutableList.of("env1=env1Value", "env2=env2Value"));
-  }
-
-  private void verifyBasicConfigValues(RunJobParameters jobRunParameters,
-      List<String> expectedEnvs) {
-    assertEquals("testInputPath", jobRunParameters.getInputPath());
-    assertEquals("testCheckpointPath", jobRunParameters.getCheckpointPath());
-    assertEquals("testDockerImage", jobRunParameters.getDockerImageName());
-
-    assertNotNull(jobRunParameters.getLocalizations());
-    assertEquals(2, jobRunParameters.getLocalizations().size());
-
-    assertNotNull(jobRunParameters.getQuicklinks());
-    assertEquals(2, jobRunParameters.getQuicklinks().size());
-
-    assertTrue(SubmarineLogs.isVerbose());
-    assertTrue(jobRunParameters.isWaitJobFinish());
-
-    for (String env : expectedEnvs) {
-      assertTrue(String.format(
-          "%s should be in env list of jobRunParameters!", env),
-          jobRunParameters.getEnvars().contains(env));
-    }
-  }
-
-  private void verifyPsValues(RunJobParameters jobRunParameters,
-      String prefix) {
-    assertTrue(RunJobParameters.class + " must be an instance of " +
-            TensorFlowRunJobParameters.class,
-        jobRunParameters instanceof TensorFlowRunJobParameters);
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) jobRunParameters;
-
-    assertEquals(4, tensorFlowParams.getNumPS());
-    assertEquals(prefix + "testLaunchCmdPs", tensorFlowParams.getPSLaunchCmd());
-    assertEquals(prefix + "testDockerImagePs",
-        tensorFlowParams.getPsDockerImage());
-    assertEquals(Resources.createResource(20500, 34),
-        tensorFlowParams.getPsResource());
-  }
-
-  private TensorFlowRunJobParameters verifyWorkerCommonValues(
-      RunJobParameters jobRunParameters, String prefix) {
-    assertTrue(RunJobParameters.class + " must be an instance of " +
-            TensorFlowRunJobParameters.class,
-        jobRunParameters instanceof TensorFlowRunJobParameters);
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) jobRunParameters;
-
-    assertEquals(3, tensorFlowParams.getNumWorkers());
-    assertEquals(prefix + "testLaunchCmdWorker",
-        tensorFlowParams.getWorkerLaunchCmd());
-    assertEquals(prefix + "testDockerImageWorker",
-        tensorFlowParams.getWorkerDockerImage());
-    return tensorFlowParams;
-  }
-
-  private void verifyWorkerValues(RunJobParameters jobRunParameters,
-      String prefix) {
-    TensorFlowRunJobParameters tensorFlowParams = verifyWorkerCommonValues
-        (jobRunParameters, prefix);
-    assertEquals(Resources.createResource(20480, 32),
-        tensorFlowParams.getWorkerResource());
-  }
-
-  private void verifyWorkerValuesWithGpu(RunJobParameters jobRunParameters,
-                                  String prefix) {
-    TensorFlowRunJobParameters tensorFlowParams = verifyWorkerCommonValues
-        (jobRunParameters, prefix);
-    Resource workResource = Resources.createResource(20480, 32);
-    ResourceUtils.setResource(workResource, ResourceUtils.GPU_URI, 2);
-    assertEquals(workResource, tensorFlowParams.getWorkerResource());
-  }
-
-  private void verifySecurityValues(RunJobParameters jobRunParameters) {
-    assertEquals("keytabPath", jobRunParameters.getKeytab());
-    assertEquals("testPrincipal", jobRunParameters.getPrincipal());
-    assertTrue(jobRunParameters.isDistributeKeytab());
-  }
-
-  private void verifyTensorboardValues(RunJobParameters jobRunParameters) {
-    assertTrue(RunJobParameters.class + " must be an instance of " +
-            TensorFlowRunJobParameters.class,
-        jobRunParameters instanceof TensorFlowRunJobParameters);
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) jobRunParameters;
-
-    assertTrue(tensorFlowParams.isTensorboardEnabled());
-    assertEquals("tensorboardDockerImage",
-        tensorFlowParams.getTensorboardDockerImage());
-    assertEquals(Resources.createResource(21000, 37),
-        tensorFlowParams.getTensorboardResource());
-  }
-
-  @Test
-  public void testValidYamlParsing() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/valid-config.yaml");
-    runJobCli.run(
-        new String[] {"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    verifyBasicConfigValues(jobRunParameters);
-    verifyPsValues(jobRunParameters, "");
-    verifyWorkerValues(jobRunParameters, "");
-    verifySecurityValues(jobRunParameters);
-    verifyTensorboardValues(jobRunParameters);
-  }
-
-  @Test
-  public void testValidGpuYamlParsing() throws Exception {
-    try {
-      ResourceUtils.configureResourceType(ResourceUtils.GPU_URI);
-    } catch (SubmarineRuntimeException e) {
-      LOG.info("The hadoop dependency doesn't support gpu resource, " +
-          "so just skip this test case.");
-      return;
-    }
-
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/valid-gpu-config.yaml");
-    runJobCli.run(
-        new String[] {"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    verifyBasicConfigValues(jobRunParameters);
-    verifyPsValues(jobRunParameters, "");
-    verifyWorkerValuesWithGpu(jobRunParameters, "");
-    verifySecurityValues(jobRunParameters);
-    verifyTensorboardValues(jobRunParameters);
-  }
-
-  @Test
-  public void testRoleOverrides() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    Assert.assertFalse(SubmarineLogs.isVerbose());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/valid-config-with-overrides.yaml");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    verifyBasicConfigValues(jobRunParameters);
-    verifyPsValues(jobRunParameters, OVERRIDDEN_PREFIX);
-    verifyWorkerValues(jobRunParameters, OVERRIDDEN_PREFIX);
-    verifySecurityValues(jobRunParameters);
-    verifyTensorboardValues(jobRunParameters);
-  }
-
-  @Test
-  public void testMissingPrincipalUnderSecuritySection() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/security-principal-is-missing.yaml");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    verifyBasicConfigValues(jobRunParameters);
-    verifyPsValues(jobRunParameters, "");
-    verifyWorkerValues(jobRunParameters, "");
-    verifyTensorboardValues(jobRunParameters);
-
-    //Verify security values
-    assertEquals("keytabPath", jobRunParameters.getKeytab());
-    assertNull("Principal should be null!", jobRunParameters.getPrincipal());
-    assertTrue(jobRunParameters.isDistributeKeytab());
-  }
-
-  @Test
-  public void testMissingTensorBoardDockerImage() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/tensorboard-dockerimage-is-missing.yaml");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-
-    verifyBasicConfigValues(jobRunParameters);
-    verifyPsValues(jobRunParameters, "");
-    verifyWorkerValues(jobRunParameters, "");
-    verifySecurityValues(jobRunParameters);
-
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) jobRunParameters;
-
-    assertTrue(tensorFlowParams.isTensorboardEnabled());
-    assertNull("tensorboardDockerImage should be null!",
-        tensorFlowParams.getTensorboardDockerImage());
-    assertEquals(Resources.createResource(21000, 37),
-        tensorFlowParams.getTensorboardResource());
-  }
-
-  @Test
-  public void testMissingEnvs() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-
-    yamlConfig = YamlConfigTestUtils.createTempFileWithContents(
-        DIR_NAME + "/envs-are-missing.yaml");
-    runJobCli.run(
-        new String[]{"-f", yamlConfig.getAbsolutePath(), "--verbose"});
-
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-    verifyBasicConfigValues(jobRunParameters, ImmutableList.of());
-    verifyPsValues(jobRunParameters, "");
-    verifyWorkerValues(jobRunParameters, "");
-    verifySecurityValues(jobRunParameters);
-    verifyTensorboardValues(jobRunParameters);
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlowYamlStandalone.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlowYamlStandalone.java
deleted file mode 100644
index 2088d1df838d..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlowYamlStandalone.java
+++ /dev/null
@@ -1,204 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.hadoop.yarn.submarine.client.cli.runjob.tensorflow;
-
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Configs;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Role;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Roles;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Scheduling;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Security;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.Spec;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.TensorBoard;
-import org.apache.hadoop.yarn.submarine.client.cli.param.yaml.YamlConfigFile;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.util.List;
-import java.util.Map;
-
-import static org.apache.hadoop.yarn.submarine.client.cli.YamlConfigTestUtils.readYamlConfigFile;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertTrue;
-
-/**
- * Test class that verifies the correctness of YAML configuration parsing.
- * Please note that this class just tests YAML parsing,
- * but only in an isolated fashion.
- */
-public class TestRunJobCliParsingTensorFlowYamlStandalone {
-  private static final String OVERRIDDEN_PREFIX = "overridden_";
-  private static final String DIR_NAME = "runjob-tensorflow-yaml";
-
-  private void verifyBasicConfigValues(YamlConfigFile yamlConfigFile) {
-    assertNotNull("Spec file should not be null!", yamlConfigFile);
-    Spec spec = yamlConfigFile.getSpec();
-    assertNotNull("Spec should not be null!", spec);
-
-    assertEquals("testJobName", spec.getName());
-    assertEquals("testJobType", spec.getJobType());
-
-    Configs configs = yamlConfigFile.getConfigs();
-    assertNotNull("Configs should not be null!", configs);
-
-    assertEquals("testInputPath", configs.getInputPath());
-    assertEquals("testCheckpointPath", configs.getCheckpointPath());
-    assertEquals("testSavedModelPath", configs.getSavedModelPath());
-    assertEquals("testDockerImage", configs.getDockerImage());
-
-    Map<String, String> envs = configs.getEnvs();
-    assertNotNull("Envs should not be null!", envs);
-    assertEquals(2, envs.size());
-    assertEquals("env1Value", envs.get("env1"));
-    assertEquals("env2Value", envs.get("env2"));
-
-    List<String> localizations = configs.getLocalizations();
-    assertNotNull("Localizations should not be null!", localizations);
-    assertEquals("Size of localizations must be 2!", 2, localizations.size());
-    assertEquals("hdfs://remote-file1:/local-filename1:rw",
-        localizations.get(0));
-    assertEquals("nfs://remote-file2:/local-filename2:rw",
-        localizations.get(1));
-
-    List<String> mounts = configs.getMounts();
-    assertNotNull("Mounts should not be null!", mounts);
-    assertEquals("Size of mounts must be 2!", 2, mounts.size());
-    assertEquals("/etc/passwd:/etc/passwd:rw", mounts.get(0));
-    assertEquals("/etc/hosts:/etc/hosts:rw", mounts.get(1));
-
-    assertTrue(
-        configs.getQuicklinks().contains("Notebook_UI=https://master-0:7070"));
-    assertTrue(
-        configs.getQuicklinks().contains("Notebook_UI2=https://master-0:7071"));
-    assertEquals("true", configs.getWaitJobFinish());
-  }
-
-  private void assertRoleConfigOverrides(Role role, String prefix,
-      String roleType) {
-    assertNotNull(roleType + " role should not be null!", role);
-
-    assertEquals(String.format("%stestDockerImage%s", prefix, roleType),
-        role.getDockerImage());
-
-    //envs, localizations and mounts for Roles
-    // are only present in valid-config-with-overrides.yaml
-    boolean validateAll = !prefix.equals("");
-    if (validateAll) {
-      Map<String, String> envs = role.getEnvs();
-      assertNotNull("Envs should not be null!", envs);
-      assertEquals(String.format("%senv1%s", prefix, roleType),
-          envs.get("env1"));
-      assertEquals(String.format("%senv2%s", prefix, roleType),
-          envs.get("env2"));
-    }
-
-    if (validateAll) {
-      List<String> localizations = role.getLocalizations();
-      assertNotNull("Localizations should not be null!", localizations);
-      assertEquals("Size of localizations must be 2!", 2, localizations.size());
-      assertEquals(String.format("hdfs://remote-file1:/%slocal" +
-          "-filename1%s:rw", prefix, roleType), localizations.get(0));
-      assertEquals(String.format("nfs://remote-file2:/%slocal" +
-          "-filename2%s:rw", prefix, roleType), localizations.get(1));
-    }
-
-    if (validateAll) {
-      List<String> mounts = role.getMounts();
-      assertNotNull("Mounts should not be null!", mounts);
-      assertEquals("Size of mounts must be 2!", 2, mounts.size());
-      assertEquals(String.format("/etc/passwd:/%s%s", prefix, roleType),
-          mounts.get(0));
-      assertEquals(String.format("/etc/hosts:/%s%s", prefix, roleType),
-          mounts.get(1));
-    }
-  }
-
-  private void assertWorkerValues(Role worker) {
-    assertEquals("testLaunchCmdWorker", worker.getLaunchCmd());
-    assertEquals("testDockerImageWorker", worker.getDockerImage());
-    assertEquals("memory=20480M,vcores=32", worker.getResources());
-    assertEquals(3, worker.getReplicas());
-  }
-
-  private void assertPsValues(Role ps) {
-    assertEquals("testLaunchCmdPs", ps.getLaunchCmd());
-    assertEquals("testDockerImagePs", ps.getDockerImage());
-    assertEquals("memory=20500M,vcores=34", ps.getResources());
-    assertEquals(4, ps.getReplicas());
-  }
-
-  private void verifySchedulingValues(YamlConfigFile yamlConfigFile) {
-    Scheduling scheduling = yamlConfigFile.getScheduling();
-    assertNotNull("Scheduling should not be null!", scheduling);
-    assertEquals("queue1", scheduling.getQueue());
-  }
-
-  private void verifySecurityValues(YamlConfigFile yamlConfigFile) {
-    Security security = yamlConfigFile.getSecurity();
-    assertNotNull("Security should not be null!", security);
-    assertEquals("keytabPath", security.getKeytab());
-    assertEquals("testPrincipal", security.getPrincipal());
-    assertTrue(security.isDistributeKeytab());
-  }
-
-  private void verifyTensorboardValues(YamlConfigFile yamlConfigFile) {
-    TensorBoard tensorBoard = yamlConfigFile.getTensorBoard();
-    assertNotNull("Tensorboard should not be null!", tensorBoard);
-    assertEquals("tensorboardDockerImage", tensorBoard.getDockerImage());
-    assertEquals("memory=21000M,vcores=37", tensorBoard.getResources());
-  }
-
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @Test
-  public void testLaunchCommandYaml() {
-    YamlConfigFile yamlConfigFile = readYamlConfigFile(DIR_NAME +
-        "/valid-config.yaml");
-
-    verifyBasicConfigValues(yamlConfigFile);
-
-    Roles roles = yamlConfigFile.getRoles();
-    assertNotNull("Roles should not be null!", roles);
-    assertRoleConfigOverrides(roles.getWorker(), "", "Worker");
-    assertRoleConfigOverrides(roles.getPs(), "", "Ps");
-
-    assertWorkerValues(roles.getWorker());
-    assertPsValues(roles.getPs());
-
-    verifySchedulingValues(yamlConfigFile);
-    verifySecurityValues(yamlConfigFile);
-    verifyTensorboardValues(yamlConfigFile);
-  }
-
-  @Test
-  public void testOverrides() {
-    YamlConfigFile yamlConfigFile = readYamlConfigFile(DIR_NAME +
-        "/valid-config-with-overrides.yaml");
-
-    verifyBasicConfigValues(yamlConfigFile);
-
-    Roles roles = yamlConfigFile.getRoles();
-    assertNotNull("Roles should not be null!", roles);
-    assertRoleConfigOverrides(roles.getWorker(), OVERRIDDEN_PREFIX, "Worker");
-    assertRoleConfigOverrides(roles.getPs(), OVERRIDDEN_PREFIX, "Ps");
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/common/MockClientContext.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/common/MockClientContext.java
deleted file mode 100644
index 3740f8677d8d..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/common/MockClientContext.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.common;
-
-import org.apache.hadoop.yarn.submarine.common.fs.MockRemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-
-public class MockClientContext extends ClientContext {
-
-  private RemoteDirectoryManager remoteDirectoryMgr =
-      new MockRemoteDirectoryManager();
-
-  @Override
-  public RemoteDirectoryManager getRemoteDirectoryManager() {
-    return remoteDirectoryMgr;
-  }
-
-  public void setRemoteDirectoryMgr(
-      RemoteDirectoryManager remoteDirectoryMgr) {
-    this.remoteDirectoryMgr = remoteDirectoryMgr;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/common/fs/MockRemoteDirectoryManager.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/common/fs/MockRemoteDirectoryManager.java
deleted file mode 100644
index 7ef03f535711..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/common/fs/MockRemoteDirectoryManager.java
+++ /dev/null
@@ -1,170 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.common.fs;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Objects;
-
-public class MockRemoteDirectoryManager implements RemoteDirectoryManager {
-  private File jobsParentDir = null;
-  private File modelParentDir = null;
-
-  private File jobDir = null;
-  @Override
-  public Path getJobStagingArea(String jobName, boolean create)
-      throws IOException {
-    Objects.requireNonNull(jobName, "Job name must not be null!");
-    if (jobsParentDir == null && create) {
-      jobsParentDir = new File(
-          "target/_staging_area_" + System.currentTimeMillis());
-      if (!jobsParentDir.mkdirs()) {
-        throw new IOException(
-            "Failed to mkdirs for" + jobsParentDir.getAbsolutePath());
-      }
-    }
-
-    this.jobDir = new File(jobsParentDir.getAbsolutePath(), jobName);
-    if (create && !jobDir.exists()) {
-      if (!jobDir.mkdirs()) {
-        throw new IOException("Failed to mkdirs for "
-            + jobDir.getAbsolutePath());
-      }
-    }
-    return new Path(jobDir.getAbsolutePath());
-  }
-
-  @Override
-  public Path getJobCheckpointDir(String jobName, boolean create)
-      throws IOException {
-    return new Path("s3://generated_checkpoint_dir");
-  }
-
-  @Override
-  public Path getModelDir(String modelName, boolean create)
-      throws IOException {
-    if (modelParentDir == null && create) {
-      modelParentDir = new File(
-          "target/_models_" + System.currentTimeMillis());
-      if (!modelParentDir.mkdirs()) {
-        throw new IOException(
-            "Failed to mkdirs for " + modelParentDir.getAbsolutePath());
-      }
-    }
-
-    File modelDir = new File(modelParentDir.getAbsolutePath(), modelName);
-    if (create) {
-      if (!modelDir.exists() && !modelDir.mkdirs()) {
-        throw new IOException("Failed to mkdirs for "
-            + modelDir.getAbsolutePath());
-      }
-    }
-    return new Path(modelDir.getAbsolutePath());
-  }
-
-  @Override
-  public FileSystem getDefaultFileSystem() throws IOException {
-    return FileSystem.getLocal(new Configuration());
-  }
-
-  @Override
-  public FileSystem getFileSystemByUri(String uri) throws IOException {
-    return getDefaultFileSystem();
-  }
-
-  @Override
-  public Path getUserRootFolder() throws IOException {
-    return new Path("s3://generated_root_dir");
-  }
-
-  @Override
-  public boolean isDir(String uri) throws IOException {
-    return getDefaultFileSystem().getFileStatus(
-        new Path(convertToStagingPath(uri))).isDirectory();
-
-  }
-
-  @Override
-  public boolean isRemote(String uri) throws IOException {
-    String scheme = new Path(uri).toUri().getScheme();
-    if (null == scheme) {
-      return false;
-    }
-    return !scheme.startsWith("file://");
-  }
-
-  private String convertToStagingPath(String uri) throws IOException {
-    String ret = uri;
-    if (isRemote(uri)) {
-      String dirName = new Path(uri).getName();
-      ret = this.jobDir.getAbsolutePath()
-          + "/" + dirName;
-    }
-    return ret;
-  }
-
-  /**
-   * We use staging dir as mock HDFS dir.
-   * */
-  @Override
-  public boolean copyRemoteToLocal(String remoteUri, String localUri)
-      throws IOException {
-    // mock the copy from HDFS into a local copy
-    Path remoteToLocalDir = new Path(convertToStagingPath(remoteUri));
-    File old = new File(convertToStagingPath(localUri));
-    if (old.isDirectory() && old.exists()) {
-      if (!FileUtil.fullyDelete(old)) {
-        throw new IOException("Cannot delete temp dir:"
-            + old.getAbsolutePath());
-      }
-    }
-    return FileUtil.copy(getDefaultFileSystem(), remoteToLocalDir,
-        new File(localUri), false,
-        getDefaultFileSystem().getConf());
-  }
-
-  @Override
-  public boolean existsRemoteFile(Path uri) throws IOException {
-    String fakeLocalFilePath = this.jobDir.getAbsolutePath()
-        + "/" + uri.getName();
-    return new File(fakeLocalFilePath).exists();
-  }
-
-  @Override
-  public FileStatus getRemoteFileStatus(Path p) throws IOException {
-    return getDefaultFileSystem().getFileStatus(new Path(
-        convertToStagingPath(p.toUri().toString())));
-  }
-
-  @Override
-  public long getRemoteFileSize(String uri) throws IOException {
-    // 5 byte for this file to test
-    if (uri.equals("https://a/b/1.patch")) {
-      return 5;
-    }
-    return 100 * 1024 * 1024;
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/common/MemorySubmarineStorage.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/common/MemorySubmarineStorage.java
deleted file mode 100644
index 013614e6406c..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/common/MemorySubmarineStorage.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.common;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-public class MemorySubmarineStorage extends SubmarineStorage {
-  private Map<String, Map<String, String>> jobsInfo = new HashMap<>();
-  private Map<String, Map<String, Map<String, String>>> modelsInfo =
-      new HashMap<>();
-
-  @Override
-  public synchronized void addNewJob(String jobName, Map<String, String> jobInfo)
-      throws IOException {
-    jobsInfo.put(jobName, jobInfo);
-  }
-
-  @Override
-  public synchronized Map<String, String> getJobInfoByName(String jobName)
-      throws IOException {
-    Map<String, String> info = jobsInfo.get(jobName);
-    if (info == null) {
-      throw new IOException("Failed to find job=" + jobName);
-    }
-    return info;
-  }
-
-  @Override
-  public synchronized void addNewModel(String modelName, String version,
-      Map<String, String> modelInfo) throws IOException {
-    if (!modelsInfo.containsKey(modelName)) {
-      modelsInfo.put(modelName, new HashMap<>());
-    }
-    modelsInfo.get(modelName).put(version, modelInfo);
-  }
-
-  @Override
-  public synchronized Map<String, String> getModelInfoByName(String modelName,
-      String version) throws IOException {
-
-    boolean notFound = false;
-    Map<String, String> info = null;
-    try {
-       info = modelsInfo.get(modelName).get(version);
-    } catch (NullPointerException e) {
-      notFound = true;
-    }
-
-    if (notFound || info == null) {
-      throw new IOException(
-          "Failed to find, model=" + modelName + " version=" + version);
-    }
-
-    return info;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/common/TestFSBasedSubmarineStorage.java b/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/common/TestFSBasedSubmarineStorage.java
deleted file mode 100644
index 52a68b3f8830..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/common/TestFSBasedSubmarineStorage.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.common;
-
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.fs.MockRemoteDirectoryManager;
-import org.junit.Assert;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-public class TestFSBasedSubmarineStorage {
-  private Map<String, String> getMap(String prefix) {
-    Map<String, String> map = new HashMap<>();
-    map.put(prefix + "1", "1");
-    map.put(prefix + "2", "2");
-    map.put(prefix + "3", "3");
-    map.put(prefix + "4", "4");
-    return map;
-  }
-
-  private void compareMap(Map<String, String> map1, Map<String, String> map2) {
-    Assert.assertEquals(map1.size(), map2.size());
-    for (String k : map1.keySet()) {
-      Assert.assertEquals(map1.get(k), map2.get(k));
-    }
-  }
-
-  @Test
-  public void testStorageOps() throws IOException {
-    MockRemoteDirectoryManager remoteDirectoryManager = new MockRemoteDirectoryManager();
-    ClientContext clientContext = mock(ClientContext.class);
-    when(clientContext.getRemoteDirectoryManager()).thenReturn(remoteDirectoryManager);
-    FSBasedSubmarineStorageImpl storage = new FSBasedSubmarineStorageImpl(
-        clientContext);
-    storage.addNewJob("job1", getMap("job1"));
-    storage.addNewJob("job2", getMap("job2"));
-    storage.addNewJob("job3", getMap("job3"));
-    storage.addNewJob("job4", new HashMap<>());
-    storage.addNewModel("model1", "1.0", getMap("model1_1.0"));
-    storage.addNewModel("model1", "2.0.0", getMap("model1_2.0.0"));
-    storage.addNewModel("model2", null, getMap("model1_default"));
-    storage.addNewModel("model2", "1.0", getMap("model2_1.0"));
-
-    // create a new storage and read it back.
-    storage = new FSBasedSubmarineStorageImpl(
-        clientContext);
-    compareMap(getMap("job1"), storage.getJobInfoByName("job1"));
-    compareMap(getMap("job2"), storage.getJobInfoByName("job2"));
-    compareMap(getMap("job3"), storage.getJobInfoByName("job3"));
-    compareMap(new HashMap<>(), storage.getJobInfoByName("job4"));
-    compareMap(getMap("model1_1.0"), storage.getModelInfoByName("model1", "1.0"));
-    compareMap(getMap("model1_2.0.0"), storage.getModelInfoByName("model1", "2.0.0"));
-    compareMap(getMap("model2_1.0"), storage.getModelInfoByName("model2", "1.0"));
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/core-site.xml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/core-site.xml
deleted file mode 100644
index 50ec1460bd60..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/core-site.xml
+++ /dev/null
@@ -1,21 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-<!-- Put site-specific property overrides in this file. -->
-
-<configuration>
-
-</configuration>
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/hdfs-site.xml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/hdfs-site.xml
deleted file mode 100644
index 50ec1460bd60..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/hdfs-site.xml
+++ /dev/null
@@ -1,21 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-<!-- Put site-specific property overrides in this file. -->
-
-<configuration>
-
-</configuration>
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/empty-framework.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/empty-framework.yaml
deleted file mode 100644
index e3c15a4f7722..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/empty-framework.yaml
+++ /dev/null
@@ -1,63 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework:
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-    docker_image: testDockerImagePs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/invalid-framework.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/invalid-framework.yaml
deleted file mode 100644
index ec91588687cd..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/invalid-framework.yaml
+++ /dev/null
@@ -1,63 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: bla
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32,gpu=2
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-  ps:
-    resources: memory=20500M,vcores=34,gpu=4
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-    docker_image: testDockerImagePs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37,gpu=3
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/missing-configs.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/missing-configs.yaml
deleted file mode 100644
index 6fa8ed45d4bb..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/missing-configs.yaml
+++ /dev/null
@@ -1,41 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/missing-framework.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/missing-framework.yaml
deleted file mode 100644
index 6523a38538fa..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/missing-framework.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-    docker_image: testDockerImagePs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/some-sections-missing.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/some-sections-missing.yaml
deleted file mode 100644
index 38505376d7b5..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/some-sections-missing.yaml
+++ /dev/null
@@ -1,49 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: tensorflow
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/test-false-values.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/test-false-values.yaml
deleted file mode 100644
index 0e300a0fe9f4..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/test-false-values.yaml
+++ /dev/null
@@ -1,57 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: tensorflow
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-  wait_job_finish: false
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: false
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/wrong-indentation.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/wrong-indentation.yaml
deleted file mode 100644
index 375c6803308a..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/wrong-indentation.yaml
+++ /dev/null
@@ -1,60 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-
-configs:
-      input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-    quicklinks:
-    - Notebook_UI=https://master-0:7070
-    - Notebook_UI2=https://master-0:7071
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/wrong-property-name.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/wrong-property-name.yaml
deleted file mode 100644
index 6ea9c37c2ad4..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-common-yaml/wrong-property-name.yaml
+++ /dev/null
@@ -1,60 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-
-CONFIGS:
-      input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-    quicklinks:
-    - Notebook_UI=https://master-0:7070
-    - Notebook_UI2=https://master-0:7071
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/envs-are-missing.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/envs-are-missing.yaml
deleted file mode 100644
index 758be1e154c7..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/envs-are-missing.yaml
+++ /dev/null
@@ -1,51 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: pytorch
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/invalid-config-ps-section.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/invalid-config-ps-section.yaml
deleted file mode 100644
index a161e61a8e67..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/invalid-config-ps-section.yaml
+++ /dev/null
@@ -1,56 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: pytorch
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-  ps:
-    docker_image: testPSDockerImage
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/invalid-config-tensorboard-section.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/invalid-config-tensorboard-section.yaml
deleted file mode 100644
index 675fba5bbb78..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/invalid-config-tensorboard-section.yaml
+++ /dev/null
@@ -1,57 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: pytorch
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/security-principal-is-missing.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/security-principal-is-missing.yaml
deleted file mode 100644
index 5f6c25661b45..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/security-principal-is-missing.yaml
+++ /dev/null
@@ -1,53 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: pytorch
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-
-security:
-  keytab: keytabPath
-  distribute_keytab: true
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/valid-config-with-overrides.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/valid-config-with-overrides.yaml
deleted file mode 100644
index 71ac275e31ec..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/valid-config-with-overrides.yaml
+++ /dev/null
@@ -1,63 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: pytorch
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: overridden_testLaunchCmdWorker
-    docker_image: overridden_testDockerImageWorker
-    envs:
-      env1: 'overridden_env1Worker'
-      env2: 'overridden_env2Worker'
-    localizations:
-    - hdfs://remote-file1:/overridden_local-filename1Worker:rw
-    - nfs://remote-file2:/overridden_local-filename2Worker:rw
-    mounts:
-    - /etc/passwd:/overridden_Worker
-    - /etc/hosts:/overridden_Worker
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/valid-config.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/valid-config.yaml
deleted file mode 100644
index 2b9dd50ba3b4..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/valid-config.yaml
+++ /dev/null
@@ -1,54 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: pytorch
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/valid-gpu-config.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/valid-gpu-config.yaml
deleted file mode 100644
index a494e9c7e7f5..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-pytorch-yaml/valid-gpu-config.yaml
+++ /dev/null
@@ -1,54 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: pytorch
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32,gpu=2
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/envs-are-missing.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/envs-are-missing.yaml
deleted file mode 100644
index 4deb962ecb1f..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/envs-are-missing.yaml
+++ /dev/null
@@ -1,60 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: tensorflow
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-    docker_image: testDockerImagePs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/security-principal-is-missing.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/security-principal-is-missing.yaml
deleted file mode 100644
index 43419232069d..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/security-principal-is-missing.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: tensorflow
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-    docker_image: testDockerImagePs
-
-security:
-  keytab: keytabPath
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/tensorboard-dockerimage-is-missing.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/tensorboard-dockerimage-is-missing.yaml
deleted file mode 100644
index cdefb9fa5ef3..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/tensorboard-dockerimage-is-missing.yaml
+++ /dev/null
@@ -1,62 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: tensorflow
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-    docker_image: testDockerImagePs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/valid-config-with-overrides.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/valid-config-with-overrides.yaml
deleted file mode 100644
index 042bf357b93a..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/valid-config-with-overrides.yaml
+++ /dev/null
@@ -1,82 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: tensorflow
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: overridden_testLaunchCmdWorker
-    docker_image: overridden_testDockerImageWorker
-    envs:
-      env1: 'overridden_env1Worker'
-      env2: 'overridden_env2Worker'
-    localizations:
-    - hdfs://remote-file1:/overridden_local-filename1Worker:rw
-    - nfs://remote-file2:/overridden_local-filename2Worker:rw
-    mounts:
-    - /etc/passwd:/overridden_Worker
-    - /etc/hosts:/overridden_Worker
-
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: overridden_testLaunchCmdPs
-    docker_image: overridden_testDockerImagePs
-    envs:
-      env1: 'overridden_env1Ps'
-      env2: 'overridden_env2Ps'
-    localizations:
-    - hdfs://remote-file1:/overridden_local-filename1Ps:rw
-    - nfs://remote-file2:/overridden_local-filename2Ps:rw
-    mounts:
-    - /etc/passwd:/overridden_Ps
-    - /etc/hosts:/overridden_Ps
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/valid-config.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/valid-config.yaml
deleted file mode 100644
index 7e312004adee..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/valid-config.yaml
+++ /dev/null
@@ -1,63 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: tensorflow
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-    docker_image: testDockerImagePs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/valid-gpu-config.yaml b/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/valid-gpu-config.yaml
deleted file mode 100644
index 1ef1df091a24..000000000000
--- a/hadoop-submarine/hadoop-submarine-core/src/test/resources/runjob-tensorflow-yaml/valid-gpu-config.yaml
+++ /dev/null
@@ -1,63 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-spec:
-  name: testJobName
-  job_type: testJobType
-  framework: tensorflow
-
-configs:
-  input_path: testInputPath
-  checkpoint_path: testCheckpointPath
-  saved_model_path: testSavedModelPath
-  docker_image: testDockerImage
-  wait_job_finish: true
-  envs:
-    env1: 'env1Value'
-    env2: 'env2Value'
-  localizations:
-  - hdfs://remote-file1:/local-filename1:rw
-  - nfs://remote-file2:/local-filename2:rw
-  mounts:
-  - /etc/passwd:/etc/passwd:rw
-  - /etc/hosts:/etc/hosts:rw
-  quicklinks:
-  - Notebook_UI=https://master-0:7070
-  - Notebook_UI2=https://master-0:7071
-
-scheduling:
-  queue: queue1
-
-roles:
-  worker:
-    resources: memory=20480M,vcores=32,gpu=2
-    replicas: 3
-    launch_cmd: testLaunchCmdWorker
-    docker_image: testDockerImageWorker
-  ps:
-    resources: memory=20500M,vcores=34
-    replicas: 4
-    launch_cmd: testLaunchCmdPs
-    docker_image: testDockerImagePs
-
-security:
-  keytab: keytabPath
-  principal: testPrincipal
-  distribute_keytab: true
-
-tensorBoard:
-  resources: memory=21000M,vcores=37
-  docker_image: tensorboardDockerImage
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-dist/pom.xml b/hadoop-submarine/hadoop-submarine-dist/pom.xml
deleted file mode 100644
index 8aa7db13f936..000000000000
--- a/hadoop-submarine/hadoop-submarine-dist/pom.xml
+++ /dev/null
@@ -1,131 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
-                      https://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <artifactId>hadoop-submarine</artifactId>
-    <groupId>org.apache.hadoop</groupId>
-    <version>0.3.0-SNAPSHOT</version>
-  </parent>
-  <artifactId>${project.artifactId}</artifactId>
-  <version>${project.version}</version>
-  <name>Hadoop Submarine Dist</name>
-  <packaging>pom</packaging>
-
-  <properties>
-    <!-- Needed for generating FindBugs warnings using parent pom -->
-    <yarn.basedir>${project.parent.parent.basedir}</yarn.basedir>
-    <project.artifactId>hadoop-submarine-dist</project.artifactId>
-    <project.version>0.3.0-SNAPSHOT</project.version>
-  </properties>
-
-  <dependencies>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-submarine-core</artifactId>
-      <version>${project.version}</version>
-    </dependency>
-  </dependencies>
-
-  <profiles>
-    <profile>
-      <id>hadoop-3.2</id>
-      <dependencies>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-yarnservice-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-tony-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-      </dependencies>
-    </profile>
-
-    <!-- Default profile-->
-    <profile>
-      <id>hadoop-3.1</id>
-      <activation>
-        <activeByDefault>true</activeByDefault>
-      </activation>
-      <dependencies>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-yarnservice-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-tony-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-      </dependencies>
-    </profile>
-
-    <profile>
-      <id>hadoop-2.9</id>
-      <dependencies>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-tony-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-      </dependencies>
-    </profile>
-
-    <profile>
-      <id>hadoop-2.7</id>
-      <dependencies>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-submarine-tony-runtime</artifactId>
-          <version>${project.version}</version>
-        </dependency>
-      </dependencies>
-    </profile>
-  </profiles>
-
-  <build>
-    <plugins>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-assembly-plugin</artifactId>
-        <executions>
-          <execution>
-            <id>dist</id>
-            <phase>package</phase>
-            <goals>
-              <goal>single</goal>
-            </goals>
-            <configuration>
-              <finalName>${project.artifactId}-${project.version}-${project.activeProfiles[0].id}</finalName>
-              <appendAssemblyId>false</appendAssemblyId>
-              <attach>false</attach>
-              <descriptors>
-                <descriptor>src/assembly/distribution.xml</descriptor>
-              </descriptors>
-            </configuration>
-          </execution>
-        </executions>
-      </plugin>
-    </plugins>
-  </build>
-
-</project>
diff --git a/hadoop-submarine/hadoop-submarine-dist/src/assembly/distribution.xml b/hadoop-submarine/hadoop-submarine-dist/src/assembly/distribution.xml
deleted file mode 100644
index c6e1c255a82b..000000000000
--- a/hadoop-submarine/hadoop-submarine-dist/src/assembly/distribution.xml
+++ /dev/null
@@ -1,61 +0,0 @@
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-<assembly xmlns="http://maven.apache.org/ASSEMBLY/2.0.0"
-    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-    xsi:schemaLocation="http://maven.apache.org/ASSEMBLY/2.0.0 http://maven.apache.org/xsd/assembly-2.0.0.xsd">
-  <id>distribution</id>
-  <formats>
-    <format>dir</format>
-    <format>tar.gz</format>
-  </formats>
-
-  <dependencySets>
-    <dependencySet>
-      <outputDirectory>/</outputDirectory>
-      <includes>
-        <include>org.apache.hadoop:hadoop-submarine-core</include>
-      </includes>
-    </dependencySet>
-    <dependencySet>
-      <outputDirectory>/</outputDirectory>
-      <includes>
-        <include>org.apache.hadoop:hadoop-submarine-tony-runtime</include>
-      </includes>
-    </dependencySet>
-    <dependencySet>
-      <outputDirectory>/</outputDirectory>
-      <includes>
-        <include>org.apache.hadoop:hadoop-submarine-yarnservice-runtime</include>
-      </includes>
-    </dependencySet>
-  </dependencySets>
-
-  <fileSets>
-    <fileSet>
-      <directory>../../</directory>
-      <includes>
-        <include>LICENSE*</include>
-        <include>NOTICE*</include>
-      </includes>
-    </fileSet>
-    <fileSet>
-      <directory>../hadoop-submarine-all/target/</directory>
-      <outputDirectory>/</outputDirectory>
-      <includes>
-        <include>hadoop-submarine-all-${project.version}-*.jar</include>
-      </includes>
-    </fileSet>
-  </fileSets>
-</assembly>
diff --git a/hadoop-submarine/hadoop-submarine-tony-runtime/README.md b/hadoop-submarine/hadoop-submarine-tony-runtime/README.md
deleted file mode 100644
index 988565bd1b1d..000000000000
--- a/hadoop-submarine/hadoop-submarine-tony-runtime/README.md
+++ /dev/null
@@ -1,25 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-# Overview
-[TonY](https://github.com/linkedin/TonY/) is an open source project that TonY
-is a framework to natively run deep learning frameworks on Apache Hadoop.
-As Submarine supports multiple runtimes, this module serves as an adaptor so
- that Submarine could leverage TonY as a Runtime implementation to run
-TensorFlow with 2.x version of Hadoop and Hadoop installations w/o docker
-support or native service. 
-
-Please jump to [QuickStart](src/site/markdown/QuickStart.md) to understand how
-to run Submarine with TonY runtime.
- 
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-tony-runtime/pom.xml b/hadoop-submarine/hadoop-submarine-tony-runtime/pom.xml
deleted file mode 100644
index 6e0615ad0c5f..000000000000
--- a/hadoop-submarine/hadoop-submarine-tony-runtime/pom.xml
+++ /dev/null
@@ -1,70 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
-    <parent>
-        <artifactId>hadoop-submarine</artifactId>
-        <groupId>org.apache.hadoop</groupId>
-        <version>0.3.0-SNAPSHOT</version>
-    </parent>
-    <modelVersion>4.0.0</modelVersion>
-
-    <artifactId>hadoop-submarine-tony-runtime</artifactId>
-    <name>Hadoop Submarine Tony Runtime</name>
-    <dependencies>
-        <dependency>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-submarine-core</artifactId>
-            <version>0.3.0-SNAPSHOT</version>
-            <scope>compile</scope>
-        </dependency>
-        <dependency>
-            <groupId>com.linkedin.tony</groupId>
-            <artifactId>tony-core</artifactId>
-            <version>0.3.19</version>
-            <exclusions>
-                <exclusion>
-                    <groupId>com.linkedin.tony</groupId>
-                    <artifactId>tony-mini</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>com.linkedin.azkaban</groupId>
-                    <artifactId>azkaban-common</artifactId>
-                </exclusion>
-                <exclusion>
-                    <groupId>com.linkedin.azkaban</groupId>
-                    <artifactId>az-hadoop-jobtype-plugin</artifactId>
-                </exclusion>
-            </exclusions>
-        </dependency>
-        <dependency>
-            <groupId>junit</groupId>
-            <artifactId>junit</artifactId>
-        </dependency>
-        <dependency>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-submarine-core</artifactId>
-            <type>test-jar</type>
-            <scope>test</scope>
-            <version>0.3.0-SNAPSHOT</version>
-        </dependency>
-        <dependency>
-            <groupId>org.mockito</groupId>
-            <artifactId>mockito-core</artifactId>
-            <scope>test</scope>
-        </dependency>
-    </dependencies>
-</project>
diff --git a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyJobMonitor.java b/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyJobMonitor.java
deleted file mode 100644
index 7540da8103c1..000000000000
--- a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyJobMonitor.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.tony;
-
-import com.linkedin.tony.TonyClient;
-import com.linkedin.tony.client.TaskUpdateListener;
-import com.linkedin.tony.rpc.TaskInfo;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.JobStatus;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobMonitor;
-import org.apache.hadoop.yarn.submarine.runtimes.tony.buider.JobStatusBuilder;
-
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.Set;
-
-/**
- * An implementation of JobMonitor with TonY library.
- */
-public class TonyJobMonitor extends JobMonitor implements TaskUpdateListener {
-  private Set<TaskInfo> taskInfos = new HashSet<>();
-
-  public TonyJobMonitor(ClientContext clientContext, TonyClient client) {
-    super(clientContext);
-    client.addListener(this);
-  }
-
-  @Override
-  public JobStatus getTrainingJobStatus(String jobName)
-      throws IOException, YarnException {
-    JobStatus jobStatus = JobStatusBuilder.fromTaskInfoSet(taskInfos);
-    jobStatus.setJobName(jobName);
-    return jobStatus;
-  }
-
-  @Override
-  public void onTaskInfosUpdated(Set<TaskInfo> taskInfoSet) {
-    this.taskInfos = taskInfoSet;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyJobSubmitter.java b/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyJobSubmitter.java
deleted file mode 100644
index 799de6df7484..000000000000
--- a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyJobSubmitter.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.tony;
-
-import com.linkedin.tony.Constants;
-import com.linkedin.tony.TonyClient;
-import com.linkedin.tony.client.CallbackHandler;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.Framework;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.OutputStream;
-
-/**
- * Implementation of JobSumitter with TonY runtime.
- */
-public class TonyJobSubmitter implements JobSubmitter, CallbackHandler {
-
-  private static final Log LOG = LogFactory.getLog(TonyJobSubmitter.class);
-  private ApplicationId applicationId;
-  private TonyClient tonyClient;
-
-  public TonyJobSubmitter() { }
-  public void setTonyClient(TonyClient client) {
-    this.tonyClient = client;
-  }
-
-  @Override
-  public ApplicationId submitJob(ParametersHolder parameters)
-      throws IOException {
-    if (parameters.getFramework() == Framework.PYTORCH) {
-      // we need to throw an exception, as ParametersHolder's parameters field
-      // could not be casted to TensorFlowRunJobParameters.
-      throw new UnsupportedOperationException(
-          "Support \"–-framework\" option for PyTorch in Tony is coming. " +
-              "Please check the documentation about how to submit a " +
-              "PyTorch job with TonY runtime.");
-    }
-
-    LOG.info("Starting Tony runtime..");
-
-    File tonyFinalConfPath = File.createTempFile("temp",
-        Constants.TONY_FINAL_XML);
-    // Write user's overridden conf to an xml to be localized.
-    Configuration tonyConf = TonyUtils.tonyConfFromClientContext(
-        (TensorFlowRunJobParameters) parameters.getParameters());
-    try (OutputStream os = new FileOutputStream(tonyFinalConfPath)) {
-      tonyConf.writeXml(os);
-    } catch (IOException e) {
-      throw new RuntimeException("Failed to create " + tonyFinalConfPath
-          + " conf file. Exiting.", e);
-    }
-
-    try {
-      tonyClient.init(new String[]{
-          "--conf_file", tonyFinalConfPath.getAbsolutePath()
-      });
-    } catch (Exception e) {
-      LOG.error("Failed to init TonyClient: ", e);
-    }
-    Thread clientThread = new Thread(tonyClient::start);
-    java.lang.Runtime.getRuntime().addShutdownHook(new Thread(() -> {
-      try {
-        tonyClient.forceKillApplication();
-      } catch (YarnException | IOException e) {
-        LOG.error("Failed to kill application during shutdown.", e);
-      }
-    }));
-    clientThread.start();
-    while (clientThread.isAlive()) {
-      if (applicationId != null) {
-        LOG.info("TonyClient returned applicationId: " + applicationId);
-        return applicationId;
-      }
-      try {
-        Thread.sleep(5000);
-      } catch (InterruptedException e) {
-        LOG.error(e);
-      }
-    }
-    return null;
-  }
-
-  @Override
-  public void onApplicationIdReceived(ApplicationId appId) {
-    applicationId = appId;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyRuntimeFactory.java b/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyRuntimeFactory.java
deleted file mode 100644
index 7a0d1703b91e..000000000000
--- a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyRuntimeFactory.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.tony;
-
-import com.linkedin.tony.TonyClient;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.runtimes.RuntimeFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.common.FSBasedSubmarineStorageImpl;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobMonitor;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.common.SubmarineStorage;
-
-/**
- * Implementation of RuntimeFactory with Tony Runtime
- */
-public class TonyRuntimeFactory extends RuntimeFactory {
-  private TonyClient tonyClient;
-  private TonyJobSubmitter submitter;
-  private TonyJobMonitor monitor;
-
-  public TonyRuntimeFactory(ClientContext clientContext) {
-    super(clientContext);
-    submitter = new TonyJobSubmitter();
-    tonyClient = new TonyClient(submitter, new Configuration());
-    monitor = new TonyJobMonitor(clientContext, tonyClient);
-    submitter.setTonyClient(tonyClient);
-  }
-
-  @Override
-  protected JobSubmitter internalCreateJobSubmitter() {
-    return submitter;
-  }
-
-  @Override
-  protected JobMonitor internalCreateJobMonitor() {
-    return monitor;
-  }
-
-  @Override
-  protected SubmarineStorage internalCreateSubmarineStorage() {
-    return new FSBasedSubmarineStorageImpl(super.clientContext);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyUtils.java b/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyUtils.java
deleted file mode 100644
index d818fe0d1fde..000000000000
--- a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyUtils.java
+++ /dev/null
@@ -1,159 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.tony;
-
-import com.linkedin.tony.Constants;
-import com.linkedin.tony.TonyConfigurationKeys;
-import com.linkedin.tony.util.Utils;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.resource.ResourceUtils;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-
-/**
- * Utilities for Tony Runtime.
- */
-public final class TonyUtils {
-  private static final Log LOG = LogFactory.getLog(TonyUtils.class);
-
-  public static Configuration tonyConfFromClientContext(
-      TensorFlowRunJobParameters parameters) {
-    Configuration tonyConf = new Configuration();
-    tonyConf.setInt(
-        TonyConfigurationKeys.getInstancesKey(Constants.WORKER_JOB_NAME),
-        parameters.getNumWorkers());
-    tonyConf.setInt(
-        TonyConfigurationKeys.getInstancesKey(Constants.PS_JOB_NAME),
-        parameters.getNumPS());
-    // Resources for PS & Worker
-    if (parameters.getPsResource() != null) {
-      tonyConf.setInt(
-          TonyConfigurationKeys.getResourceKey(Constants.PS_JOB_NAME,
-              Constants.VCORES),
-          parameters.getPsResource().getVirtualCores());
-      tonyConf.setLong(
-          TonyConfigurationKeys.getResourceKey(Constants.PS_JOB_NAME,
-              Constants.MEMORY),
-          ResourceUtils.getMemorySize(parameters.getPsResource()));
-    }
-    if (parameters.getWorkerResource() != null) {
-      tonyConf.setInt(
-          TonyConfigurationKeys.getResourceKey(Constants.WORKER_JOB_NAME,
-              Constants.VCORES),
-          parameters.getWorkerResource().getVirtualCores());
-      tonyConf.setLong(
-          TonyConfigurationKeys.getResourceKey(Constants.WORKER_JOB_NAME,
-              Constants.MEMORY),
-          ResourceUtils.getMemorySize(parameters.getWorkerResource()));
-      tonyConf.setLong(
-          TonyConfigurationKeys.getResourceKey(Constants.WORKER_JOB_NAME,
-              Constants.GPUS),
-          ResourceUtils.getResourceValue(parameters.getWorkerResource(),
-              ResourceUtils.GPU_URI));
-    }
-    if (parameters.getQueue() != null) {
-      tonyConf.set(
-          TonyConfigurationKeys.YARN_QUEUE_NAME,
-          parameters.getQueue());
-    }
-    // Set up Docker for PS & Worker
-    if (parameters.getDockerImageName() != null) {
-      tonyConf.set(TonyConfigurationKeys.getContainerDockerKey(),
-          parameters.getDockerImageName());
-      tonyConf.setBoolean(TonyConfigurationKeys.DOCKER_ENABLED, true);
-    }
-    if (parameters.getWorkerDockerImage() != null) {
-      tonyConf.set(
-          TonyConfigurationKeys.getDockerImageKey(Constants.WORKER_JOB_NAME),
-          parameters.getWorkerDockerImage());
-      tonyConf.setBoolean(TonyConfigurationKeys.DOCKER_ENABLED, true);
-    }
-    if (parameters.getPsDockerImage() != null) {
-      tonyConf.set(
-          TonyConfigurationKeys.getDockerImageKey(Constants.PS_JOB_NAME),
-          parameters.getPsDockerImage());
-      tonyConf.setBoolean(TonyConfigurationKeys.DOCKER_ENABLED, true);
-    }
-
-    // Set up container environment
-    List<String> envs = parameters.getEnvars();
-    tonyConf.setStrings(
-        TonyConfigurationKeys.CONTAINER_LAUNCH_ENV,
-        envs.toArray(new String[0]));
-    tonyConf.setStrings(TonyConfigurationKeys.EXECUTION_ENV,
-        envs.stream()
-            .map(env -> env.replaceAll("DOCKER_", ""))
-            .toArray(String[]::new));
-    tonyConf.setStrings(TonyConfigurationKeys.CONTAINER_LAUNCH_ENV,
-        envs.stream().map(env -> env.replaceAll("DOCKER_", ""))
-            .toArray(String[]::new));
-
-    // Set up running command
-    if (parameters.getWorkerLaunchCmd() != null) {
-      tonyConf.set(
-          TonyConfigurationKeys.getExecuteCommandKey(Constants.WORKER_JOB_NAME),
-          parameters.getWorkerLaunchCmd());
-    }
-
-    if (parameters.getPSLaunchCmd() != null) {
-      tonyConf.set(
-          TonyConfigurationKeys.getExecuteCommandKey(Constants.PS_JOB_NAME),
-          parameters.getPSLaunchCmd());
-    }
-
-    tonyConf.setBoolean(TonyConfigurationKeys.SECURITY_ENABLED,
-        !parameters.isSecurityDisabled());
-
-    // Set up container resources
-    if (parameters.getLocalizations() != null) {
-      tonyConf.setStrings(TonyConfigurationKeys.getContainerResourcesKey(),
-          parameters.getLocalizations().stream()
-              .map(lo -> lo.getRemoteUri() + Constants.RESOURCE_DIVIDER
-                  + lo.getLocalPath())
-              .toArray(String[]::new));
-    }
-
-    if (parameters.getConfPairs() != null) {
-      String[] confArray = parameters.getConfPairs().toArray(new String[0]);
-      for (Map.Entry<String, String> cliConf : Utils
-          .parseKeyValue(confArray).entrySet()) {
-        String[] existingValue = tonyConf.getStrings(cliConf.getKey());
-        if (existingValue != null
-            && TonyConfigurationKeys
-            .MULTI_VALUE_CONF.contains(cliConf.getKey())) {
-          ArrayList<String> newValues = new ArrayList<>(Arrays
-              .asList(existingValue));
-          newValues.add(cliConf.getValue());
-          tonyConf.setStrings(cliConf.getKey(),
-                newValues.toArray(new String[0]));
-        } else {
-          tonyConf.set(cliConf.getKey(), cliConf.getValue());
-        }
-      }
-    }
-
-    LOG.info("Resources: " + tonyConf.get(
-        TonyConfigurationKeys.getContainerResourcesKey()));
-    return tonyConf;
-  }
-
-  private TonyUtils() {
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/buider/JobStatusBuilder.java b/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/buider/JobStatusBuilder.java
deleted file mode 100644
index c9e72dca0c0e..000000000000
--- a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/buider/JobStatusBuilder.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.tony.buider;
-
-import com.linkedin.tony.rpc.TaskInfo;
-import org.apache.hadoop.yarn.submarine.common.api.JobComponentStatus;
-import org.apache.hadoop.yarn.submarine.common.api.JobStatus;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Set;
-import java.util.stream.Collectors;
-
-/**
- * JobStatusBuilder builds the job status from a set of TaskInfos.
- */
-public final class JobStatusBuilder {
-  public static JobStatus fromTaskInfoSet(final Set<TaskInfo> taskInfos) {
-    JobStatus status = new JobStatus();
-    Set<String> jobNames =
-        taskInfos.stream().map(TaskInfo::getName).collect(Collectors.toSet());
-    List<JobComponentStatus> jobComponentStatusList = new ArrayList<>();
-    for (String jobName : jobNames) {
-      Set<TaskInfo> filterTasks = taskInfos.stream()
-          .filter(taskInfo -> taskInfo.getName().equals(jobName))
-          .collect(Collectors.toSet());
-      long numReadyContainers = 0;
-      long numRunningContainers = 0;
-      long totalAskedContainers = 0;
-      for (TaskInfo taskInfo : filterTasks) {
-        totalAskedContainers += 1;
-        switch (taskInfo.getStatus()) {
-        case READY:
-          numReadyContainers += 1;
-          break;
-        case RUNNING:
-          numRunningContainers += 1;
-          break;
-        default:
-        }
-      }
-      jobComponentStatusList.add(new JobComponentStatus(jobName,
-          numReadyContainers, numRunningContainers, totalAskedContainers));
-    }
-    status.setComponentStatus(jobComponentStatusList);
-    return status;
-  }
-
-  private JobStatusBuilder() { }
-}
diff --git a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/buider/package-info.java b/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/buider/package-info.java
deleted file mode 100644
index 5dfd5a35d5c9..000000000000
--- a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/buider/package-info.java
+++ /dev/null
@@ -1,14 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.tony.buider;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/package-info.java b/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/package-info.java
deleted file mode 100644
index 4596202bcc0e..000000000000
--- a/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/package-info.java
+++ /dev/null
@@ -1,14 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.tony;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-tony-runtime/src/test/java/TestTonyUtils.java b/hadoop-submarine/hadoop-submarine-tony-runtime/src/test/java/TestTonyUtils.java
deleted file mode 100644
index 720bb00a5e53..000000000000
--- a/hadoop-submarine/hadoop-submarine-tony-runtime/src/test/java/TestTonyUtils.java
+++ /dev/null
@@ -1,125 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-import com.linkedin.tony.Constants;
-import com.linkedin.tony.TonyConfigurationKeys;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.RunJobCli;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.runtimes.RuntimeFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobMonitor;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.common.SubmarineStorage;
-import org.apache.hadoop.yarn.submarine.runtimes.tony.TonyUtils;
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.junit.Assert.assertTrue;
-import static org.mockito.ArgumentMatchers.any;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestTonyUtils {
-
-  private MockClientContext getMockClientContext()
-      throws IOException, YarnException {
-    MockClientContext mockClientContext = new MockClientContext();
-    JobSubmitter mockJobSubmitter = mock(JobSubmitter.class);
-    when(mockJobSubmitter.submitJob(
-        any(ParametersHolder.class))).thenReturn(
-        ApplicationId.newInstance(1234L, 1));
-    JobMonitor mockJobMonitor = mock(JobMonitor.class);
-    SubmarineStorage storage = mock(SubmarineStorage.class);
-    RuntimeFactory rtFactory = mock(RuntimeFactory.class);
-
-    when(rtFactory.getJobSubmitterInstance()).thenReturn(mockJobSubmitter);
-    when(rtFactory.getJobMonitorInstance()).thenReturn(mockJobMonitor);
-    when(rtFactory.getSubmarineStorage()).thenReturn(storage);
-
-    mockClientContext.setRuntimeFactory(rtFactory);
-    return mockClientContext;
-  }
-
-  @Before
-  public void before() {
-    SubmarineLogs.verboseOff();
-  }
-
-  @Test
-  public void testTonyConfFromClientContext() throws Exception {
-    RunJobCli runJobCli = new RunJobCli(getMockClientContext());
-    runJobCli.run(
-        new String[] {"--framework", "tensorflow", "--name", "my-job",
-            "--docker_image", "tf-docker:1.1.0",
-            "--input_path", "hdfs://input",
-            "--num_workers", "3", "--num_ps", "2", "--worker_launch_cmd",
-            "python run-job.py", "--worker_resources", "memory=2048M,vcores=2",
-            "--ps_resources", "memory=4G,vcores=4", "--ps_launch_cmd",
-            "python run-ps.py"});
-    RunJobParameters jobRunParameters = runJobCli.getRunJobParameters();
-
-    assertTrue(RunJobParameters.class + " must be an instance of " +
-            TensorFlowRunJobParameters.class,
-        jobRunParameters instanceof TensorFlowRunJobParameters);
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) jobRunParameters;
-
-    Configuration tonyConf = TonyUtils
-        .tonyConfFromClientContext(tensorFlowParams);
-    Assert.assertEquals(jobRunParameters.getDockerImageName(),
-        tonyConf.get(TonyConfigurationKeys.getContainerDockerKey()));
-    Assert.assertEquals("3", tonyConf.get(TonyConfigurationKeys
-        .getInstancesKey("worker")));
-    Assert.assertEquals(tensorFlowParams.getWorkerLaunchCmd(),
-        tonyConf.get(TonyConfigurationKeys
-            .getExecuteCommandKey("worker")));
-    Assert.assertEquals("2048", tonyConf.get(TonyConfigurationKeys
-        .getResourceKey(Constants.WORKER_JOB_NAME, Constants.MEMORY)));
-    Assert.assertEquals("2", tonyConf.get(TonyConfigurationKeys
-        .getResourceKey(Constants.WORKER_JOB_NAME, Constants.VCORES)));
-    Assert.assertEquals("4096", tonyConf.get(TonyConfigurationKeys
-        .getResourceKey(Constants.PS_JOB_NAME, Constants.MEMORY)));
-    Assert.assertEquals("4", tonyConf.get(TonyConfigurationKeys
-        .getResourceKey(Constants.PS_JOB_NAME,
-        Constants.VCORES)));
-    Assert.assertEquals(tensorFlowParams.getPSLaunchCmd(),
-        tonyConf.get(TonyConfigurationKeys.getExecuteCommandKey("ps")));
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/README.md b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/README.md
deleted file mode 100644
index cb2e2da107bc..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/README.md
+++ /dev/null
@@ -1,55 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-# Overview
-
-```$xslt
-              _                              _
-             | |                            (_)
-  ___  _   _ | |__   _ __ ___    __ _  _ __  _  _ __    ___
- / __|| | | || '_ \ | '_ ` _ \  / _` || '__|| || '_ \  / _ \
- \__ \| |_| || |_) || | | | | || (_| || |   | || | | ||  __/
- |___/ \__,_||_.__/ |_| |_| |_| \__,_||_|   |_||_| |_| \___|
-
-                             ?
- ~~~~~~~~~~~~~~~~~~~~~~~~~~~|^"~~~~~~~~~~~~~~~~~~~~~~~~~o~~~~~~~~~~~
-        o                   |                  o      __o
-         o                  |                 o     |X__>
-       ___o                 |                __o
-     (X___>--             __|__            |X__>     o
-                         |     \                   __o
-                         |      \                |X__>
-  _______________________|_______\________________
- <                                                \____________   _
-  \                                                            \ (_)
-   \    O       O       O                                       >=)
-    \__________________________________________________________/ (_)
-```
-
-Submarine is a project which allows infra engineer / data scientist to run *unmodified* Tensorflow programs on YARN.
-
-Goals of Submarine:
-- It allows jobs easy access data/models in HDFS and other storages.
-- Can launch services to serve Tensorflow/MXNet models.
-- Support run distributed Tensorflow jobs with simple configs.
-- Support run user-specified Docker images.
-- Support specify GPU and other resources.
-- Support launch tensorboard for training jobs if user specified.
-- Support customized DNS name for roles (like tensorboard.$user.$domain:6006)
-
-Please jump to [QuickStart](src/site/markdown/QuickStart.md) guide to quickly understand how to use this framework.
-
-Please jump to [Examples](src/site/markdown/Examples.md) to try other examples like running Distributed Tensorflow Training for CIFAR 10.
-
-If you're a developer, please find [Developer](src/site/markdown/DeveloperGuide.md) guide for more details.
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/pom.xml b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/pom.xml
deleted file mode 100644
index 19e629e81d14..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/pom.xml
+++ /dev/null
@@ -1,159 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
-                      https://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <artifactId>hadoop-submarine</artifactId>
-    <groupId>org.apache.hadoop</groupId>
-    <version>0.3.0-SNAPSHOT</version>
-  </parent>
-  <artifactId>hadoop-submarine-yarnservice-runtime</artifactId>
-  <version>0.3.0-SNAPSHOT</version>
-  <name>Hadoop Submarine YARN Service Runtime</name>
-
-  <properties>
-    <!-- Needed for generating FindBugs warnings using parent pom -->
-    <yarn.basedir>${project.parent.parent.basedir}</yarn.basedir>
-  </properties>
-
-  <dependencies>
-
-    <dependency>
-      <groupId>junit</groupId>
-      <artifactId>junit</artifactId>
-      <scope>test</scope>
-    </dependency>
-
-    <dependency>
-      <groupId>log4j</groupId>
-      <artifactId>log4j</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>com.google.guava</groupId>
-      <artifactId>guava</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>commons-logging</groupId>
-      <artifactId>commons-logging</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>commons-cli</groupId>
-      <artifactId>commons-cli</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>commons-io</groupId>
-      <artifactId>commons-io</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.slf4j</groupId>
-      <artifactId>slf4j-api</artifactId>
-    </dependency>
-
-    <!-- Dependencies for Hadoop commons -->
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-common</artifactId>
-      <scope>provided</scope>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-api</artifactId>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-common</artifactId>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-client</artifactId>
-    </dependency>
-
-    <dependency>
-      <groupId>org.mockito</groupId>
-      <artifactId>mockito-core</artifactId>
-      <scope>test</scope>
-    </dependency>
-
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-submarine-core</artifactId>
-      <type>test-jar</type>
-      <scope>test</scope>
-      <version>0.3.0-SNAPSHOT</version>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-submarine-core</artifactId>
-      <version>0.3.0-SNAPSHOT</version>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-services-api</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-services-core</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-common</artifactId>
-      <type>test-jar</type>
-      <scope>test</scope>
-    </dependency>
-  </dependencies>
-
-  <build>
-    <plugins>
-      <plugin>
-        <artifactId>maven-jar-plugin</artifactId>
-        <executions>
-          <execution>
-            <goals>
-              <goal>jar</goal>
-            </goals>
-            <!-- strictly speaking, the unit test is really a regression test. It
-                 needs the main jar to be available to be able to run. -->
-            <phase>test-compile</phase>
-          </execution>
-        </executions>
-        <configuration>
-           <archive>
-             <manifest>
-               <mainClass>org.apache.hadoop.yarn.submarine.client.cli.Cli</mainClass>
-             </manifest>
-           </archive>
-        </configuration>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-surefire-plugin</artifactId>
-        <configuration>
-          <environmentVariables>
-            <JAVA_HOME>${java.home}</JAVA_HOME>
-          </environmentVariables>
-       </configuration>
-      </plugin>
-    </plugins>
-  </build>
-
-
-</project>
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/AbstractComponent.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/AbstractComponent.java
deleted file mode 100644
index 0e0bc751b1a7..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/AbstractComponent.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.PyTorchRole;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.AbstractLaunchCommand;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.LaunchCommandFactory;
-
-import java.io.IOException;
-import java.util.Objects;
-
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowCommons.addCommonEnvironments;
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowCommons.getScriptFileName;
-import static org.apache.hadoop.yarn.submarine.utils.DockerUtilities.getDockerArtifact;
-import static org.apache.hadoop.yarn.submarine.utils.SubmarineResourceUtils.convertYarnResourceToServiceResource;
-
-/**
- * Abstract base class for Component classes.
- * The implementations of this class are act like factories for
- * {@link Component} instances.
- * All dependencies are passed to the constructor so that child classes
- * are obliged to provide matching constructors.
- */
-public abstract class AbstractComponent {
-  private final FileSystemOperations fsOperations;
-  protected final RunJobParameters parameters;
-  protected final Role role;
-  private final RemoteDirectoryManager remoteDirectoryManager;
-  protected final Configuration yarnConfig;
-  private final LaunchCommandFactory launchCommandFactory;
-
-  /**
-   * This is only required for testing.
-   */
-  private String localScriptFile;
-
-  public AbstractComponent(FileSystemOperations fsOperations,
-      RemoteDirectoryManager remoteDirectoryManager,
-      RunJobParameters parameters, Role role,
-      Configuration yarnConfig,
-      LaunchCommandFactory launchCommandFactory) {
-    this.fsOperations = fsOperations;
-    this.remoteDirectoryManager = remoteDirectoryManager;
-    this.parameters = parameters;
-    this.role = role;
-    this.launchCommandFactory = launchCommandFactory;
-    this.yarnConfig = yarnConfig;
-  }
-
-  protected abstract Component createComponent() throws IOException;
-
-  protected Component createComponentInternal() throws IOException {
-    Objects.requireNonNull(this.parameters.getWorkerResource(),
-        "Worker resource must not be null!");
-    if (parameters.getNumWorkers() < 1) {
-      throw new IllegalArgumentException(
-          "Number of workers should be at least 1!");
-    }
-
-    Component component = new Component();
-    component.setName(role.getComponentName());
-
-    if (role.equals(TensorFlowRole.PRIMARY_WORKER) ||
-        role.equals(PyTorchRole.PRIMARY_WORKER)) {
-      component.setNumberOfContainers(1L);
-      // If the dependencies are upgraded to hadoop 3.3.0.
-      // yarn.service.container-state-report-as-service-state can be replaced
-      // with CONTAINER_STATE_REPORT_AS_SERVICE_STATE
-      component.getConfiguration().setProperty(
-          "yarn.service.container-state-report-as-service-state", "true");
-    } else {
-      component.setNumberOfContainers(
-          (long) parameters.getNumWorkers() - 1);
-    }
-
-    if (parameters.getWorkerDockerImage() != null) {
-      component.setArtifact(
-          getDockerArtifact(parameters.getWorkerDockerImage()));
-    }
-
-    component.setResource(convertYarnResourceToServiceResource(
-        parameters.getWorkerResource()));
-    component.setRestartPolicy(Component.RestartPolicyEnum.NEVER);
-
-    addCommonEnvironments(component, role);
-    generateLaunchCommand(component);
-
-    return component;
-  }
-
-  /**
-   * Generates a command launch script on local disk,
-   * returns path to the script.
-   */
-  protected void generateLaunchCommand(Component component)
-      throws IOException {
-    AbstractLaunchCommand launchCommand =
-        launchCommandFactory.createLaunchCommand(role, component);
-    this.localScriptFile = launchCommand.generateLaunchScript();
-
-    String remoteLaunchCommand = uploadLaunchCommand(component);
-    component.setLaunchCommand(remoteLaunchCommand);
-  }
-
-  private String uploadLaunchCommand(Component component)
-      throws IOException {
-    Objects.requireNonNull(localScriptFile, "localScriptFile should be " +
-        "set before calling this method!");
-    Path stagingDir =
-        remoteDirectoryManager.getJobStagingArea(parameters.getName(), true);
-
-    String destScriptFileName = getScriptFileName(role);
-    fsOperations.uploadToRemoteFileAndLocalizeToContainerWorkDir(stagingDir,
-        localScriptFile, destScriptFileName, component);
-
-    return "./" + destScriptFileName;
-  }
-
-  String getLocalScriptFile() {
-    return localScriptFile;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/AbstractServiceSpec.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/AbstractServiceSpec.java
deleted file mode 100644
index 3790ffe0bf50..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/AbstractServiceSpec.java
+++ /dev/null
@@ -1,168 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.submarine.client.cli.param.Quicklink;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.Framework;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.PyTorchRole;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.LaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.utils.KerberosPrincipalFactory;
-import org.apache.hadoop.yarn.submarine.utils.Localizer;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowCommons.getDNSDomain;
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowCommons.getUserName;
-import static org.apache.hadoop.yarn.submarine.utils.DockerUtilities.getDockerArtifact;
-import static org.apache.hadoop.yarn.submarine.utils.EnvironmentUtilities.handleServiceEnvs;
-
-/**
- * Abstract base class that supports creating service specs for Native Service.
- */
-public abstract class AbstractServiceSpec implements ServiceSpec {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(AbstractServiceSpec.class);
-  protected final RunJobParameters parameters;
-  protected final FileSystemOperations fsOperations;
-  private final Localizer localizer;
-  protected final RemoteDirectoryManager remoteDirectoryManager;
-  protected final Configuration yarnConfig;
-  protected final LaunchCommandFactory launchCommandFactory;
-  private final WorkerComponentFactory workerFactory;
-
-  public AbstractServiceSpec(RunJobParameters parameters,
-      ClientContext clientContext, FileSystemOperations fsOperations,
-      LaunchCommandFactory launchCommandFactory,
-      Localizer localizer) {
-    this.parameters = parameters;
-    this.remoteDirectoryManager = clientContext.getRemoteDirectoryManager();
-    this.yarnConfig = clientContext.getYarnConfig();
-    this.fsOperations = fsOperations;
-    this.localizer = localizer;
-    this.launchCommandFactory = launchCommandFactory;
-    this.workerFactory = new WorkerComponentFactory(fsOperations,
-        remoteDirectoryManager, parameters, launchCommandFactory, yarnConfig);
-  }
-
-  protected ServiceWrapper createServiceSpecWrapper() throws IOException {
-    Service serviceSpec = new Service();
-    serviceSpec.setName(parameters.getName());
-    serviceSpec.setVersion(String.valueOf(System.currentTimeMillis()));
-    serviceSpec.setArtifact(getDockerArtifact(parameters.getDockerImageName()));
-    serviceSpec.setQueue(parameters.getQueue());
-
-    KerberosPrincipal kerberosPrincipal = KerberosPrincipalFactory
-        .create(fsOperations, remoteDirectoryManager, parameters);
-    if (kerberosPrincipal != null) {
-      serviceSpec.setKerberosPrincipal(kerberosPrincipal);
-    }
-
-    handleServiceEnvs(serviceSpec, yarnConfig, parameters.getEnvars());
-    localizer.handleLocalizations(serviceSpec);
-    return new ServiceWrapper(serviceSpec);
-  }
-
-
-  // Handle worker and primary_worker.
-  protected void addWorkerComponents(ServiceWrapper serviceWrapper,
-      Framework framework)
-      throws IOException {
-    final Role primaryWorkerRole;
-    final Role workerRole;
-    if (framework == Framework.TENSORFLOW) {
-      primaryWorkerRole = TensorFlowRole.PRIMARY_WORKER;
-      workerRole = TensorFlowRole.WORKER;
-    } else {
-      primaryWorkerRole = PyTorchRole.PRIMARY_WORKER;
-      workerRole = PyTorchRole.WORKER;
-    }
-
-    addWorkerComponent(serviceWrapper, primaryWorkerRole, framework);
-
-    if (parameters.getNumWorkers() > 1) {
-      addWorkerComponent(serviceWrapper, workerRole, framework);
-    }
-  }
-  private void addWorkerComponent(ServiceWrapper serviceWrapper,
-      Role role, Framework framework) throws IOException {
-    AbstractComponent component = workerFactory.create(framework, role);
-    serviceWrapper.addComponent(component);
-  }
-
-  protected void handleQuicklinks(Service serviceSpec)
-      throws IOException {
-    List<Quicklink> quicklinks = parameters.getQuicklinks();
-    if (quicklinks != null && !quicklinks.isEmpty()) {
-      for (Quicklink ql : quicklinks) {
-        // Make sure it is a valid instance name
-        String instanceName = ql.getComponentInstanceName();
-        boolean found = false;
-
-        for (Component comp : serviceSpec.getComponents()) {
-          for (int i = 0; i < comp.getNumberOfContainers(); i++) {
-            String possibleInstanceName = comp.getName() + "-" + i;
-            if (possibleInstanceName.equals(instanceName)) {
-              found = true;
-              break;
-            }
-          }
-        }
-
-        if (!found) {
-          throw new IOException(
-              "Couldn't find a component instance = " + instanceName
-                  + " while adding quicklink");
-        }
-
-        String link = ql.getProtocol()
-            + YarnServiceUtils.getDNSName(serviceSpec.getName(), instanceName,
-            getUserName(), getDNSDomain(yarnConfig), ql.getPort());
-        addQuicklink(serviceSpec, ql.getLabel(), link);
-      }
-    }
-  }
-
-  protected static void addQuicklink(Service serviceSpec, String label,
-      String link) {
-    Map<String, String> quicklinks = serviceSpec.getQuicklinks();
-    if (quicklinks == null) {
-      quicklinks = new HashMap<>();
-      serviceSpec.setQuicklinks(quicklinks);
-    }
-
-    if (SubmarineLogs.isVerbose()) {
-      LOG.info("Added quicklink, " + label + "=" + link);
-    }
-
-    quicklinks.put(label, link);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/FileSystemOperations.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/FileSystemOperations.java
deleted file mode 100644
index 14955fb8b8ac..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/FileSystemOperations.java
+++ /dev/null
@@ -1,211 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import com.google.common.annotations.VisibleForTesting;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.ConfigFile;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineConfiguration;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.utils.ZipUtilities;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-/**
- * Contains methods to perform file system operations. Almost all of the methods
- * are regular non-static methods as the operations are performed with the help
- * of a {@link RemoteDirectoryManager} instance passed in as a constructor
- * dependency. Please note that some operations require to read config settings
- * as well, so that we have Submarine and YARN config objects as dependencies as
- * well.
- */
-public class FileSystemOperations {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(FileSystemOperations.class);
-  private final Configuration submarineConfig;
-  private final Configuration yarnConfig;
-
-  private Set<Path> uploadedFiles = new HashSet<>();
-  private RemoteDirectoryManager remoteDirectoryManager;
-
-  public FileSystemOperations(ClientContext clientContext) {
-    this.remoteDirectoryManager = clientContext.getRemoteDirectoryManager();
-    this.submarineConfig = clientContext.getSubmarineConfig();
-    this.yarnConfig = clientContext.getYarnConfig();
-  }
-
-  /**
-   * May download a remote uri(file/dir) and zip.
-   * Skip download if local dir
-   * Remote uri can be a local dir(won't download)
-   * or remote HDFS dir, s3 dir/file .etc
-   * */
-  public String downloadAndZip(String remoteDir, String zipFileName,
-      boolean doZip)
-      throws IOException {
-    //Append original modification time and size to zip file name
-    String suffix;
-    String srcDir = remoteDir;
-    String zipDirPath =
-        System.getProperty("java.io.tmpdir") + "/" + zipFileName;
-    boolean needDeleteTempDir = false;
-    if (remoteDirectoryManager.isRemote(remoteDir)) {
-      //Append original modification time and size to zip file name
-      FileStatus status =
-          remoteDirectoryManager.getRemoteFileStatus(new Path(remoteDir));
-      suffix = "_" + status.getModificationTime()
-          + "-" + remoteDirectoryManager.getRemoteFileSize(remoteDir);
-      // Download them to temp dir
-      boolean downloaded =
-          remoteDirectoryManager.copyRemoteToLocal(remoteDir, zipDirPath);
-      if (!downloaded) {
-        throw new IOException("Failed to download files from "
-            + remoteDir);
-      }
-      LOG.info("Downloaded remote: {} to local: {}", remoteDir, zipDirPath);
-      srcDir = zipDirPath;
-      needDeleteTempDir = true;
-    } else {
-      File localDir = new File(remoteDir);
-      suffix = "_" + localDir.lastModified()
-          + "-" + localDir.length();
-    }
-    if (!doZip) {
-      return srcDir;
-    }
-    // zip a local dir
-    String zipFileUri =
-        ZipUtilities.zipDir(srcDir, zipDirPath + suffix + ".zip");
-    // delete downloaded temp dir
-    if (needDeleteTempDir) {
-      deleteFiles(srcDir);
-    }
-    return zipFileUri;
-  }
-
-  public void deleteFiles(String localUri) {
-    boolean success = FileUtil.fullyDelete(new File(localUri));
-    if (!success) {
-      LOG.warn("Failed to delete {}", localUri);
-    }
-    LOG.info("Deleted {}", localUri);
-  }
-
-  @VisibleForTesting
-  public void uploadToRemoteFileAndLocalizeToContainerWorkDir(Path stagingDir,
-      String fileToUpload, String destFilename, Component comp)
-      throws IOException {
-    Path uploadedFilePath = uploadToRemoteFile(stagingDir, fileToUpload);
-    locateRemoteFileToContainerWorkDir(destFilename, comp, uploadedFilePath);
-  }
-
-  private void locateRemoteFileToContainerWorkDir(String destFilename,
-      Component comp, Path uploadedFilePath)
-      throws IOException {
-    FileSystem fs = FileSystem.get(yarnConfig);
-
-    FileStatus fileStatus = fs.getFileStatus(uploadedFilePath);
-    LOG.info("Uploaded file path = " + fileStatus.getPath());
-
-    // Set it to component's files list
-    comp.getConfiguration().getFiles().add(new ConfigFile().srcFile(
-        fileStatus.getPath().toUri().toString()).destFile(destFilename)
-        .type(ConfigFile.TypeEnum.STATIC));
-  }
-
-  public Path uploadToRemoteFile(Path stagingDir, String fileToUpload) throws
-      IOException {
-    FileSystem fs = remoteDirectoryManager.getDefaultFileSystem();
-
-    // Upload to remote FS under staging area
-    File localFile = new File(fileToUpload);
-    if (!localFile.exists()) {
-      throw new FileNotFoundException(
-          "Trying to upload file=" + localFile.getAbsolutePath()
-              + " to remote, but couldn't find local file.");
-    }
-    String filename = new File(fileToUpload).getName();
-
-    Path uploadedFilePath = new Path(stagingDir, filename);
-    if (!uploadedFiles.contains(uploadedFilePath)) {
-      if (SubmarineLogs.isVerbose()) {
-        LOG.info("Copying local file=" + fileToUpload + " to remote="
-            + uploadedFilePath);
-      }
-      fs.copyFromLocalFile(new Path(fileToUpload), uploadedFilePath);
-      uploadedFiles.add(uploadedFilePath);
-    }
-    return uploadedFilePath;
-  }
-
-  public void validFileSize(String uri) throws IOException {
-    long actualSizeByte;
-    String locationType = "Local";
-    if (remoteDirectoryManager.isRemote(uri)) {
-      actualSizeByte = remoteDirectoryManager.getRemoteFileSize(uri);
-      locationType = "Remote";
-    } else {
-      actualSizeByte = FileUtil.getDU(new File(uri));
-    }
-    long maxFileSizeMB = submarineConfig
-        .getLong(SubmarineConfiguration.LOCALIZATION_MAX_ALLOWED_FILE_SIZE_MB,
-            SubmarineConfiguration.DEFAULT_MAX_ALLOWED_REMOTE_URI_SIZE_MB);
-    LOG.info("{} fie/dir: {}, size(Byte):{},"
-            + " Allowed max file/dir size: {}",
-        locationType, uri, actualSizeByte, maxFileSizeMB * 1024 * 1024);
-
-    if (actualSizeByte > maxFileSizeMB * 1024 * 1024) {
-      throw new IOException(uri + " size(Byte): "
-          + actualSizeByte + " exceeds configured max size:"
-          + maxFileSizeMB * 1024 * 1024);
-    }
-  }
-
-  public void setPermission(Path destPath, FsPermission permission) throws
-      IOException {
-    FileSystem fs = FileSystem.get(yarnConfig);
-    fs.setPermission(destPath, new FsPermission(permission));
-  }
-
-  public static boolean needHdfs(List<String> stringsToCheck) {
-    for (String content : stringsToCheck) {
-      if (content != null && content.contains("hdfs://")) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  public static boolean needHdfs(String content) {
-    return content != null && content.contains("hdfs://");
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/HadoopEnvironmentSetup.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/HadoopEnvironmentSetup.java
deleted file mode 100644
index 7bfa8774244a..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/HadoopEnvironmentSetup.java
+++ /dev/null
@@ -1,176 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import org.apache.curator.shaded.com.google.common.collect.ImmutableList;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.List;
-import java.util.Objects;
-
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations.needHdfs;
-import static org.apache.hadoop.yarn.submarine.utils.ClassPathUtilities.findFileOnClassPath;
-import static org.apache.hadoop.yarn.submarine.utils.EnvironmentUtilities.getValueOfEnvironment;
-
-/**
- * This class contains helper methods to fill HDFS and Java environment
- * variables into scripts.
- */
-public class HadoopEnvironmentSetup {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(HadoopEnvironmentSetup.class);
-  private static final String CORE_SITE_XML = "core-site.xml";
-  private static final String HDFS_SITE_XML = "hdfs-site.xml";
-
-  public static final String DOCKER_HADOOP_HDFS_HOME =
-      "DOCKER_HADOOP_HDFS_HOME";
-  public static final String DOCKER_JAVA_HOME = "DOCKER_JAVA_HOME";
-  private final RemoteDirectoryManager remoteDirectoryManager;
-  private final FileSystemOperations fsOperations;
-
-  public HadoopEnvironmentSetup(ClientContext clientContext,
-      FileSystemOperations fsOperations) {
-    this.remoteDirectoryManager = clientContext.getRemoteDirectoryManager();
-    this.fsOperations = fsOperations;
-  }
-
-  public void addHdfsClassPath(RunJobParameters parameters,
-      PrintWriter fw, Component comp) throws IOException {
-    // Find envs to use HDFS
-    String hdfsHome = null;
-    String javaHome = null;
-
-    boolean hadoopEnv = false;
-
-    for (String envVar : parameters.getEnvars()) {
-      if (envVar.startsWith(DOCKER_HADOOP_HDFS_HOME + "=")) {
-        hdfsHome = getValueOfEnvironment(envVar);
-        hadoopEnv = true;
-      } else if (envVar.startsWith(DOCKER_JAVA_HOME + "=")) {
-        javaHome = getValueOfEnvironment(envVar);
-      }
-    }
-
-    boolean hasHdfsEnvs = hdfsHome != null && javaHome != null;
-    boolean needHdfs = doesNeedHdfs(parameters, hadoopEnv);
-    if (needHdfs) {
-      // HDFS is asked either in input or output, set LD_LIBRARY_PATH
-      // and classpath
-      if (hdfsHome != null) {
-        appendHdfsHome(fw, hdfsHome);
-      }
-
-      // hadoop confs will be uploaded to HDFS and localized to container's
-      // local folder, so here set $HADOOP_CONF_DIR to $WORK_DIR.
-      fw.append("export HADOOP_CONF_DIR=$WORK_DIR\n");
-      if (javaHome != null) {
-        appendJavaHome(fw, javaHome);
-      }
-
-      fw.append(
-          "export CLASSPATH=`$HADOOP_HDFS_HOME/bin/hadoop classpath --glob`\n");
-    }
-
-    if (needHdfs && !hasHdfsEnvs) {
-      LOG.error("When HDFS is being used to read/write models/data, " +
-          "the following environment variables are required: " +
-          "1) {}=<HDFS_HOME inside docker container> " +
-          "2) {}=<JAVA_HOME inside docker container>. " +
-          "You can use --env to pass these environment variables.",
-          DOCKER_HADOOP_HDFS_HOME, DOCKER_JAVA_HOME);
-      throw new IOException("Failed to detect HDFS-related environments.");
-    }
-
-    // Trying to upload core-site.xml and hdfs-site.xml
-    Path stagingDir =
-        remoteDirectoryManager.getJobStagingArea(
-            parameters.getName(), true);
-    File coreSite = findFileOnClassPath(CORE_SITE_XML);
-    File hdfsSite = findFileOnClassPath(HDFS_SITE_XML);
-    if (coreSite == null || hdfsSite == null) {
-      LOG.error("HDFS is being used, however we could not locate " +
-          "{} nor {} on classpath! " +
-          "Please double check your classpath setting and make sure these " +
-          "setting files are included!", CORE_SITE_XML, HDFS_SITE_XML);
-      throw new IOException(
-          "Failed to locate core-site.xml / hdfs-site.xml on classpath!");
-    }
-    fsOperations.uploadToRemoteFileAndLocalizeToContainerWorkDir(stagingDir,
-        coreSite.getAbsolutePath(), CORE_SITE_XML, comp);
-    fsOperations.uploadToRemoteFileAndLocalizeToContainerWorkDir(stagingDir,
-        hdfsSite.getAbsolutePath(), HDFS_SITE_XML, comp);
-
-    // DEBUG
-    if (SubmarineLogs.isVerbose()) {
-      appendEchoOfEnvVars(fw);
-    }
-  }
-
-  private boolean doesNeedHdfs(RunJobParameters parameters, boolean hadoopEnv) {
-    List<String> launchCommands = parameters.getLaunchCommands();
-    if (launchCommands != null) {
-      launchCommands.removeIf(Objects::isNull);
-    }
-
-    ImmutableList.Builder<String> listBuilder = ImmutableList.builder();
-
-    if (launchCommands != null && !launchCommands.isEmpty()) {
-      listBuilder.addAll(launchCommands);
-    }
-    if (parameters.getInputPath() != null) {
-      listBuilder.add(parameters.getInputPath());
-    }
-    List<String> stringsToCheck = listBuilder.build();
-
-    return needHdfs(stringsToCheck) || hadoopEnv;
-  }
-
-  private void appendHdfsHome(PrintWriter fw, String hdfsHome) {
-    // Unset HADOOP_HOME/HADOOP_YARN_HOME to make sure host machine's envs
-    // won't pollute docker's env.
-    fw.append("export HADOOP_HOME=\n");
-    fw.append("export HADOOP_YARN_HOME=\n");
-    fw.append("export HADOOP_HDFS_HOME=" + hdfsHome + "\n");
-    fw.append("export HADOOP_COMMON_HOME=" + hdfsHome + "\n");
-  }
-
-  private void appendJavaHome(PrintWriter fw, String javaHome) {
-    fw.append("export JAVA_HOME=" + javaHome + "\n");
-    fw.append("export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:"
-        + "$JAVA_HOME/lib/amd64/server\n");
-  }
-
-  private void appendEchoOfEnvVars(PrintWriter fw) {
-    fw.append("echo \"CLASSPATH:$CLASSPATH\"\n");
-    fw.append("echo \"HADOOP_CONF_DIR:$HADOOP_CONF_DIR\"\n");
-    fw.append(
-        "echo \"HADOOP_TOKEN_FILE_LOCATION:$HADOOP_TOKEN_FILE_LOCATION\"\n");
-    fw.append("echo \"JAVA_HOME:$JAVA_HOME\"\n");
-    fw.append("echo \"LD_LIBRARY_PATH:$LD_LIBRARY_PATH\"\n");
-    fw.append("echo \"HADOOP_HDFS_HOME:$HADOOP_HDFS_HOME\"\n");
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceSpec.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceSpec.java
deleted file mode 100644
index f26d61071c6f..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceSpec.java
+++ /dev/null
@@ -1,27 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import java.io.IOException;
-
-/**
- * This interface is to provide means of creating wrappers around
- * {@link org.apache.hadoop.yarn.service.api.records.Service} instances.
- */
-public interface ServiceSpec {
-  ServiceWrapper create() throws IOException;
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceSpecFileGenerator.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceSpecFileGenerator.java
deleted file mode 100644
index 9c29f0ed02a7..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceSpecFileGenerator.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import org.apache.hadoop.yarn.service.api.records.Service;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.OutputStreamWriter;
-import java.io.PrintWriter;
-import java.io.Writer;
-import java.nio.charset.StandardCharsets;
-
-import static org.apache.hadoop.yarn.service.utils.ServiceApiUtil.jsonSerDeser;
-
-/**
- * This class is merely responsible for creating Json representation of
- * {@link Service} instances.
- */
-public final class ServiceSpecFileGenerator {
-  private ServiceSpecFileGenerator() {
-    throw new UnsupportedOperationException("This class should not be " +
-        "instantiated!");
-  }
-
-  public static String generateJson(Service service) throws IOException {
-    File serviceSpecFile = File.createTempFile(service.getName(), ".json");
-    String buffer = jsonSerDeser.toJson(service);
-    Writer w = new OutputStreamWriter(new FileOutputStream(serviceSpecFile),
-        StandardCharsets.UTF_8);
-    try (PrintWriter pw = new PrintWriter(w)) {
-      pw.append(buffer);
-    }
-    return serviceSpecFile.getAbsolutePath();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceWrapper.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceWrapper.java
deleted file mode 100644
index 3891602c0239..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceWrapper.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.collect.Maps;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.Service;
-
-import java.io.IOException;
-import java.util.Map;
-
-/**
- * This class is only existing because we need a component name to
- * local launch command mapping from the test code.
- * Once this is solved in more clean or different way, we can delete this class.
- */
-public class ServiceWrapper {
-  private final Service service;
-
-  @VisibleForTesting
-  private Map<String, String> componentToLocalLaunchCommand = Maps.newHashMap();
-
-  public ServiceWrapper(Service service) {
-    this.service = service;
-  }
-
-  public void addComponent(AbstractComponent abstractComponent)
-      throws IOException {
-    Component component = abstractComponent.createComponent();
-    service.addComponent(component);
-    storeComponentName(abstractComponent, component.getName());
-  }
-
-  private void storeComponentName(
-      AbstractComponent component, String name) {
-    componentToLocalLaunchCommand.put(name,
-        component.getLocalScriptFile());
-  }
-
-  public Service getService() {
-    return service;
-  }
-
-  public String getLocalLaunchCommandPathForComponent(String componentName) {
-    return componentToLocalLaunchCommand.get(componentName);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/WorkerComponentFactory.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/WorkerComponentFactory.java
deleted file mode 100644
index a79a05bbed98..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/WorkerComponentFactory.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.Framework;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.LaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.PyTorchLaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.TensorFlowLaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.pytorch.component.PyTorchWorkerComponent;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component.TensorFlowWorkerComponent;
-
-/**
- * Factory class that helps creating Native Service components.
- */
-public class WorkerComponentFactory {
-  private final FileSystemOperations fsOperations;
-  private final RemoteDirectoryManager remoteDirectoryManager;
-  private final RunJobParameters parameters;
-  private final LaunchCommandFactory launchCommandFactory;
-  private final Configuration yarnConfig;
-
-  WorkerComponentFactory(FileSystemOperations fsOperations,
-      RemoteDirectoryManager remoteDirectoryManager,
-      RunJobParameters parameters,
-      LaunchCommandFactory launchCommandFactory,
-      Configuration yarnConfig) {
-    this.fsOperations = fsOperations;
-    this.remoteDirectoryManager = remoteDirectoryManager;
-    this.parameters = parameters;
-    this.launchCommandFactory = launchCommandFactory;
-    this.yarnConfig = yarnConfig;
-  }
-
-  /**
-   * Creates either a TensorFlow or a PyTorch Native Service component.
-   */
-  public AbstractComponent create(Framework framework, Role role) {
-    if (framework == Framework.TENSORFLOW) {
-      return new TensorFlowWorkerComponent(fsOperations, remoteDirectoryManager,
-          (TensorFlowRunJobParameters) parameters, role,
-          (TensorFlowLaunchCommandFactory) launchCommandFactory, yarnConfig);
-    } else if (framework == Framework.PYTORCH) {
-      return new PyTorchWorkerComponent(fsOperations, remoteDirectoryManager,
-          (PyTorchRunJobParameters) parameters, role,
-          (PyTorchLaunchCommandFactory) launchCommandFactory, yarnConfig);
-    } else {
-      throw new UnsupportedOperationException("Only supported frameworks are: "
-          + Framework.getValues());
-    }
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceJobMonitor.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceJobMonitor.java
deleted file mode 100644
index 58b1e076fb27..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceJobMonitor.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import org.apache.hadoop.yarn.client.api.AppAdminClient;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.service.utils.ServiceApiUtil;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.JobStatus;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobMonitor;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.builder.JobStatusBuilder;
-
-import java.io.IOException;
-
-public class YarnServiceJobMonitor extends JobMonitor {
-  private volatile AppAdminClient serviceClient = null;
-
-  public YarnServiceJobMonitor(ClientContext clientContext) {
-    super(clientContext);
-  }
-
-  @Override
-  public JobStatus getTrainingJobStatus(String jobName)
-      throws IOException, YarnException {
-    if (this.serviceClient == null) {
-      synchronized(this) {
-        if (this.serviceClient == null) {
-          this.serviceClient = YarnServiceUtils.createServiceClient(
-              clientContext.getYarnConfig());
-        }
-      }
-    }
-    String appStatus=serviceClient.getStatusString(jobName);
-    Service serviceSpec= ServiceApiUtil.jsonSerDeser.fromJson(appStatus);
-    JobStatus jobStatus = JobStatusBuilder.fromServiceSpec(serviceSpec);
-    return jobStatus;
-  }
-
-  @Override
-  public void cleanup() throws IOException{
-    if (this.serviceClient != null) {
-      this.serviceClient.close();
-    }
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceJobSubmitter.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceJobSubmitter.java
deleted file mode 100644
index 9b1475e4ee44..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceJobSubmitter.java
+++ /dev/null
@@ -1,175 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import com.google.common.annotations.VisibleForTesting;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.client.api.AppAdminClient;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.service.utils.ServiceApiUtil;
-import org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.Framework;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.PyTorchLaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.TensorFlowLaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.pytorch.PyTorchServiceSpec;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowServiceSpec;
-import org.apache.hadoop.yarn.submarine.utils.Localizer;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-
-import static org.apache.hadoop.yarn.service.exceptions.LauncherExitCodes.EXIT_SUCCESS;
-import static org.apache.hadoop.yarn.submarine.client.cli.param.ParametersHolder.SUPPORTED_FRAMEWORKS_MESSAGE;
-
-/**
- * Submit a job to cluster.
- */
-public class YarnServiceJobSubmitter implements JobSubmitter {
-
-  private static final Logger LOG =
-      LoggerFactory.getLogger(YarnServiceJobSubmitter.class);
-  private ClientContext clientContext;
-  private ServiceWrapper serviceWrapper;
-
-  YarnServiceJobSubmitter(ClientContext clientContext) {
-    this.clientContext = clientContext;
-  }
-
-  /**
-   * {@inheritDoc}
-   */
-  @Override
-  public ApplicationId submitJob(ParametersHolder paramsHolder)
-      throws IOException, YarnException {
-    Framework framework = paramsHolder.getFramework();
-    RunJobParameters parameters =
-        (RunJobParameters) paramsHolder.getParameters();
-
-    if (framework == Framework.TENSORFLOW) {
-      return submitTensorFlowJob((TensorFlowRunJobParameters) parameters);
-    } else if (framework == Framework.PYTORCH) {
-      return submitPyTorchJob((PyTorchRunJobParameters) parameters);
-    } else {
-      throw new UnsupportedOperationException(SUPPORTED_FRAMEWORKS_MESSAGE);
-    }
-  }
-
-  private ApplicationId submitTensorFlowJob(
-      TensorFlowRunJobParameters parameters) throws IOException, YarnException {
-    FileSystemOperations fsOperations = new FileSystemOperations(clientContext);
-    HadoopEnvironmentSetup hadoopEnvSetup =
-        new HadoopEnvironmentSetup(clientContext, fsOperations);
-
-    Service serviceSpec = createTensorFlowServiceSpec(parameters,
-        fsOperations, hadoopEnvSetup);
-    return submitJobInternal(serviceSpec);
-  }
-
-  private ApplicationId submitPyTorchJob(PyTorchRunJobParameters parameters)
-      throws IOException, YarnException {
-    FileSystemOperations fsOperations = new FileSystemOperations(clientContext);
-    HadoopEnvironmentSetup hadoopEnvSetup =
-        new HadoopEnvironmentSetup(clientContext, fsOperations);
-
-    Service serviceSpec = createPyTorchServiceSpec(parameters,
-        fsOperations, hadoopEnvSetup);
-    return submitJobInternal(serviceSpec);
-  }
-
-  private ApplicationId submitJobInternal(Service serviceSpec)
-      throws IOException, YarnException {
-    String serviceSpecFile = ServiceSpecFileGenerator.generateJson(serviceSpec);
-
-    AppAdminClient appAdminClient =
-        YarnServiceUtils.createServiceClient(clientContext.getYarnConfig());
-    int code = appAdminClient.actionLaunch(serviceSpecFile,
-        serviceSpec.getName(), null, null);
-    if (code != EXIT_SUCCESS) {
-      throw new YarnException(
-          "Fail to launch application with exit code:" + code);
-    }
-
-    String appStatus = appAdminClient.getStatusString(serviceSpec.getName());
-    Service app = ServiceApiUtil.jsonSerDeser.fromJson(appStatus);
-
-    // Retry multiple times if applicationId is null
-    int maxRetryTimes = 30;
-    int count = 0;
-    while (app.getId() == null && count < maxRetryTimes) {
-      LOG.info("Waiting for application Id. AppStatusString=\n {}", appStatus);
-      try {
-        Thread.sleep(1000);
-      } catch (InterruptedException e) {
-        throw new IOException(e);
-      }
-      appStatus = appAdminClient.getStatusString(serviceSpec.getName());
-      app = ServiceApiUtil.jsonSerDeser.fromJson(appStatus);
-      count++;
-    }
-    // Retry timeout
-    if (app.getId() == null) {
-      throw new YarnException(
-          "Can't get application id for Service " + serviceSpec.getName());
-    }
-    ApplicationId appid = ApplicationId.fromString(app.getId());
-    appAdminClient.stop();
-    return appid;
-  }
-
-  private Service createTensorFlowServiceSpec(
-      TensorFlowRunJobParameters parameters,
-      FileSystemOperations fsOperations, HadoopEnvironmentSetup hadoopEnvSetup)
-      throws IOException {
-    TensorFlowLaunchCommandFactory launchCommandFactory =
-        new TensorFlowLaunchCommandFactory(hadoopEnvSetup, parameters,
-            clientContext.getYarnConfig());
-    Localizer localizer = new Localizer(fsOperations,
-        clientContext.getRemoteDirectoryManager(), parameters);
-    TensorFlowServiceSpec tensorFlowServiceSpec = new TensorFlowServiceSpec(
-        parameters, this.clientContext, fsOperations, launchCommandFactory,
-        localizer);
-
-    serviceWrapper = tensorFlowServiceSpec.create();
-    return serviceWrapper.getService();
-  }
-
-  private Service createPyTorchServiceSpec(PyTorchRunJobParameters parameters,
-      FileSystemOperations fsOperations, HadoopEnvironmentSetup hadoopEnvSetup)
-      throws IOException {
-    PyTorchLaunchCommandFactory launchCommandFactory =
-        new PyTorchLaunchCommandFactory(hadoopEnvSetup, parameters,
-            clientContext.getYarnConfig());
-    Localizer localizer = new Localizer(fsOperations,
-        clientContext.getRemoteDirectoryManager(), parameters);
-    PyTorchServiceSpec pyTorchServiceSpec = new PyTorchServiceSpec(
-        parameters, this.clientContext, fsOperations, launchCommandFactory,
-        localizer);
-
-    serviceWrapper = pyTorchServiceSpec.create();
-    return serviceWrapper.getService();
-  }
-
-  @VisibleForTesting
-  public ServiceWrapper getServiceWrapper() {
-    return serviceWrapper;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceRuntimeFactory.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceRuntimeFactory.java
deleted file mode 100644
index 3489e49233f3..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceRuntimeFactory.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.runtimes.RuntimeFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.common.FSBasedSubmarineStorageImpl;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobMonitor;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.common.SubmarineStorage;
-
-public class YarnServiceRuntimeFactory extends RuntimeFactory {
-
-  public YarnServiceRuntimeFactory(ClientContext clientContext) {
-    super(clientContext);
-  }
-
-  @Override
-  protected JobSubmitter internalCreateJobSubmitter() {
-    return new YarnServiceJobSubmitter(super.clientContext);
-  }
-
-  @Override
-  protected JobMonitor internalCreateJobMonitor() {
-    return new YarnServiceJobMonitor(super.clientContext);
-  }
-
-  @Override
-  protected SubmarineStorage internalCreateSubmarineStorage() {
-    return new FSBasedSubmarineStorageImpl(super.clientContext);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceUtils.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceUtils.java
deleted file mode 100644
index 352fd79dedc6..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceUtils.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import com.google.common.annotations.VisibleForTesting;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.client.api.AppAdminClient;
-
-import static org.apache.hadoop.yarn.client.api.AppAdminClient.DEFAULT_TYPE;
-
-/**
- * This class contains some static helper methods to query DNS data
- * based on the provided parameters.
- */
-public final class YarnServiceUtils {
-  private YarnServiceUtils() {
-  }
-
-  // This will be true only in UT.
-  private static AppAdminClient stubServiceClient = null;
-
-  static AppAdminClient createServiceClient(
-      Configuration yarnConfiguration) {
-    if (stubServiceClient != null) {
-      return stubServiceClient;
-    }
-
-    return AppAdminClient.createAppAdminClient(DEFAULT_TYPE, yarnConfiguration);
-  }
-
-  @VisibleForTesting
-  public static void setStubServiceClient(AppAdminClient stubServiceClient) {
-    YarnServiceUtils.stubServiceClient = stubServiceClient;
-  }
-
-  public static String getDNSName(String serviceName,
-      String componentInstanceName, String userName, String domain, int port) {
-    return componentInstanceName + getDNSNameCommonSuffix(serviceName, userName,
-        domain, port);
-  }
-
-  public static String getDNSNameCommonSuffix(String serviceName,
-      String userName, String domain, int port) {
-    return "." + serviceName + "." + userName + "." + domain + ":" + port;
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/builder/JobComponentStatusBuilder.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/builder/JobComponentStatusBuilder.java
deleted file mode 100644
index 351fd2c43311..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/builder/JobComponentStatusBuilder.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.builder;
-
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.Container;
-import org.apache.hadoop.yarn.service.api.records.ContainerState;
-import org.apache.hadoop.yarn.submarine.common.api.JobComponentStatus;
-
-public class JobComponentStatusBuilder {
-  public static JobComponentStatus fromServiceComponent(Component component) {
-    long totalAskedContainers = component.getNumberOfContainers();
-    int numReadyContainers = 0;
-    int numRunningButUnreadyContainers = 0;
-    String compName = component.getName();
-
-    for (Container c : component.getContainers()) {
-      if (c.getState() == ContainerState.READY) {
-        numReadyContainers++;
-      } else if (c.getState() == ContainerState.RUNNING_BUT_UNREADY) {
-        numRunningButUnreadyContainers++;
-      }
-    }
-
-    return new JobComponentStatus(compName, numReadyContainers,
-        numRunningButUnreadyContainers, totalAskedContainers);
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/builder/JobStatusBuilder.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/builder/JobStatusBuilder.java
deleted file mode 100644
index 724f164e4127..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/builder/JobStatusBuilder.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.builder;
-
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.service.api.records.ServiceState;
-import org.apache.hadoop.yarn.submarine.common.api.JobComponentStatus;
-import org.apache.hadoop.yarn.submarine.common.api.JobState;
-import org.apache.hadoop.yarn.submarine.common.api.JobStatus;
-
-import java.util.ArrayList;
-import java.util.List;
-
-public class JobStatusBuilder {
-  public static JobStatus fromServiceSpec(Service serviceSpec) {
-    JobStatus status = new JobStatus();
-    status.setState(fromServiceState(serviceSpec.getState()));
-
-    // If it is a final state, return.
-    if (JobState.isFinal(status.getState())) {
-      return status;
-    }
-
-    List<JobComponentStatus> componentStatusList = new ArrayList<>();
-
-    for (Component component : serviceSpec.getComponents()) {
-      componentStatusList.add(
-          JobComponentStatusBuilder.fromServiceComponent(component));
-    }
-    status.setComponentStatus(componentStatusList);
-
-    // TODO, handle tensorboard differently.
-    // status.setTensorboardLink(getTensorboardLink(serviceSpec, clientContext));
-
-    status.setJobName(serviceSpec.getName());
-
-    return status;
-  }
-
-  private static JobState fromServiceState(ServiceState serviceState) {
-    switch (serviceState) {
-    case STOPPED:
-      // TODO, once YARN-8488 gets committed, we need to update this.
-      return JobState.SUCCEEDED;
-    case FAILED:
-      return JobState.FAILED;
-    }
-
-    return JobState.RUNNING;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/AbstractLaunchCommand.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/AbstractLaunchCommand.java
deleted file mode 100644
index 4ede8fbbbe07..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/AbstractLaunchCommand.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command;
-
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import java.io.IOException;
-
-/**
- * Abstract base class for Launch command implementations for Services.
- * Currently we have launch command implementations
- * for TensorFlow PS, worker and Tensorboard instances.
- */
-public abstract class AbstractLaunchCommand {
-  private final LaunchScriptBuilder builder;
-
-  public AbstractLaunchCommand(HadoopEnvironmentSetup hadoopEnvSetup,
-      Component component, RunJobParameters parameters,
-      String launchCommandPrefix) throws IOException {
-    this.builder = new LaunchScriptBuilder(launchCommandPrefix, hadoopEnvSetup,
-        parameters, component);
-  }
-
-  protected LaunchScriptBuilder getBuilder() {
-    return builder;
-  }
-
-  /**
-   * Subclasses need to defined this method and return a valid launch script.
-   * Implementors can utilize the {@link LaunchScriptBuilder} using
-   * the getBuilder method of this class.
-   * @return The contents of a script.
-   * @throws IOException If any IO issue happens.
-   */
-  public abstract String generateLaunchScript() throws IOException;
-
-  /**
-   * Subclasses need to provide a service-specific launch command
-   * of the service.
-   * Please note that this method should only return the launch command
-   * but not the whole script.
-   * @return The launch command
-   */
-  public abstract String createLaunchCommand();
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/LaunchCommandFactory.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/LaunchCommandFactory.java
deleted file mode 100644
index 4dfa21716e24..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/LaunchCommandFactory.java
+++ /dev/null
@@ -1,30 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command;
-
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-
-import java.io.IOException;
-
-/**
- * Interface for creating launch commands.
- */
-public interface LaunchCommandFactory {
-  AbstractLaunchCommand createLaunchCommand(Role role, Component component)
-      throws IOException;
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/LaunchScriptBuilder.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/LaunchScriptBuilder.java
deleted file mode 100644
index 7f12a1a621b5..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/LaunchScriptBuilder.java
+++ /dev/null
@@ -1,108 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command;
-
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.OutputStreamWriter;
-import java.io.PrintWriter;
-
-import static java.nio.charset.StandardCharsets.UTF_8;
-
-/**
- * This class is a builder to conveniently create launch scripts.
- * All dependencies are provided with the constructor except
- * the launch command.
- */
-public class LaunchScriptBuilder {
-  private static final Logger LOG = LoggerFactory.getLogger(
-      LaunchScriptBuilder.class);
-
-  private final File file;
-  private final HadoopEnvironmentSetup hadoopEnvSetup;
-  private final RunJobParameters parameters;
-  private final Component component;
-  private final OutputStreamWriter writer;
-  private final StringBuilder scriptBuffer;
-  private String launchCommand;
-
-  LaunchScriptBuilder(String launchScriptPrefix,
-      HadoopEnvironmentSetup hadoopEnvSetup, RunJobParameters parameters,
-      Component component) throws IOException {
-    this.file = File.createTempFile(launchScriptPrefix +
-        "-launch-script", ".sh");
-    this.hadoopEnvSetup = hadoopEnvSetup;
-    this.parameters = parameters;
-    this.component = component;
-    this.writer = new OutputStreamWriter(new FileOutputStream(file), UTF_8);
-    this.scriptBuffer = new StringBuilder();
-  }
-
-  public void append(String s) {
-    scriptBuffer.append(s);
-  }
-
-  public LaunchScriptBuilder withLaunchCommand(String command) {
-    this.launchCommand = command;
-    return this;
-  }
-
-  public String build() throws IOException {
-    if (launchCommand != null) {
-      append(launchCommand);
-    } else {
-      LOG.warn("LaunchScript object was null!");
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("LaunchScript's Builder object: {}", this);
-      }
-    }
-
-    try (PrintWriter pw = new PrintWriter(writer)) {
-      writeBashHeader(pw);
-      hadoopEnvSetup.addHdfsClassPath(parameters, pw, component);
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("Appending command to launch script: {}", scriptBuffer);
-      }
-      pw.append(scriptBuffer);
-    }
-    return file.getAbsolutePath();
-  }
-
-  @Override
-  public String toString() {
-    return "LaunchScriptBuilder{" +
-        "file=" + file +
-        ", hadoopEnvSetup=" + hadoopEnvSetup +
-        ", parameters=" + parameters +
-        ", component=" + component +
-        ", writer=" + writer +
-        ", scriptBuffer=" + scriptBuffer +
-        ", launchCommand='" + launchCommand + '\'' +
-        '}';
-  }
-
-  private void writeBashHeader(PrintWriter pw) {
-    pw.append("#!/bin/bash\n");
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/PyTorchLaunchCommandFactory.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/PyTorchLaunchCommandFactory.java
deleted file mode 100644
index dbb82bdd6ded..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/PyTorchLaunchCommandFactory.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command;
-
-import java.io.IOException;
-import java.util.Objects;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.PyTorchRole;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.pytorch.command.PyTorchWorkerLaunchCommand;
-
-/**
- * Simple factory to create instances of {@link AbstractLaunchCommand}
- * based on the {@link Role}.
- * All dependencies are passed to this factory that could be required
- * by any implementor of {@link AbstractLaunchCommand}.
- */
-public class PyTorchLaunchCommandFactory implements LaunchCommandFactory {
-  private final HadoopEnvironmentSetup hadoopEnvSetup;
-  private final PyTorchRunJobParameters parameters;
-  private final Configuration yarnConfig;
-
-  public PyTorchLaunchCommandFactory(HadoopEnvironmentSetup hadoopEnvSetup,
-      PyTorchRunJobParameters parameters, Configuration yarnConfig) {
-    this.hadoopEnvSetup = hadoopEnvSetup;
-    this.parameters = parameters;
-    this.yarnConfig = yarnConfig;
-  }
-
-  public AbstractLaunchCommand createLaunchCommand(Role role,
-      Component component) throws IOException {
-    Objects.requireNonNull(role, "Role must not be null!");
-
-    if (role == PyTorchRole.WORKER ||
-        role == PyTorchRole.PRIMARY_WORKER) {
-      return new PyTorchWorkerLaunchCommand(hadoopEnvSetup, role,
-          component, parameters, yarnConfig);
-
-    } else {
-      throw new IllegalStateException("Unknown task type: " + role);
-    }
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/TensorFlowLaunchCommandFactory.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/TensorFlowLaunchCommandFactory.java
deleted file mode 100644
index 1ae6643f23b2..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/TensorFlowLaunchCommandFactory.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command.TensorBoardLaunchCommand;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command.TensorFlowPsLaunchCommand;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command.TensorFlowWorkerLaunchCommand;
-
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- * Simple factory to create instances of {@link AbstractLaunchCommand}
- * based on the {@link Role}.
- * All dependencies are passed to this factory that could be required
- * by any implementor of {@link AbstractLaunchCommand}.
- */
-public class TensorFlowLaunchCommandFactory implements LaunchCommandFactory {
-  private final HadoopEnvironmentSetup hadoopEnvSetup;
-  private final TensorFlowRunJobParameters parameters;
-  private final Configuration yarnConfig;
-
-  public TensorFlowLaunchCommandFactory(HadoopEnvironmentSetup hadoopEnvSetup,
-      TensorFlowRunJobParameters parameters, Configuration yarnConfig) {
-    this.hadoopEnvSetup = hadoopEnvSetup;
-    this.parameters = parameters;
-    this.yarnConfig = yarnConfig;
-  }
-
-  @Override
-  public AbstractLaunchCommand createLaunchCommand(Role role,
-      Component component) throws IOException {
-    Objects.requireNonNull(role, "Role must not be null!");
-
-    if (role == TensorFlowRole.WORKER ||
-        role == TensorFlowRole.PRIMARY_WORKER) {
-      return new TensorFlowWorkerLaunchCommand(hadoopEnvSetup, role,
-          component, parameters, yarnConfig);
-
-    } else if (role == TensorFlowRole.PS) {
-      return new TensorFlowPsLaunchCommand(hadoopEnvSetup, role, component,
-          parameters, yarnConfig);
-
-    } else if (role == TensorFlowRole.TENSORBOARD) {
-      return new TensorBoardLaunchCommand(hadoopEnvSetup, role, component,
-          parameters);
-    }
-    throw new IllegalStateException("Unknown task type: " + role);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/package-info.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/package-info.java
deleted file mode 100644
index a2572047e3ae..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/package-info.java
+++ /dev/null
@@ -1,19 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains classes to produce launch commands and scripts.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/PyTorchServiceSpec.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/PyTorchServiceSpec.java
deleted file mode 100644
index ad260026c894..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/PyTorchServiceSpec.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.pytorch;
-
-import java.io.IOException;
-
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.Framework;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.AbstractServiceSpec;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.ServiceWrapper;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.PyTorchLaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.utils.Localizer;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * This class contains all the logic to create an instance
- * of a {@link Service} object for PyTorch.
- * Please note that currently, only single-node (non-distributed)
- * support is implemented for PyTorch.
- */
-public class PyTorchServiceSpec extends AbstractServiceSpec {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(PyTorchServiceSpec.class);
-  //this field is needed in the future!
-  private final PyTorchRunJobParameters pyTorchParameters;
-
-  public PyTorchServiceSpec(PyTorchRunJobParameters parameters,
-      ClientContext clientContext, FileSystemOperations fsOperations,
-      PyTorchLaunchCommandFactory launchCommandFactory, Localizer localizer) {
-    super(parameters, clientContext, fsOperations, launchCommandFactory,
-        localizer);
-    this.pyTorchParameters = parameters;
-  }
-
-  @Override
-  public ServiceWrapper create() throws IOException {
-    LOG.info("Creating PyTorch service spec");
-    ServiceWrapper serviceWrapper = createServiceSpecWrapper();
-
-    if (parameters.getNumWorkers() > 0) {
-      addWorkerComponents(serviceWrapper, Framework.PYTORCH);
-    }
-
-    // After all components added, handle quicklinks
-    handleQuicklinks(serviceWrapper.getService());
-
-    return serviceWrapper;
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/command/PyTorchWorkerLaunchCommand.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/command/PyTorchWorkerLaunchCommand.java
deleted file mode 100644
index beda5f2a3555..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/command/PyTorchWorkerLaunchCommand.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.pytorch.command;
-
-import java.io.IOException;
-
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.AbstractLaunchCommand;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.LaunchScriptBuilder;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Launch command implementation for PyTorch components.
- */
-public class PyTorchWorkerLaunchCommand extends AbstractLaunchCommand {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(PyTorchWorkerLaunchCommand.class);
-  private final Configuration yarnConfig;
-  private final boolean distributed;
-  private final int numberOfWorkers;
-  private final String name;
-  private final Role role;
-  private final String launchCommand;
-
-  public PyTorchWorkerLaunchCommand(HadoopEnvironmentSetup hadoopEnvSetup,
-      Role role, Component component,
-      PyTorchRunJobParameters parameters,
-      Configuration yarnConfig) throws IOException {
-    super(hadoopEnvSetup, component, parameters, role.getName());
-    this.role = role;
-    this.name = parameters.getName();
-    this.distributed = parameters.isDistributed();
-    this.numberOfWorkers = parameters.getNumWorkers();
-    this.yarnConfig = yarnConfig;
-    logReceivedParameters();
-
-    this.launchCommand = parameters.getWorkerLaunchCmd();
-
-    if (StringUtils.isEmpty(this.launchCommand)) {
-      throw new IllegalArgumentException("LaunchCommand must not be null " +
-          "or empty!");
-    }
-  }
-
-  private void logReceivedParameters() {
-    if (this.numberOfWorkers <= 0) {
-      LOG.warn("Received number of workers: {}", this.numberOfWorkers);
-    }
-  }
-
-  @Override
-  public String generateLaunchScript() throws IOException {
-    LaunchScriptBuilder builder = getBuilder();
-    return builder
-        .withLaunchCommand(createLaunchCommand())
-        .build();
-  }
-
-  @Override
-  public String createLaunchCommand() {
-    if (SubmarineLogs.isVerbose()) {
-      LOG.info("PyTorch Worker command =[" + launchCommand + "]");
-    }
-    return launchCommand + '\n';
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/command/package-info.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/command/package-info.java
deleted file mode 100644
index 840eb9b1b2a5..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/command/package-info.java
+++ /dev/null
@@ -1,19 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains classes to generate PyTorch launch commands.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.pytorch.command;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/component/PyTorchWorkerComponent.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/component/PyTorchWorkerComponent.java
deleted file mode 100644
index d7b90e203651..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/component/PyTorchWorkerComponent.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.pytorch.component;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.AbstractComponent;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.PyTorchLaunchCommandFactory;
-
-import java.io.IOException;
-
-/**
- * Component implementation for Worker process of PyTorch.
- */
-public class PyTorchWorkerComponent extends AbstractComponent {
-  public PyTorchWorkerComponent(FileSystemOperations fsOperations,
-      RemoteDirectoryManager remoteDirectoryManager,
-      PyTorchRunJobParameters parameters, Role role,
-      PyTorchLaunchCommandFactory launchCommandFactory,
-      Configuration yarnConfig) {
-    super(fsOperations, remoteDirectoryManager, parameters, role,
-        yarnConfig, launchCommandFactory);
-  }
-
-  @Override
-  public Component createComponent() throws IOException {
-    return createComponentInternal();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/component/package-info.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/component/package-info.java
deleted file mode 100644
index 4fa079244e2a..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/component/package-info.java
+++ /dev/null
@@ -1,20 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains classes to generate
- * PyTorch Native Service components.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.pytorch.component;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/package-info.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/package-info.java
deleted file mode 100644
index 1f3e0faac608..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/package-info.java
+++ /dev/null
@@ -1,20 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains classes to generate
- * PyTorch-related Native Service runtime artifacts.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.pytorch;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/TensorFlowCommons.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/TensorFlowCommons.java
deleted file mode 100644
index 0cb5b6b170ee..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/TensorFlowCommons.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.ServiceApiConstants;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.common.Envs;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceUtils;
-
-import java.util.Map;
-
-/**
- * This class has common helper methods for TensorFlow.
- */
-public final class TensorFlowCommons {
-  private TensorFlowCommons() {
-    throw new UnsupportedOperationException("This class should not be " +
-        "instantiated!");
-  }
-
-  public static void addCommonEnvironments(Component component,
-      Role role) {
-    Map<String, String> envs = component.getConfiguration().getEnv();
-    envs.put(Envs.TASK_INDEX_ENV, ServiceApiConstants.COMPONENT_ID);
-    envs.put(Envs.TASK_TYPE_ENV, role.getName());
-  }
-
-  public static String getUserName() {
-    return System.getProperty("user.name");
-  }
-
-  public static String getDNSDomain(Configuration yarnConfig) {
-    return yarnConfig.get("hadoop.registry.dns.domain-name");
-  }
-
-  public static String getScriptFileName(Role role) {
-    return "run-" + role.getName() + ".sh";
-  }
-
-  public static String getTFConfigEnv(String componentName, int nWorkers,
-      int nPs, String serviceName, String userName, String domain) {
-    String commonEndpointSuffix = YarnServiceUtils
-        .getDNSNameCommonSuffix(serviceName, userName, domain, 8000);
-
-    String json = "{\\\"cluster\\\":{";
-
-    String master = getComponentArrayJson("master", 1, commonEndpointSuffix)
-        + ",";
-    String worker = getComponentArrayJson("worker", nWorkers - 1,
-        commonEndpointSuffix) + ",";
-    String ps = getComponentArrayJson("ps", nPs, commonEndpointSuffix) + "},";
-
-    StringBuilder sb = new StringBuilder();
-    sb.append("\\\"task\\\":{");
-    sb.append(" \\\"type\\\":\\\"");
-    sb.append(componentName);
-    sb.append("\\\",");
-    sb.append(" \\\"index\\\":");
-    sb.append('$');
-    sb.append(Envs.TASK_INDEX_ENV + "},");
-    String task = sb.toString();
-    String environment = "\\\"environment\\\":\\\"cloud\\\"}";
-
-    sb = new StringBuilder();
-    sb.append(json);
-    sb.append(master);
-    sb.append(worker);
-    sb.append(ps);
-    sb.append(task);
-    sb.append(environment);
-    return sb.toString();
-  }
-
-  private static String getComponentArrayJson(String componentName, int count,
-      String endpointSuffix) {
-    String component = "\\\"" + componentName + "\\\":";
-    StringBuilder array = new StringBuilder();
-    array.append("[");
-    for (int i = 0; i < count; i++) {
-      array.append("\\\"");
-      array.append(componentName);
-      array.append("-");
-      array.append(i);
-      array.append(endpointSuffix);
-      array.append("\\\"");
-      if (i != count - 1) {
-        array.append(",");
-      }
-    }
-    array.append("]");
-    return component + array.toString();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/TensorFlowServiceSpec.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/TensorFlowServiceSpec.java
deleted file mode 100644
index bacd50238888..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/TensorFlowServiceSpec.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow;
-
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.Framework;
-import org.apache.hadoop.yarn.submarine.common.ClientContext;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.AbstractServiceSpec;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.ServiceWrapper;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.TensorFlowLaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component.TensorBoardComponent;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component.TensorFlowPsComponent;
-import org.apache.hadoop.yarn.submarine.utils.Localizer;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component.TensorBoardComponent.TENSORBOARD_QUICKLINK_LABEL;
-
-/**
- * This class contains all the logic to create an instance
- * of a {@link Service} object for TensorFlow.
- * Worker,PS and Tensorboard components are added to the Service
- * based on the value of the received {@link RunJobParameters}.
- */
-public class TensorFlowServiceSpec extends AbstractServiceSpec {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TensorFlowServiceSpec.class);
-  private final TensorFlowRunJobParameters tensorFlowParameters;
-
-  public TensorFlowServiceSpec(TensorFlowRunJobParameters parameters,
-      ClientContext clientContext, FileSystemOperations fsOperations,
-      TensorFlowLaunchCommandFactory launchCommandFactory,
-      Localizer localizer) {
-    super(parameters, clientContext, fsOperations, launchCommandFactory,
-        localizer);
-    this.tensorFlowParameters = parameters;
-  }
-
-  @Override
-  public ServiceWrapper create() throws IOException {
-    LOG.info("Creating TensorFlow service spec");
-    ServiceWrapper serviceWrapper = createServiceSpecWrapper();
-
-    if (tensorFlowParameters.getNumWorkers() > 0) {
-      addWorkerComponents(serviceWrapper, Framework.TENSORFLOW);
-    }
-
-    if (tensorFlowParameters.getNumPS() > 0) {
-      addPsComponent(serviceWrapper);
-    }
-
-    if (tensorFlowParameters.isTensorboardEnabled()) {
-      createTensorBoardComponent(serviceWrapper);
-    }
-
-    // After all components added, handle quicklinks
-    handleQuicklinks(serviceWrapper.getService());
-
-    return serviceWrapper;
-  }
-
-  private void createTensorBoardComponent(ServiceWrapper serviceWrapper)
-      throws IOException {
-    TensorBoardComponent tbComponent = new TensorBoardComponent(fsOperations,
-        remoteDirectoryManager, parameters,
-        (TensorFlowLaunchCommandFactory) launchCommandFactory, yarnConfig);
-    serviceWrapper.addComponent(tbComponent);
-
-    addQuicklink(serviceWrapper.getService(), TENSORBOARD_QUICKLINK_LABEL,
-        tbComponent.getTensorboardLink());
-  }
-
-  private void addPsComponent(ServiceWrapper serviceWrapper)
-      throws IOException {
-    serviceWrapper.addComponent(
-        new TensorFlowPsComponent(fsOperations, remoteDirectoryManager,
-            (TensorFlowLaunchCommandFactory) launchCommandFactory,
-            parameters, yarnConfig));
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorBoardLaunchCommand.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorBoardLaunchCommand.java
deleted file mode 100644
index 31b0e20c9f30..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorBoardLaunchCommand.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command;
-
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.AbstractLaunchCommand;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- * Launch command implementation for Tensorboard.
- */
-public class TensorBoardLaunchCommand extends AbstractLaunchCommand {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TensorBoardLaunchCommand.class);
-  private final String checkpointPath;
-
-  public TensorBoardLaunchCommand(HadoopEnvironmentSetup hadoopEnvSetup,
-      Role role, Component component, RunJobParameters parameters)
-      throws IOException {
-    super(hadoopEnvSetup, component, parameters, role.getName());
-    Objects.requireNonNull(parameters.getCheckpointPath(),
-        "CheckpointPath must not be null as it is part "
-            + "of the tensorboard command!");
-    if (StringUtils.isEmpty(parameters.getCheckpointPath())) {
-      throw new IllegalArgumentException("CheckpointPath must not be empty!");
-    }
-
-    this.checkpointPath = parameters.getCheckpointPath();
-  }
-
-  @Override
-  public String generateLaunchScript() throws IOException {
-    return getBuilder()
-        .withLaunchCommand(createLaunchCommand())
-        .build();
-  }
-
-  @Override
-  public String createLaunchCommand() {
-    String tbCommand = String.format("export LC_ALL=C && tensorboard " +
-        "--logdir=%s%n", checkpointPath);
-    LOG.info("Tensorboard command=" + tbCommand);
-    return tbCommand;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowLaunchCommand.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowLaunchCommand.java
deleted file mode 100644
index f5e2f6575c53..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowLaunchCommand.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.AbstractLaunchCommand;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.LaunchScriptBuilder;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowCommons;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- * Launch command implementation for
- * TensorFlow PS and Worker Service components.
- */
-public abstract class TensorFlowLaunchCommand extends AbstractLaunchCommand {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TensorFlowLaunchCommand.class);
-  private final Configuration yarnConfig;
-  private final boolean distributed;
-  private final int numberOfWorkers;
-  private final int numberOfPS;
-  private final String name;
-  private final Role role;
-
-  TensorFlowLaunchCommand(HadoopEnvironmentSetup hadoopEnvSetup,
-      Role role, Component component,
-      TensorFlowRunJobParameters parameters,
-      Configuration yarnConfig) throws IOException {
-    super(hadoopEnvSetup, component, parameters,
-        role != null ? role.getName(): "");
-    Objects.requireNonNull(role, "TensorFlowRole must not be null!");
-    this.role = role;
-    this.name = parameters.getName();
-    this.distributed = parameters.isDistributed();
-    this.numberOfWorkers = parameters.getNumWorkers();
-    this.numberOfPS = parameters.getNumPS();
-    this.yarnConfig = yarnConfig;
-    logReceivedParameters();
-  }
-
-  private void logReceivedParameters() {
-    if (this.numberOfWorkers <= 0) {
-      LOG.warn("Received number of workers: {}", this.numberOfWorkers);
-    }
-    if (this.numberOfPS <= 0) {
-      LOG.warn("Received number of PS: {}", this.numberOfPS);
-    }
-  }
-
-  @Override
-  public String generateLaunchScript() throws IOException {
-    LaunchScriptBuilder builder = getBuilder();
-
-    // When distributed training is required
-    if (distributed) {
-      String tfConfigEnvValue = TensorFlowCommons.getTFConfigEnv(
-          role.getComponentName(), numberOfWorkers,
-          numberOfPS, name,
-          TensorFlowCommons.getUserName(),
-          TensorFlowCommons.getDNSDomain(yarnConfig));
-      String tfConfig = "export TF_CONFIG=\"" + tfConfigEnvValue + "\"\n";
-      builder.append(tfConfig);
-    }
-
-    return builder
-        .withLaunchCommand(createLaunchCommand())
-        .build();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowPsLaunchCommand.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowPsLaunchCommand.java
deleted file mode 100644
index edbdee4d0813..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowPsLaunchCommand.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command;
-
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-
-/**
- * Launch command implementation for Tensorboard's PS component.
- */
-public class TensorFlowPsLaunchCommand extends TensorFlowLaunchCommand {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TensorFlowPsLaunchCommand.class);
-  private final String launchCommand;
-
-  public TensorFlowPsLaunchCommand(HadoopEnvironmentSetup hadoopEnvSetup,
-      Role role, Component component,
-      TensorFlowRunJobParameters parameters,
-      Configuration yarnConfig) throws IOException {
-    super(hadoopEnvSetup, role, component, parameters, yarnConfig);
-    this.launchCommand = parameters.getPSLaunchCmd();
-
-    if (StringUtils.isEmpty(this.launchCommand)) {
-      throw new IllegalArgumentException("LaunchCommand must not be null " +
-          "or empty!");
-    }
-  }
-
-  @Override
-  public String createLaunchCommand() {
-    if (SubmarineLogs.isVerbose()) {
-      LOG.info("PS command =[" + launchCommand + "]");
-    }
-    return launchCommand + '\n';
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowWorkerLaunchCommand.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowWorkerLaunchCommand.java
deleted file mode 100644
index 8edc37b04690..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowWorkerLaunchCommand.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command;
-
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-
-/**
- * Launch command implementation for Tensorboard's Worker component.
- */
-public class TensorFlowWorkerLaunchCommand extends TensorFlowLaunchCommand {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TensorFlowWorkerLaunchCommand.class);
-  private final String launchCommand;
-
-  public TensorFlowWorkerLaunchCommand(
-      HadoopEnvironmentSetup hadoopEnvSetup, Role role,
-      Component component, TensorFlowRunJobParameters parameters,
-      Configuration yarnConfig) throws IOException {
-    super(hadoopEnvSetup, role, component, parameters, yarnConfig);
-    this.launchCommand = parameters.getWorkerLaunchCmd();
-
-    if (StringUtils.isEmpty(this.launchCommand)) {
-      throw new IllegalArgumentException("LaunchCommand must not be null " +
-          "or empty!");
-    }
-  }
-
-  @Override
-  public String createLaunchCommand() {
-    if (SubmarineLogs.isVerbose()) {
-      LOG.info("Worker command =[" + launchCommand + "]");
-    }
-    return launchCommand + '\n';
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/package-info.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/package-info.java
deleted file mode 100644
index f8df3bbf123f..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/package-info.java
+++ /dev/null
@@ -1,19 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains classes to generate TensorFlow launch commands.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorBoardComponent.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorBoardComponent.java
deleted file mode 100644
index 68ef76e72080..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorBoardComponent.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.Component.RestartPolicyEnum;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.AbstractComponent;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceUtils;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.TensorFlowLaunchCommandFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-import java.util.Objects;
-
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowCommons.addCommonEnvironments;
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowCommons.getDNSDomain;
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowCommons.getUserName;
-import static org.apache.hadoop.yarn.submarine.utils.DockerUtilities.getDockerArtifact;
-import static org.apache.hadoop.yarn.submarine.utils.SubmarineResourceUtils.convertYarnResourceToServiceResource;
-
-/**
- * Component implementation for Tensorboard's Tensorboard.
- */
-public class TensorBoardComponent extends AbstractComponent {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TensorBoardComponent.class);
-
-  public static final String TENSORBOARD_QUICKLINK_LABEL = "Tensorboard";
-  private static final int DEFAULT_PORT = 6006;
-
-  //computed fields
-  private String tensorboardLink;
-
-  public TensorBoardComponent(FileSystemOperations fsOperations,
-      RemoteDirectoryManager remoteDirectoryManager,
-      RunJobParameters parameters,
-      TensorFlowLaunchCommandFactory launchCommandFactory,
-      Configuration yarnConfig) {
-    super(fsOperations, remoteDirectoryManager, parameters,
-        TensorFlowRole.TENSORBOARD, yarnConfig, launchCommandFactory);
-  }
-
-  @Override
-  public Component createComponent() throws IOException {
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) this.parameters;
-
-    Objects.requireNonNull(tensorFlowParams.getTensorboardResource(),
-        "TensorBoard resource must not be null!");
-
-    Component component = new Component();
-    component.setName(role.getComponentName());
-    component.setNumberOfContainers(1L);
-    component.setRestartPolicy(RestartPolicyEnum.NEVER);
-    component.setResource(convertYarnResourceToServiceResource(
-        tensorFlowParams.getTensorboardResource()));
-
-    if (tensorFlowParams.getTensorboardDockerImage() != null) {
-      component.setArtifact(
-          getDockerArtifact(tensorFlowParams.getTensorboardDockerImage()));
-    }
-
-    addCommonEnvironments(component, role);
-    generateLaunchCommand(component);
-
-    tensorboardLink = "http://" + YarnServiceUtils.getDNSName(
-        parameters.getName(),
-        role.getComponentName() + "-" + 0, getUserName(),
-        getDNSDomain(yarnConfig), DEFAULT_PORT);
-    LOG.info("Link to tensorboard:" + tensorboardLink);
-
-    return component;
-  }
-
-  public String getTensorboardLink() {
-    return tensorboardLink;
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorFlowPsComponent.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorFlowPsComponent.java
deleted file mode 100644
index b463f58fda2b..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorFlowPsComponent.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.AbstractComponent;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.TensorFlowLaunchCommandFactory;
-
-import java.io.IOException;
-import java.util.Objects;
-
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowCommons.addCommonEnvironments;
-import static org.apache.hadoop.yarn.submarine.utils.DockerUtilities.getDockerArtifact;
-import static org.apache.hadoop.yarn.submarine.utils.SubmarineResourceUtils.convertYarnResourceToServiceResource;
-
-/**
- * Component implementation for TensorFlow's PS process.
- */
-public class TensorFlowPsComponent extends AbstractComponent {
-  public TensorFlowPsComponent(FileSystemOperations fsOperations,
-      RemoteDirectoryManager remoteDirectoryManager,
-      TensorFlowLaunchCommandFactory launchCommandFactory,
-      RunJobParameters parameters,
-      Configuration yarnConfig) {
-    super(fsOperations, remoteDirectoryManager, parameters,
-        TensorFlowRole.PS, yarnConfig, launchCommandFactory);
-  }
-
-  @Override
-  public Component createComponent() throws IOException {
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) this.parameters;
-
-    Objects.requireNonNull(tensorFlowParams.getPsResource(),
-        "PS resource must not be null!");
-    if (tensorFlowParams.getNumPS() < 1) {
-      throw new IllegalArgumentException("Number of PS should be at least 1!");
-    }
-
-    Component component = new Component();
-    component.setName(role.getComponentName());
-    component.setNumberOfContainers((long) tensorFlowParams.getNumPS());
-    component.setRestartPolicy(Component.RestartPolicyEnum.NEVER);
-    component.setResource(
-        convertYarnResourceToServiceResource(tensorFlowParams.getPsResource()));
-
-    // Override global docker image if needed.
-    if (tensorFlowParams.getPsDockerImage() != null) {
-      component.setArtifact(
-          getDockerArtifact(tensorFlowParams.getPsDockerImage()));
-    }
-    addCommonEnvironments(component, role);
-    generateLaunchCommand(component);
-
-    return component;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorFlowWorkerComponent.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorFlowWorkerComponent.java
deleted file mode 100644
index dffa584ea192..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorFlowWorkerComponent.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.AbstractComponent;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.TensorFlowLaunchCommandFactory;
-import java.io.IOException;
-
-/**
- * Component implementation for TensorFlow's Worker process.
- */
-public class TensorFlowWorkerComponent extends AbstractComponent {
-  public TensorFlowWorkerComponent(FileSystemOperations fsOperations,
-      RemoteDirectoryManager remoteDirectoryManager,
-      TensorFlowRunJobParameters parameters, Role role,
-      TensorFlowLaunchCommandFactory launchCommandFactory,
-      Configuration yarnConfig) {
-    super(fsOperations, remoteDirectoryManager, parameters, role,
-        yarnConfig, launchCommandFactory);
-  }
-
-  @Override
-  public Component createComponent() throws IOException {
-    return createComponentInternal();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/package-info.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/package-info.java
deleted file mode 100644
index 10978b717ed3..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/package-info.java
+++ /dev/null
@@ -1,20 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains classes to generate
- * TensorFlow Native Service components.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/package-info.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/package-info.java
deleted file mode 100644
index 0c5148584333..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/package-info.java
+++ /dev/null
@@ -1,20 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains classes to generate
- * TensorFlow-related Native Service runtime artifacts.
- */
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/ClassPathUtilities.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/ClassPathUtilities.java
deleted file mode 100644
index fc8f6ea62016..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/ClassPathUtilities.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import java.io.File;
-import java.util.StringTokenizer;
-
-/**
- * Utilities for classpath operations.
- */
-public final class ClassPathUtilities {
-  private ClassPathUtilities() {
-    throw new UnsupportedOperationException("This class should not be " +
-        "instantiated!");
-  }
-
-  public static File findFileOnClassPath(final String fileName) {
-    final String classpath = System.getProperty("java.class.path");
-    final String pathSeparator = System.getProperty("path.separator");
-    final StringTokenizer tokenizer = new StringTokenizer(classpath,
-        pathSeparator);
-
-    while (tokenizer.hasMoreTokens()) {
-      final String pathElement = tokenizer.nextToken();
-      final File directoryOrJar = new File(pathElement);
-      final File absoluteDirectoryOrJar = directoryOrJar.getAbsoluteFile();
-      if (absoluteDirectoryOrJar.isFile()) {
-        final File target =
-            new File(absoluteDirectoryOrJar.getParent(), fileName);
-        if (target.exists()) {
-          return target;
-        }
-      } else {
-        final File target = new File(directoryOrJar, fileName);
-        if (target.exists()) {
-          return target;
-        }
-      }
-    }
-
-    return null;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/DockerUtilities.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/DockerUtilities.java
deleted file mode 100644
index 78cee3396763..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/DockerUtilities.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import org.apache.hadoop.yarn.service.api.records.Artifact;
-
-/**
- * Utilities for Docker-related operations.
- */
-public final class DockerUtilities {
-  private DockerUtilities() {
-    throw new UnsupportedOperationException("This class should not be " +
-        "instantiated!");
-  }
-
-  public static Artifact getDockerArtifact(String dockerImageName) {
-    return new Artifact().type(Artifact.TypeEnum.DOCKER).id(dockerImageName);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/EnvironmentUtilities.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/EnvironmentUtilities.java
deleted file mode 100644
index f4ef7b4e7a6e..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/EnvironmentUtilities.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.util.List;
-import java.util.Map;
-
-import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION;
-
-/**
- * Utilities for environment variable related operations
- * for {@link Service} objects.
- */
-public final class EnvironmentUtilities {
-  private EnvironmentUtilities() {
-    throw new UnsupportedOperationException("This class should not be " +
-        "instantiated!");
-  }
-
-  private static final Logger LOG =
-      LoggerFactory.getLogger(EnvironmentUtilities.class);
-
-  static final String ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME =
-      "YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS";
-  private static final String MOUNTS_DELIM = ",";
-  private static final String ENV_SEPARATOR = "=";
-  private static final String ETC_PASSWD_MOUNT_STRING =
-      "/etc/passwd:/etc/passwd:ro";
-  private static final String KERBEROS_CONF_MOUNT_STRING =
-      "/etc/krb5.conf:/etc/krb5.conf:ro";
-  private static final String ENV_VAR_DELIM = ":";
-
-  /**
-   * Extracts value from a string representation of an environment variable.
-   * @param envVar The environment variable in 'key=value' format.
-   * @return The value of the environment variable
-   */
-  public static String getValueOfEnvironment(String envVar) {
-    if (envVar == null || !envVar.contains(ENV_SEPARATOR)) {
-      return "";
-    } else {
-      return envVar.substring(envVar.indexOf(ENV_SEPARATOR) + 1);
-    }
-  }
-
-  public static void handleServiceEnvs(Service service,
-      Configuration yarnConfig, List<String> envVars) {
-    if (envVars != null) {
-      for (String envVarPair : envVars) {
-        String key, value;
-        if (envVarPair.contains(ENV_SEPARATOR)) {
-          int idx = envVarPair.indexOf(ENV_SEPARATOR);
-          key = envVarPair.substring(0, idx);
-          value = envVarPair.substring(idx + 1);
-        } else {
-          LOG.warn("Found environment variable with unusual format: '{}'",
-              envVarPair);
-          // No "=" found so use the whole key
-          key = envVarPair;
-          value = "";
-        }
-        appendToEnv(service, key, value, ENV_VAR_DELIM);
-      }
-    }
-    appendOtherConfigs(service, yarnConfig);
-  }
-
-  /**
-   * Appends other configs like /etc/passwd, /etc/krb5.conf.
-   * @param service
-   * @param yarnConfig
-   */
-  private static void appendOtherConfigs(Service service,
-      Configuration yarnConfig) {
-    appendToEnv(service, ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME,
-        ETC_PASSWD_MOUNT_STRING, MOUNTS_DELIM);
-
-    String authentication = yarnConfig.get(HADOOP_SECURITY_AUTHENTICATION);
-    if (authentication != null && authentication.equals("kerberos")) {
-      appendToEnv(service, ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME,
-          KERBEROS_CONF_MOUNT_STRING, MOUNTS_DELIM);
-    }
-  }
-
-  static void appendToEnv(Service service, String key, String value,
-      String delim) {
-    Map<String, String> env = service.getConfiguration().getEnv();
-    if (!env.containsKey(key)) {
-      env.put(key, value);
-    } else {
-      if (!value.isEmpty()) {
-        String existingValue = env.get(key);
-        if (!existingValue.endsWith(delim)) {
-          env.put(key, existingValue + delim + value);
-        } else {
-          env.put(key, existingValue + value);
-        }
-      }
-    }
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/KerberosPrincipalFactory.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/KerberosPrincipalFactory.java
deleted file mode 100644
index ac28bdde500c..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/KerberosPrincipalFactory.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- * Simple factory that creates a {@link KerberosPrincipal}.
- */
-public final class KerberosPrincipalFactory {
-  private KerberosPrincipalFactory() {
-    throw new UnsupportedOperationException("This class should not be " +
-        "instantiated!");
-  }
-
-  private static final Logger LOG =
-      LoggerFactory.getLogger(KerberosPrincipalFactory.class);
-
-  public static KerberosPrincipal create(FileSystemOperations fsOperations,
-      RemoteDirectoryManager remoteDirectoryManager,
-      RunJobParameters parameters) throws IOException {
-    Objects.requireNonNull(fsOperations,
-        "FileSystemOperations must not be null!");
-    Objects.requireNonNull(remoteDirectoryManager,
-        "RemoteDirectoryManager must not be null!");
-    Objects.requireNonNull(parameters, "Parameters must not be null!");
-
-    if (StringUtils.isNotBlank(parameters.getKeytab()) && StringUtils
-        .isNotBlank(parameters.getPrincipal())) {
-      String keytab = parameters.getKeytab();
-      String principal = parameters.getPrincipal();
-      if (parameters.isDistributeKeytab()) {
-        return handleDistributedKeytab(fsOperations, remoteDirectoryManager,
-            parameters, keytab, principal);
-      } else {
-        return handleNormalKeytab(keytab, principal);
-      }
-    }
-    LOG.debug("Principal and keytab was null or empty, " +
-        "returning null KerberosPrincipal!");
-    return null;
-  }
-
-  private static KerberosPrincipal handleDistributedKeytab(
-      FileSystemOperations fsOperations,
-      RemoteDirectoryManager remoteDirectoryManager,
-      RunJobParameters parameters, String keytab, String principal)
-      throws IOException {
-    Path stagingDir = remoteDirectoryManager
-        .getJobStagingArea(parameters.getName(), true);
-    Path remoteKeytabPath =
-        fsOperations.uploadToRemoteFile(stagingDir, keytab);
-    // Only the owner has read access
-    fsOperations.setPermission(remoteKeytabPath,
-        FsPermission.createImmutable((short)Integer.parseInt("400", 8)));
-    return new KerberosPrincipal()
-        .keytab(remoteKeytabPath.toString())
-        .principalName(principal);
-  }
-
-  private static KerberosPrincipal handleNormalKeytab(String keytab,
-      String principal) {
-    if(!keytab.startsWith("file")) {
-      keytab = "file://" + keytab;
-    }
-    return new KerberosPrincipal()
-        .keytab(keytab)
-        .principalName(principal);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/Localizer.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/Localizer.java
deleted file mode 100644
index c28b98b5a47b..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/Localizer.java
+++ /dev/null
@@ -1,170 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.yarn.service.api.records.ConfigFile;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.submarine.client.cli.param.Localization;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.List;
-
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations.needHdfs;
-import static org.apache.hadoop.yarn.submarine.utils.EnvironmentUtilities.appendToEnv;
-
-/**
- * This class holds all dependencies in order to localize dependencies
- * for containers.
- */
-public class Localizer {
-  private static final Logger LOG = LoggerFactory.getLogger(Localizer.class);
-
-  private final FileSystemOperations fsOperations;
-  private final RemoteDirectoryManager remoteDirectoryManager;
-  private final RunJobParameters parameters;
-
-  public Localizer(FileSystemOperations fsOperations,
-      RemoteDirectoryManager remoteDirectoryManager,
-      RunJobParameters parameters) {
-    this.fsOperations = fsOperations;
-    this.remoteDirectoryManager = remoteDirectoryManager;
-    this.parameters = parameters;
-  }
-
-  /**
-   * Localize dependencies for all containers.
-   * If remoteUri is a local directory,
-   * we'll zip it, upload to HDFS staging dir HDFS.
-   * If remoteUri is directory, we'll download it, zip it and upload
-   * to HDFS.
-   * If localFilePath is ".", we'll use remoteUri's file/dir name
-   * */
-  public void handleLocalizations(Service service)
-      throws IOException {
-    // Handle localizations
-    Path stagingDir =
-        remoteDirectoryManager.getJobStagingArea(
-            parameters.getName(), true);
-    List<Localization> localizations = parameters.getLocalizations();
-    String remoteUri;
-    String containerLocalPath;
-
-    // Check to fail fast
-    for (Localization loc : localizations) {
-      remoteUri = loc.getRemoteUri();
-      Path resourceToLocalize = new Path(remoteUri);
-      // Check if remoteUri exists
-      if (remoteDirectoryManager.isRemote(remoteUri)) {
-        // check if exists
-        if (!remoteDirectoryManager.existsRemoteFile(resourceToLocalize)) {
-          throw new FileNotFoundException(
-              "File " + remoteUri + " doesn't exists.");
-        }
-      } else {
-        // Check if exists
-        File localFile = new File(remoteUri);
-        if (!localFile.exists()) {
-          throw new FileNotFoundException(
-              "File " + remoteUri + " doesn't exists.");
-        }
-      }
-      // check remote file size
-      fsOperations.validFileSize(remoteUri);
-    }
-    // Start download remote if needed and upload to HDFS
-    for (Localization loc : localizations) {
-      remoteUri = loc.getRemoteUri();
-      containerLocalPath = loc.getLocalPath();
-      String srcFileStr = remoteUri;
-      ConfigFile.TypeEnum destFileType = ConfigFile.TypeEnum.STATIC;
-      Path resourceToLocalize = new Path(remoteUri);
-      boolean needUploadToHDFS = true;
-
-
-      // Special handling of remoteUri directory
-      boolean needDeleteTempFile = false;
-      if (remoteDirectoryManager.isDir(remoteUri)) {
-        destFileType = ConfigFile.TypeEnum.ARCHIVE;
-        srcFileStr = fsOperations.downloadAndZip(
-            remoteUri, getLastNameFromPath(srcFileStr), true);
-      } else if (remoteDirectoryManager.isRemote(remoteUri)) {
-        if (!needHdfs(remoteUri)) {
-          // Non HDFS remote uri. Non directory, no need to zip
-          srcFileStr = fsOperations.downloadAndZip(
-              remoteUri, getLastNameFromPath(srcFileStr), false);
-          needDeleteTempFile = true;
-        } else {
-          // HDFS file, no need to upload
-          needUploadToHDFS = false;
-        }
-      }
-
-      // Upload file to HDFS
-      if (needUploadToHDFS) {
-        resourceToLocalize =
-            fsOperations.uploadToRemoteFile(stagingDir, srcFileStr);
-      }
-      if (needDeleteTempFile) {
-        fsOperations.deleteFiles(srcFileStr);
-      }
-      // Remove .zip from zipped dir name
-      if (destFileType == ConfigFile.TypeEnum.ARCHIVE
-          && srcFileStr.endsWith(".zip")) {
-        // Delete local zip file
-        fsOperations.deleteFiles(srcFileStr);
-        int suffixIndex = srcFileStr.lastIndexOf('_');
-        srcFileStr = srcFileStr.substring(0, suffixIndex);
-      }
-      // If provided, use the name of local uri
-      if (!containerLocalPath.equals(".")
-          && !containerLocalPath.equals("./")) {
-        // Change the YARN localized file name to what'll used in container
-        srcFileStr = getLastNameFromPath(containerLocalPath);
-      }
-      String localizedName = getLastNameFromPath(srcFileStr);
-      LOG.info("The file/dir to be localized is {}",
-          resourceToLocalize.toString());
-      LOG.info("Its localized file name will be {}", localizedName);
-      service.getConfiguration().getFiles().add(new ConfigFile().srcFile(
-          resourceToLocalize.toUri().toString()).destFile(localizedName)
-          .type(destFileType));
-      // set mounts
-      // if mount path is absolute, just use it.
-      // if relative, no need to mount explicitly
-      if (containerLocalPath.startsWith("/")) {
-        String mountStr = getLastNameFromPath(srcFileStr) + ":"
-            + containerLocalPath + ":" + loc.getMountPermission();
-        LOG.info("Add bind-mount string {}", mountStr);
-        appendToEnv(service,
-            EnvironmentUtilities.ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME,
-            mountStr, ",");
-      }
-    }
-  }
-
-  private String getLastNameFromPath(String srcFileStr) {
-    return new Path(srcFileStr).getName();
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/SubmarineResourceUtils.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/SubmarineResourceUtils.java
deleted file mode 100644
index 3d1a237a047d..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/SubmarineResourceUtils.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import org.apache.hadoop.yarn.service.api.records.Resource;
-import org.apache.hadoop.yarn.service.api.records.ResourceInformation;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Resource utilities for Submarine.
- */
-public final class SubmarineResourceUtils {
-  private SubmarineResourceUtils() {
-    throw new UnsupportedOperationException("This class should not be " +
-        "instantiated!");
-  }
-
-  public static Resource convertYarnResourceToServiceResource(
-      org.apache.hadoop.yarn.api.records.Resource yarnResource) {
-    Resource serviceResource = new Resource();
-    serviceResource.setCpus(yarnResource.getVirtualCores());
-    serviceResource.setMemory(String.valueOf(yarnResource.getMemorySize()));
-
-    Map<String, ResourceInformation> riMap = new HashMap<>();
-    for (org.apache.hadoop.yarn.api.records.ResourceInformation ri :
-        yarnResource.getAllResourcesListCopy()) {
-      ResourceInformation serviceRi = new ResourceInformation();
-      serviceRi.setValue(ri.getValue());
-      serviceRi.setUnit(ri.getUnits());
-      riMap.put(ri.getName(), serviceRi);
-    }
-    serviceResource.setResourceInformations(riMap);
-
-    return serviceResource;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/ZipUtilities.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/ZipUtilities.java
deleted file mode 100644
index c75f2d333592..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/ZipUtilities.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import com.google.common.annotations.VisibleForTesting;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.util.zip.ZipEntry;
-import java.util.zip.ZipOutputStream;
-
-/**
- * Utilities for zipping directories and adding existing directories to zips.
- */
-public final class ZipUtilities {
-  private ZipUtilities() {
-    throw new UnsupportedOperationException("This class should not be " +
-        "instantiated!");
-  }
-
-  private static final Logger LOG = LoggerFactory.getLogger(ZipUtilities.class);
-
-  @VisibleForTesting
-  public static String zipDir(String srcDir, String dstFile)
-      throws IOException {
-    FileOutputStream fos = new FileOutputStream(dstFile);
-    ZipOutputStream zos = new ZipOutputStream(fos);
-    File srcFile = new File(srcDir);
-    LOG.info("Compressing directory {}", srcDir);
-    addDirToZip(zos, srcFile, srcFile);
-    // close the ZipOutputStream
-    zos.close();
-    LOG.info("Compressed directory {} to file: {}", srcDir, dstFile);
-    return dstFile;
-  }
-
-  private static void addDirToZip(ZipOutputStream zos, File srcFile, File base)
-      throws IOException {
-    File[] files = srcFile.listFiles();
-    if (files == null) {
-      return;
-    }
-    for (File file : files) {
-      // if it's directory, add recursively
-      if (file.isDirectory()) {
-        addDirToZip(zos, file, base);
-        continue;
-      }
-      byte[] buffer = new byte[1024];
-      try(FileInputStream fis = new FileInputStream(file)) {
-        String name = base.toURI().relativize(file.toURI()).getPath();
-        LOG.info("Adding file {} to zip", name);
-        zos.putNextEntry(new ZipEntry(name));
-        int length;
-        while ((length = fis.read(buffer)) > 0) {
-          zos.write(buffer, 0, length);
-        }
-        zos.flush();
-      } finally {
-        zos.closeEntry();
-      }
-    }
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/package-info.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/package-info.java
deleted file mode 100644
index 2f60d903cfd1..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/package-info.java
+++ /dev/null
@@ -1,19 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * This package contains classes utility classes.
- */
-package org.apache.hadoop.yarn.submarine.utils;
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/FileUtilitiesForTests.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/FileUtilitiesForTests.java
deleted file mode 100644
index 4abfcae98bc6..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/FileUtilitiesForTests.java
+++ /dev/null
@@ -1,150 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine;
-
-import com.google.common.collect.Lists;
-import org.apache.commons.io.FileUtils;
-import org.apache.hadoop.fs.Path;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.List;
-
-import static org.junit.Assert.assertTrue;
-
-/**
- * File utilities for tests.
- * Provides methods that can create, delete files or directories
- * in a temp directory, or any specified directory.
- */
-public class FileUtilitiesForTests {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(FileUtilitiesForTests.class);
-  private String tempDir;
-  private List<File> cleanupFiles;
-
-  public void setup() {
-    cleanupFiles = Lists.newArrayList();
-    tempDir = System.getProperty("java.io.tmpdir");
-  }
-
-  public void teardown() throws IOException {
-    LOG.info("About to clean up files: " + cleanupFiles);
-    List<File> dirs = Lists.newArrayList();
-    for (File cleanupFile : cleanupFiles) {
-      if (cleanupFile.isDirectory()) {
-        dirs.add(cleanupFile);
-      } else {
-        deleteFile(cleanupFile);
-      }
-    }
-
-    for (File dir : dirs) {
-      deleteFile(dir);
-    }
-  }
-
-  public File createFileInTempDir(String filename) throws IOException {
-    File file = new File(tempDir, new Path(filename).getName());
-    createFile(file);
-    return file;
-  }
-
-  public File createDirInTempDir(String dirName) {
-    File file = new File(tempDir, new Path(dirName).getName());
-    createDirectory(file);
-    return file;
-  }
-
-  public File createFileInDir(Path dir, String filename) throws IOException {
-    File dirTmp = new File(dir.toUri().getPath());
-    if (!dirTmp.exists()) {
-      createDirectory(dirTmp);
-    }
-    File file =
-        new File(dir.toUri().getPath() + "/" + new Path(filename).getName());
-    createFile(file);
-    return file;
-  }
-
-  public File createFileInDir(File dir, String filename) throws IOException {
-    if (!dir.exists()) {
-      createDirectory(dir);
-    }
-    File file = new File(dir, filename);
-    createFile(file);
-    return file;
-  }
-
-  public File createDirectory(Path parent, String dirname) {
-    File dir =
-        new File(parent.toUri().getPath() + "/" + new Path(dirname).getName());
-    createDirectory(dir);
-    return dir;
-  }
-
-  public File createDirectory(File parent, String dirname) {
-    File dir =
-        new File(parent.getPath() + "/" + new Path(dirname).getName());
-    createDirectory(dir);
-    return dir;
-  }
-
-  private void createDirectory(File dir) {
-    boolean result = dir.mkdir();
-    assertTrue("Failed to create directory " + dir.getAbsolutePath(), result);
-    assertTrue("Directory does not exist: " + dir.getAbsolutePath(),
-        dir.exists());
-    this.cleanupFiles.add(dir);
-  }
-
-  private void createFile(File file) throws IOException {
-    boolean result = file.createNewFile();
-    assertTrue("Failed to create file " + file.getAbsolutePath(), result);
-    assertTrue("File does not exist: " + file.getAbsolutePath(), file.exists());
-    this.cleanupFiles.add(file);
-  }
-
-  private static void deleteFile(File file) throws IOException {
-    if (file.isDirectory()) {
-      LOG.info("Removing directory: " + file.getAbsolutePath());
-      FileUtils.deleteDirectory(file);
-    }
-
-    if (file.exists()) {
-      LOG.info("Removing file: " + file.getAbsolutePath());
-      boolean result = file.delete();
-      assertTrue("Deletion of file " + file.getAbsolutePath()
-          + " was not successful!", result);
-    }
-  }
-
-  public File getTempFileWithName(String filename) {
-    return new File(tempDir + "/" + new Path(filename).getName());
-  }
-
-  public static File getFilename(Path parent, String filename) {
-    return new File(
-        parent.toUri().getPath() + "/" + new Path(filename).getName());
-  }
-
-  public void addTrackedFile(File file) {
-    this.cleanupFiles.add(file);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/ParamBuilderForTest.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/ParamBuilderForTest.java
deleted file mode 100644
index f4c8031ccedd..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/ParamBuilderForTest.java
+++ /dev/null
@@ -1,151 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.yarnservice;
-
-import com.google.common.collect.Lists;
-
-import java.util.List;
-
-class ParamBuilderForTest {
-  private final List<String> params = Lists.newArrayList();
-
-  static ParamBuilderForTest create() {
-    return new ParamBuilderForTest();
-  }
-
-  ParamBuilderForTest withJobName(String name) {
-    params.add("--name");
-    params.add(name);
-    return this;
-  }
-
-  ParamBuilderForTest withFramework(String framework) {
-    params.add("--framework");
-    params.add(framework);
-    return this;
-  }
-
-  ParamBuilderForTest withDockerImage(String dockerImage) {
-    params.add("--docker_image");
-    params.add(dockerImage);
-    return this;
-  }
-
-  ParamBuilderForTest withInputPath(String inputPath) {
-    params.add("--input_path");
-    params.add(inputPath);
-    return this;
-  }
-
-  ParamBuilderForTest withCheckpointPath(String checkpointPath) {
-    params.add("--checkpoint_path");
-    params.add(checkpointPath);
-    return this;
-  }
-
-  ParamBuilderForTest withQueue(String queue) {
-    params.add("--queue");
-    params.add(queue);
-    return this;
-  }
-
-  ParamBuilderForTest withNumberOfWorkers(int numWorkers) {
-    params.add("--num_workers");
-    params.add(String.valueOf(numWorkers));
-    return this;
-  }
-
-  ParamBuilderForTest withNumberOfPs(int numPs) {
-    params.add("--num_ps");
-    params.add(String.valueOf(numPs));
-    return this;
-  }
-
-  ParamBuilderForTest withWorkerLaunchCommand(String launchCommand) {
-    params.add("--worker_launch_cmd");
-    params.add(launchCommand);
-    return this;
-  }
-
-  ParamBuilderForTest withPsLaunchCommand(String launchCommand) {
-    params.add("--ps_launch_cmd");
-    params.add(launchCommand);
-    return this;
-  }
-
-  ParamBuilderForTest withWorkerResources(String workerResources) {
-    params.add("--worker_resources");
-    params.add(workerResources);
-    return this;
-  }
-
-  ParamBuilderForTest withPsResources(String psResources) {
-    params.add("--ps_resources");
-    params.add(psResources);
-    return this;
-  }
-
-  ParamBuilderForTest withWorkerDockerImage(String dockerImage) {
-    params.add("--worker_docker_image");
-    params.add(dockerImage);
-    return this;
-  }
-
-  ParamBuilderForTest withPsDockerImage(String dockerImage) {
-    params.add("--ps_docker_image");
-    params.add(dockerImage);
-    return this;
-  }
-
-  ParamBuilderForTest withVerbose() {
-    params.add("--verbose");
-    return this;
-  }
-
-  ParamBuilderForTest withTensorboard() {
-    params.add("--tensorboard");
-    return this;
-  }
-
-  ParamBuilderForTest withTensorboardResources(String resources) {
-    params.add("--tensorboard_resources");
-    params.add(resources);
-    return this;
-  }
-
-  ParamBuilderForTest withTensorboardDockerImage(String dockerImage) {
-    params.add("--tensorboard_docker_image");
-    params.add(dockerImage);
-    return this;
-  }
-
-  ParamBuilderForTest withQuickLink(String quickLink) {
-    params.add("--quicklink");
-    params.add(quickLink);
-    return this;
-  }
-
-  ParamBuilderForTest withLocalization(String remoteUrl, String localUrl) {
-    params.add("--localization");
-    params.add(remoteUrl + ":" + localUrl);
-    return this;
-  }
-
-  String[] build() {
-    return params.toArray(new String[0]);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCli.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCli.java
deleted file mode 100644
index cde74050196e..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCli.java
+++ /dev/null
@@ -1,677 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.yarnservice;
-
-import com.google.common.collect.ImmutableMap;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.RunJobCli;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.common.StorageKeyConstants;
-import org.apache.hadoop.yarn.submarine.runtimes.common.SubmarineStorage;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.ServiceSpecFileGenerator;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.ServiceWrapper;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceJobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component.TensorBoardComponent;
-import org.apache.hadoop.yarn.submarine.utils.ZipUtilities;
-import org.apache.hadoop.yarn.util.resource.Resources;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.io.IOException;
-import java.nio.charset.Charset;
-import java.nio.file.Files;
-import java.nio.file.Paths;
-import java.util.Map;
-
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_CHECKPOINT_PATH;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_DOCKER_IMAGE;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_INPUT_PATH;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_PS_DOCKER_IMAGE;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_PS_LAUNCH_CMD;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_PS_RESOURCES;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_TENSORBOARD_DOCKER_IMAGE;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_TENSORBOARD_RESOURCES;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_WORKER_DOCKER_IMAGE;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_WORKER_LAUNCH_CMD;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_WORKER_RESOURCES;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.DEFAULT_QUEUE;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
-import static org.junit.Assert.assertTrue;
-
-/**
- * Class to test YarnService with the Run job CLI action.
- */
-public class TestYarnServiceRunJobCli {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TestYarnServiceRunJobCli.class);
-
-  private TestYarnServiceRunJobCliCommons testCommons =
-      new TestYarnServiceRunJobCliCommons();
-
-  @Rule
-  public TestName testName = new TestName();
-
-  @Before
-  public void before() throws IOException, YarnException {
-    testCommons.setup();
-  }
-
-  @After
-  public void cleanup() throws IOException {
-    testCommons.teardown();
-  }
-
-  @Test
-  public void testPrintHelp() {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    runJobCli.printUsages();
-  }
-
-  private ServiceWrapper getServiceWrapperFromJobSubmitter(
-      JobSubmitter jobSubmitter) {
-    return ((YarnServiceJobSubmitter) jobSubmitter).getServiceWrapper();
-  }
-
-  private void commonVerifyDistributedTrainingSpec(Service serviceSpec) {
-    assertNotNull(serviceSpec.getComponent(TensorFlowRole.WORKER
-        .getComponentName()));
-    assertNotNull(
-        serviceSpec.getComponent(TensorFlowRole.PRIMARY_WORKER
-            .getComponentName()));
-    assertNotNull(serviceSpec.getComponent(TensorFlowRole.PS
-        .getComponentName()));
-    Component primaryWorkerComp = serviceSpec.getComponent(
-        TensorFlowRole.PRIMARY_WORKER.getComponentName());
-    assertEquals(2048, primaryWorkerComp.getResource().calcMemoryMB());
-    assertEquals(2,
-        primaryWorkerComp.getResource().getCpus().intValue());
-
-    Component workerComp = serviceSpec.getComponent(
-        TensorFlowRole.WORKER.getComponentName());
-    assertEquals(2048, workerComp.getResource().calcMemoryMB());
-    assertEquals(2, workerComp.getResource().getCpus().intValue());
-
-    Component psComp = serviceSpec.getComponent(TensorFlowRole.PS
-        .getComponentName());
-    assertEquals(4096, psComp.getResource().calcMemoryMB());
-    assertEquals(4, psComp.getResource().getCpus().intValue());
-
-    assertEquals(DEFAULT_WORKER_DOCKER_IMAGE, workerComp.getArtifact().getId());
-    assertEquals(DEFAULT_PS_DOCKER_IMAGE, psComp.getArtifact().getId());
-
-    assertEquals(DEFAULT_QUEUE, serviceSpec.getQueue());
-    assertTrue(SubmarineLogs.isVerbose());
-  }
-
-  private void verifyQuicklink(Service serviceSpec,
-      Map<String, String> expectedQuicklinks) {
-    Map<String, String> actualQuicklinks = serviceSpec.getQuicklinks();
-    if (actualQuicklinks == null || actualQuicklinks.isEmpty()) {
-      assertTrue(
-          expectedQuicklinks == null || expectedQuicklinks.isEmpty());
-      return;
-    }
-
-    assertEquals(expectedQuicklinks.size(), actualQuicklinks.size());
-    for (Map.Entry<String, String> expectedEntry : expectedQuicklinks
-        .entrySet()) {
-      assertTrue(actualQuicklinks.containsKey(expectedEntry.getKey()));
-
-      // $USER could be changed in different environment. so replace $USER by
-      // "user"
-      String expectedValue = expectedEntry.getValue();
-      String actualValue = actualQuicklinks.get(expectedEntry.getKey());
-
-      String userName = System.getProperty("user.name");
-      actualValue = actualValue.replaceAll(userName, "username");
-
-      assertEquals(expectedValue, actualValue);
-    }
-  }
-
-  private void saveServiceSpecJsonToTempFile(Service service)
-      throws IOException {
-    String jsonFileName = ServiceSpecFileGenerator.generateJson(service);
-//    testCommons.getFileUtils().addTrackedFile(new File(jsonFileName));
-    LOG.info("Saved json file: " + jsonFileName);
-  }
-
-  @Test
-  public void testBasicRunJobForDistributedTraining() throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(testName.getMethodName())
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withCheckpointPath(DEFAULT_CHECKPOINT_PATH)
-        .withQueue(DEFAULT_QUEUE)
-        .withNumberOfWorkers(3)
-        .withWorkerDockerImage(DEFAULT_WORKER_DOCKER_IMAGE)
-        .withWorkerLaunchCommand(DEFAULT_WORKER_LAUNCH_CMD)
-        .withWorkerResources(DEFAULT_WORKER_RESOURCES)
-        .withNumberOfPs(2)
-        .withPsDockerImage(DEFAULT_PS_DOCKER_IMAGE)
-        .withPsLaunchCommand(DEFAULT_PS_LAUNCH_CMD)
-        .withPsResources(DEFAULT_PS_RESOURCES)
-        .withVerbose()
-        .build();
-    runJobCli.run(params);
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    saveServiceSpecJsonToTempFile(serviceSpec);
-    assertEquals(3, serviceSpec.getComponents().size());
-
-    commonVerifyDistributedTrainingSpec(serviceSpec);
-
-    verifyQuicklink(serviceSpec, null);
-  }
-
-  @Test
-  public void testBasicRunJobForDistributedTrainingWithTensorboard()
-      throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String jobName = testName.getMethodName();
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(jobName)
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withCheckpointPath(DEFAULT_CHECKPOINT_PATH)
-        .withQueue(DEFAULT_QUEUE)
-        .withNumberOfWorkers(3)
-        .withWorkerDockerImage(DEFAULT_WORKER_DOCKER_IMAGE)
-        .withWorkerLaunchCommand(DEFAULT_WORKER_LAUNCH_CMD)
-        .withWorkerResources(DEFAULT_WORKER_RESOURCES)
-        .withNumberOfPs(2)
-        .withPsDockerImage(DEFAULT_PS_DOCKER_IMAGE)
-        .withPsLaunchCommand(DEFAULT_PS_LAUNCH_CMD)
-        .withPsResources(DEFAULT_PS_RESOURCES)
-        .withVerbose()
-        .withTensorboard()
-        .build();
-    runJobCli.run(params);
-    ServiceWrapper serviceWrapper = getServiceWrapperFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    Service serviceSpec = serviceWrapper.getService();
-    saveServiceSpecJsonToTempFile(serviceSpec);
-    assertEquals(4, serviceSpec.getComponents().size());
-
-    commonVerifyDistributedTrainingSpec(serviceSpec);
-
-    verifyTensorboardComponent(runJobCli, serviceWrapper,
-        Resources.createResource(4096, 1));
-
-    verifyQuicklink(serviceSpec, ImmutableMap
-        .of(TensorBoardComponent.TENSORBOARD_QUICKLINK_LABEL,
-            String.format("http://tensorboard-0.%s.username.null:6006",
-                jobName)));
-  }
-
-  @Test
-  public void testBasicRunJobForSingleNodeTraining() throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(testName.getMethodName())
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withCheckpointPath(DEFAULT_CHECKPOINT_PATH)
-        .withNumberOfWorkers(1)
-        .withWorkerLaunchCommand(DEFAULT_WORKER_LAUNCH_CMD)
-        .withWorkerResources(DEFAULT_WORKER_RESOURCES)
-        .withVerbose()
-        .build();
-    runJobCli.run(params);
-
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    saveServiceSpecJsonToTempFile(serviceSpec);
-    assertEquals(1, serviceSpec.getComponents().size());
-
-    commonTestSingleNodeTraining(serviceSpec);
-  }
-
-  @Test
-  public void testTensorboardOnlyService() throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(testName.getMethodName())
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withCheckpointPath(DEFAULT_CHECKPOINT_PATH)
-        .withNumberOfWorkers(0)
-        .withTensorboard()
-        .withVerbose()
-        .build();
-    runJobCli.run(params);
-
-    ServiceWrapper serviceWrapper = getServiceWrapperFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    saveServiceSpecJsonToTempFile(serviceSpec);
-
-    assertEquals(1, serviceWrapper.getService().getComponents().size());
-
-    verifyTensorboardComponent(runJobCli, serviceWrapper,
-        Resources.createResource(4096, 1));
-  }
-
-  @Test
-  public void testTensorboardOnlyServiceWithCustomDockerImageAndCheckpointPath()
-      throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(testName.getMethodName())
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withCheckpointPath(DEFAULT_CHECKPOINT_PATH)
-        .withNumberOfWorkers(0)
-        .withTensorboard()
-        .withTensorboardResources(DEFAULT_TENSORBOARD_RESOURCES)
-        .withTensorboardDockerImage(DEFAULT_TENSORBOARD_DOCKER_IMAGE)
-        .withVerbose()
-        .build();
-    runJobCli.run(params);
-
-    ServiceWrapper serviceWrapper = getServiceWrapperFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    saveServiceSpecJsonToTempFile(serviceSpec);
-    assertEquals(1, serviceWrapper.getService().getComponents().size());
-
-    verifyTensorboardComponent(runJobCli, serviceWrapper,
-        Resources.createResource(2048, 2));
-  }
-
-  @Test
-  public void testTensorboardOnlyServiceWithCustomizedDockerImageAndResource()
-      throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String jobName = testName.getMethodName();
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(jobName)
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withNumberOfWorkers(0)
-        .withTensorboard()
-        .withTensorboardResources(DEFAULT_TENSORBOARD_RESOURCES)
-        .withTensorboardDockerImage(DEFAULT_TENSORBOARD_DOCKER_IMAGE)
-        .withVerbose()
-        .build();
-    runJobCli.run(params);
-
-    ServiceWrapper serviceWrapper = getServiceWrapperFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    saveServiceSpecJsonToTempFile(serviceSpec);
-    assertEquals(1, serviceWrapper.getService().getComponents().size());
-
-    verifyTensorboardComponent(runJobCli, serviceWrapper,
-        Resources.createResource(2048, 2));
-    verifyQuicklink(serviceWrapper.getService(), ImmutableMap
-        .of(TensorBoardComponent.TENSORBOARD_QUICKLINK_LABEL,
-            String.format("http://tensorboard-0.%s.username.null:6006",
-                jobName)));
-  }
-
-  private void commonTestSingleNodeTraining(Service serviceSpec) {
-    assertNotNull(
-        serviceSpec.getComponent(TensorFlowRole.PRIMARY_WORKER
-            .getComponentName()));
-    Component primaryWorkerComp = serviceSpec.getComponent(
-        TensorFlowRole.PRIMARY_WORKER.getComponentName());
-    assertEquals(2048, primaryWorkerComp.getResource().calcMemoryMB());
-    assertEquals(2,
-        primaryWorkerComp.getResource().getCpus().intValue());
-
-    assertTrue(SubmarineLogs.isVerbose());
-  }
-
-  private void verifyTensorboardComponent(RunJobCli runJobCli,
-      ServiceWrapper serviceWrapper, Resource resource) throws Exception {
-    Service serviceSpec = serviceWrapper.getService();
-    assertNotNull(
-        serviceSpec.getComponent(TensorFlowRole.TENSORBOARD
-            .getComponentName()));
-    Component tensorboardComp = serviceSpec.getComponent(
-        TensorFlowRole.TENSORBOARD.getComponentName());
-    assertEquals(1, tensorboardComp.getNumberOfContainers().intValue());
-    assertEquals(resource.getMemorySize(),
-        tensorboardComp.getResource().calcMemoryMB());
-    assertEquals(resource.getVirtualCores(),
-        tensorboardComp.getResource().getCpus().intValue());
-
-    assertEquals("./run-TENSORBOARD.sh",
-        tensorboardComp.getLaunchCommand());
-
-    RunJobParameters runJobParameters = runJobCli.getRunJobParameters();
-    assertTrue(RunJobParameters.class + " must be an instance of " +
-            TensorFlowRunJobParameters.class,
-        runJobParameters instanceof TensorFlowRunJobParameters);
-    TensorFlowRunJobParameters tensorFlowParams =
-        (TensorFlowRunJobParameters) runJobParameters;
-
-    // Check docker image
-    if (tensorFlowParams.getTensorboardDockerImage() != null) {
-      assertEquals(
-          tensorFlowParams.getTensorboardDockerImage(),
-          tensorboardComp.getArtifact().getId());
-    } else {
-      assertNull(tensorboardComp.getArtifact());
-    }
-
-    String expectedLaunchScript =
-        "#!/bin/bash\n" + "echo \"CLASSPATH:$CLASSPATH\"\n"
-            + "echo \"HADOOP_CONF_DIR:$HADOOP_CONF_DIR\"\n"
-            + "echo \"HADOOP_TOKEN_FILE_LOCATION:" +
-            "$HADOOP_TOKEN_FILE_LOCATION\"\n"
-            + "echo \"JAVA_HOME:$JAVA_HOME\"\n"
-            + "echo \"LD_LIBRARY_PATH:$LD_LIBRARY_PATH\"\n"
-            + "echo \"HADOOP_HDFS_HOME:$HADOOP_HDFS_HOME\"\n"
-            + "export LC_ALL=C && tensorboard --logdir=" +
-            runJobParameters.getCheckpointPath() + "\n";
-
-    verifyLaunchScriptForComponent(serviceWrapper,
-        TensorFlowRole.TENSORBOARD, expectedLaunchScript);
-  }
-
-  private void verifyLaunchScriptForComponent(ServiceWrapper serviceWrapper,
-      TensorFlowRole taskType, String expectedLaunchScriptContent)
-      throws Exception {
-
-    String path = serviceWrapper
-        .getLocalLaunchCommandPathForComponent(taskType.getComponentName());
-
-    byte[] encoded = Files.readAllBytes(Paths.get(path));
-    String scriptContent = new String(encoded, Charset.defaultCharset());
-
-    assertEquals(expectedLaunchScriptContent, scriptContent);
-  }
-
-  @Test
-  public void testBasicRunJobForSingleNodeTrainingWithTensorboard()
-      throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(testName.getMethodName())
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withCheckpointPath(DEFAULT_CHECKPOINT_PATH)
-        .withNumberOfWorkers(1)
-        .withWorkerLaunchCommand(DEFAULT_WORKER_LAUNCH_CMD)
-        .withWorkerResources(DEFAULT_WORKER_RESOURCES)
-        .withTensorboard()
-        .withVerbose()
-        .build();
-    runJobCli.run(params);
-    ServiceWrapper serviceWrapper = getServiceWrapperFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    Service serviceSpec = serviceWrapper.getService();
-    saveServiceSpecJsonToTempFile(serviceSpec);
-
-    assertEquals(2, serviceSpec.getComponents().size());
-
-    commonTestSingleNodeTraining(serviceSpec);
-    verifyTensorboardComponent(runJobCli, serviceWrapper,
-        Resources.createResource(4096, 1));
-  }
-
-  @Test
-  public void testBasicRunJobForSingleNodeTrainingWithGeneratedCheckpoint()
-      throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(testName.getMethodName())
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withNumberOfWorkers(1)
-        .withWorkerLaunchCommand(DEFAULT_WORKER_LAUNCH_CMD)
-        .withWorkerResources(DEFAULT_WORKER_RESOURCES)
-        .withTensorboard()
-        .withVerbose()
-        .build();
-    runJobCli.run(params);
-    ServiceWrapper serviceWrapper = getServiceWrapperFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    Service serviceSpec = serviceWrapper.getService();
-    saveServiceSpecJsonToTempFile(serviceSpec);
-
-    assertEquals(2, serviceSpec.getComponents().size());
-
-    commonTestSingleNodeTraining(serviceSpec);
-    verifyTensorboardComponent(runJobCli, serviceWrapper,
-        Resources.createResource(4096, 1));
-  }
-
-  @Test
-  public void testParameterStorageForTrainingJob() throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String jobName = testName.getMethodName();
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(jobName)
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withCheckpointPath(DEFAULT_CHECKPOINT_PATH)
-        .withNumberOfWorkers(1)
-        .withWorkerLaunchCommand(DEFAULT_WORKER_LAUNCH_CMD)
-        .withWorkerResources(DEFAULT_WORKER_RESOURCES)
-        .withTensorboard()
-        .withVerbose()
-        .build();
-    runJobCli.run(params);
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    saveServiceSpecJsonToTempFile(serviceSpec);
-
-    SubmarineStorage storage =
-        mockClientContext.getRuntimeFactory().getSubmarineStorage();
-    Map<String, String> jobInfo = storage.getJobInfoByName(jobName);
-    assertTrue(jobInfo.size() > 0);
-    assertEquals(jobInfo.get(StorageKeyConstants.INPUT_PATH),
-        DEFAULT_INPUT_PATH);
-  }
-
-  @Test
-  public void testAddQuicklinksWithoutTensorboard() throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String jobName = testName.getMethodName();
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(jobName)
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withCheckpointPath(DEFAULT_CHECKPOINT_PATH)
-        .withQueue(DEFAULT_QUEUE)
-        .withNumberOfWorkers(3)
-        .withWorkerDockerImage(DEFAULT_WORKER_DOCKER_IMAGE)
-        .withWorkerLaunchCommand(DEFAULT_WORKER_LAUNCH_CMD)
-        .withWorkerResources(DEFAULT_WORKER_RESOURCES)
-        .withNumberOfPs(2)
-        .withPsDockerImage(DEFAULT_PS_DOCKER_IMAGE)
-        .withPsLaunchCommand(DEFAULT_PS_LAUNCH_CMD)
-        .withPsResources(DEFAULT_PS_RESOURCES)
-        .withQuickLink("AAA=http://master-0:8321")
-        .withQuickLink("BBB=http://worker-0:1234")
-        .withVerbose()
-        .build();
-    runJobCli.run(params);
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    saveServiceSpecJsonToTempFile(serviceSpec);
-    assertEquals(3, serviceSpec.getComponents().size());
-
-    commonVerifyDistributedTrainingSpec(serviceSpec);
-
-    verifyQuicklink(serviceSpec, ImmutableMap
-        .of("AAA", String.format("http://master-0.%s.username.null:8321",
-            jobName), "BBB",
-            String.format("http://worker-0.%s.username.null:1234", jobName)));
-  }
-
-  @Test
-  public void testAddQuicklinksWithTensorboard() throws Exception {
-    MockClientContext mockClientContext =
-        YarnServiceCliTestUtils.getMockClientContext();
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-
-    String jobName = testName.getMethodName();
-    String[] params = ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(jobName)
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withCheckpointPath(DEFAULT_CHECKPOINT_PATH)
-        .withQueue(DEFAULT_QUEUE)
-        .withNumberOfWorkers(3)
-        .withWorkerDockerImage(DEFAULT_WORKER_DOCKER_IMAGE)
-        .withWorkerLaunchCommand(DEFAULT_WORKER_LAUNCH_CMD)
-        .withWorkerResources(DEFAULT_WORKER_RESOURCES)
-        .withNumberOfPs(2)
-        .withPsDockerImage(DEFAULT_PS_DOCKER_IMAGE)
-        .withPsLaunchCommand(DEFAULT_PS_LAUNCH_CMD)
-        .withPsResources(DEFAULT_PS_RESOURCES)
-        .withQuickLink("AAA=http://master-0:8321")
-        .withQuickLink("BBB=http://worker-0:1234")
-        .withTensorboard()
-        .withVerbose()
-        .build();
-
-    runJobCli.run(params);
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    saveServiceSpecJsonToTempFile(serviceSpec);
-    assertEquals(4, serviceSpec.getComponents().size());
-
-    commonVerifyDistributedTrainingSpec(serviceSpec);
-
-    verifyQuicklink(serviceSpec, ImmutableMap
-        .of("AAA", String.format("http://master-0.%s.username.null:8321",
-            jobName), "BBB",
-            String.format("http://worker-0.%s.username.null:1234", jobName),
-            TensorBoardComponent.TENSORBOARD_QUICKLINK_LABEL,
-            String.format("http://tensorboard-0.%s.username.null:6006",
-                jobName)));
-  }
-
-  /**
-   * Test zip function.
-   * A dir "/user/yarn/mydir" has two files and one subdir
-   * */
-  @Test
-  public void testYarnServiceSubmitterZipFunction()
-      throws Exception {
-    String localUrl = "/user/yarn/mydir";
-    String localSubDirName = "subdir1";
-
-    // create local file
-    File localDir1 = testCommons.getFileUtils().createDirInTempDir(localUrl);
-    testCommons.getFileUtils().createFileInDir(localDir1, "1.py");
-    testCommons.getFileUtils().createFileInDir(localDir1, "2.py");
-
-    File localSubDir =
-        testCommons.getFileUtils().createDirectory(localDir1, localSubDirName);
-    testCommons.getFileUtils().createFileInDir(localSubDir, "3.py");
-
-    String tempDir = localDir1.getParent();
-    String zipFilePath = ZipUtilities.zipDir(localDir1.getAbsolutePath(),
-        new File(tempDir, "mydir.zip").getAbsolutePath());
-    File zipFile = new File(zipFilePath);
-    File unzipTargetDir = new File(tempDir, "unzipDir");
-    FileUtil.unZip(zipFile, unzipTargetDir);
-    assertTrue(
-        new File(tempDir + "/unzipDir/1.py").exists());
-    assertTrue(
-        new File(tempDir + "/unzipDir/2.py").exists());
-    assertTrue(
-        new File(tempDir + "/unzipDir/subdir1").exists());
-    assertTrue(
-        new File(tempDir + "/unzipDir/subdir1/3.py").exists());
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCliCommons.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCliCommons.java
deleted file mode 100644
index 40e047b7c12e..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCliCommons.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.yarnservice;
-
-import org.apache.hadoop.yarn.client.api.AppAdminClient;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.submarine.FileUtilitiesForTests;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.runtimes.common.JobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceJobSubmitter;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceUtils;
-
-import java.io.IOException;
-
-import static org.apache.hadoop.yarn.service.exceptions.LauncherExitCodes.EXIT_SUCCESS;
-import static org.mockito.ArgumentMatchers.any;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-/**
- * Common operations shared with test classes using Run job-related actions.
- */
-public class TestYarnServiceRunJobCliCommons {
-  static final String DEFAULT_JOB_NAME = "my-job";
-  static final String DEFAULT_DOCKER_IMAGE = "tf-docker:1.1.0";
-  static final String DEFAULT_INPUT_PATH = "s3://input";
-  static final String DEFAULT_CHECKPOINT_PATH = "s3://output";
-  static final String DEFAULT_QUEUE = "root.queue";
-  static final String DEFAULT_WORKER_DOCKER_IMAGE = "worker.image";
-  static final String DEFAULT_PS_DOCKER_IMAGE = "ps.image";
-  static final String DEFAULT_WORKER_LAUNCH_CMD = "python run-job.py";
-  static final String DEFAULT_PS_LAUNCH_CMD = "python run-ps.py";
-  static final String DEFAULT_TENSORBOARD_RESOURCES = "memory=2G,vcores=2";
-  static final String DEFAULT_WORKER_RESOURCES = "memory=2048M,vcores=2";
-  static final String DEFAULT_PS_RESOURCES = "memory=4096M,vcores=4";
-  static final String DEFAULT_TENSORBOARD_DOCKER_IMAGE = "tb_docker_image:001";
-
-  private FileUtilitiesForTests fileUtils = new FileUtilitiesForTests();
-
-  void setup() throws IOException, YarnException {
-    SubmarineLogs.verboseOff();
-    AppAdminClient serviceClient = mock(AppAdminClient.class);
-    when(serviceClient.actionLaunch(any(String.class), any(String.class),
-        any(Long.class), any(String.class))).thenReturn(EXIT_SUCCESS);
-    when(serviceClient.getStatusString(any(String.class))).thenReturn(
-        "{\"id\": \"application_1234_1\"}");
-    YarnServiceUtils.setStubServiceClient(serviceClient);
-
-    fileUtils.setup();
-  }
-
-  void teardown() throws IOException {
-    fileUtils.teardown();
-  }
-
-  FileUtilitiesForTests getFileUtils() {
-    return fileUtils;
-  }
-
-  Service getServiceSpecFromJobSubmitter(JobSubmitter jobSubmitter) {
-    return ((YarnServiceJobSubmitter) jobSubmitter).getServiceWrapper()
-        .getService();
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCliLocalization.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCliLocalization.java
deleted file mode 100644
index a0b67d860087..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCliLocalization.java
+++ /dev/null
@@ -1,600 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.yarnservice;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.service.api.records.ConfigFile;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.RunJobCli;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineConfiguration;
-import org.apache.hadoop.yarn.submarine.common.conf.SubmarineLogs;
-import org.apache.hadoop.yarn.submarine.common.fs.RemoteDirectoryManager;
-import static org.apache.hadoop.yarn.submarine.client.cli.yarnservice.TestYarnServiceRunJobCliCommons.*;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.List;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.fail;
-import static org.mockito.ArgumentMatchers.anyString;
-import static org.mockito.Mockito.reset;
-import static org.mockito.Mockito.spy;
-import static org.mockito.Mockito.times;
-import static org.mockito.Mockito.verify;
-
-/**
- * Class to test YarnService localization feature with the Run job CLI action.
- */
-public class TestYarnServiceRunJobCliLocalization {
-  private static final String ZIP_EXTENSION = ".zip";
-  private TestYarnServiceRunJobCliCommons testCommons =
-      new TestYarnServiceRunJobCliCommons();
-  private MockClientContext mockClientContext;
-  private RemoteDirectoryManager spyRdm;
-
-  @Before
-  public void before() throws IOException, YarnException {
-    testCommons.setup();
-    mockClientContext = YarnServiceCliTestUtils.getMockClientContext();
-    spyRdm = setupSpyRemoteDirManager();
-  }
-
-  @After
-  public void cleanup() throws IOException {
-    testCommons.teardown();
-  }
-
-  private ParamBuilderForTest createCommonParamsBuilder() {
-    return ParamBuilderForTest.create()
-        .withFramework("tensorflow")
-        .withJobName(DEFAULT_JOB_NAME)
-        .withDockerImage(DEFAULT_DOCKER_IMAGE)
-        .withInputPath(DEFAULT_INPUT_PATH)
-        .withCheckpointPath(DEFAULT_CHECKPOINT_PATH)
-        .withNumberOfWorkers(3)
-        .withWorkerDockerImage(DEFAULT_WORKER_DOCKER_IMAGE)
-        .withWorkerLaunchCommand(DEFAULT_WORKER_LAUNCH_CMD)
-        .withWorkerResources(DEFAULT_WORKER_RESOURCES)
-        .withNumberOfPs(2)
-        .withPsDockerImage(DEFAULT_PS_DOCKER_IMAGE)
-        .withPsLaunchCommand(DEFAULT_PS_LAUNCH_CMD)
-        .withPsResources(DEFAULT_PS_RESOURCES)
-        .withVerbose();
-  }
-
-  private void assertFilesAreDeleted(File... files) {
-    for (File file : files) {
-      assertFalse("File should be deleted: " + file.getAbsolutePath(),
-          file.exists());
-    }
-  }
-
-  private RemoteDirectoryManager setupSpyRemoteDirManager() {
-    RemoteDirectoryManager spyRdm =
-        spy(mockClientContext.getRemoteDirectoryManager());
-    mockClientContext.setRemoteDirectoryMgr(spyRdm);
-    return spyRdm;
-  }
-
-  private Path getStagingDir() throws IOException {
-    return mockClientContext.getRemoteDirectoryManager()
-        .getJobStagingArea(DEFAULT_JOB_NAME, true);
-  }
-
-  private RunJobCli createRunJobCliWithoutVerboseAssertion() {
-    return new RunJobCli(mockClientContext);
-  }
-
-  private RunJobCli createRunJobCli() {
-    RunJobCli runJobCli = new RunJobCli(mockClientContext);
-    assertFalse(SubmarineLogs.isVerbose());
-    return runJobCli;
-  }
-
-  private String getFilePath(String localUrl, Path stagingDir) {
-    return stagingDir.toUri().getPath()
-        + "/" + new Path(localUrl).getName();
-  }
-
-  private String getFilePathWithSuffix(Path stagingDir, String localUrl,
-      String suffix) {
-    return stagingDir.toUri().getPath() + "/" + new Path(localUrl).getName()
-        + suffix;
-  }
-
-  private void assertConfigFile(ConfigFile expected, ConfigFile actual) {
-    assertEquals("ConfigFile does not equal to expected!", expected, actual);
-  }
-
-  private void assertNumberOfLocalizations(List<ConfigFile> files,
-      int expected) {
-    assertEquals("Number of localizations is not the expected!", expected,
-        files.size());
-  }
-
-  private void verifyRdmCopyToRemoteLocalCalls(int expectedCalls)
-      throws IOException {
-    verify(spyRdm, times(expectedCalls)).copyRemoteToLocal(anyString(),
-        anyString());
-  }
-
-  /**
-   * Basic test.
-   * In one hand, create local temp file/dir for hdfs URI in
-   * local staging dir.
-   * In the other hand, use MockRemoteDirectoryManager mock
-   * implementation when check FileStatus or exists of HDFS file/dir
-   * --localization hdfs:///user/yarn/script1.py:.
-   * --localization /temp/script2.py:./
-   * --localization /temp/script2.py:/opt/script.py
-   */
-  @Test
-  public void testRunJobWithBasicLocalization() throws Exception {
-    String remoteUrl = "hdfs:///user/yarn/script1.py";
-    String containerLocal1 = ".";
-    String localUrl = "/temp/script2.py";
-    String containerLocal2 = "./";
-    String containerLocal3 = "/opt/script.py";
-    // Create local file, we need to put it under local temp dir
-    File localFile1 = testCommons.getFileUtils().createFileInTempDir(localUrl);
-
-    // create remote file in local staging dir to simulate HDFS
-    Path stagingDir = getStagingDir();
-    testCommons.getFileUtils().createFileInDir(stagingDir, remoteUrl);
-
-    String[] params = createCommonParamsBuilder()
-        .withLocalization(remoteUrl, containerLocal1)
-        .withLocalization(localFile1.getAbsolutePath(), containerLocal2)
-        .withLocalization(localFile1.getAbsolutePath(), containerLocal3)
-        .build();
-    RunJobCli runJobCli = createRunJobCli();
-    runJobCli.run(params);
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    assertNumberOfServiceComponents(serviceSpec, 3);
-
-    // No remote dir and HDFS file exists.
-    // Ensure download never happened.
-    verifyRdmCopyToRemoteLocalCalls(0);
-    // Ensure local original files are not deleted
-    assertTrue(localFile1.exists());
-
-    List<ConfigFile> files = serviceSpec.getConfiguration().getFiles();
-    assertNumberOfLocalizations(files, 3);
-
-    ConfigFile expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.STATIC);
-    expectedConfigFile.setSrcFile(remoteUrl);
-    expectedConfigFile.setDestFile(new Path(remoteUrl).getName());
-    assertConfigFile(expectedConfigFile, files.get(0));
-
-    expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.STATIC);
-    expectedConfigFile.setSrcFile(getFilePath(localUrl, stagingDir));
-    expectedConfigFile.setDestFile(new Path(localUrl).getName());
-    assertConfigFile(expectedConfigFile, files.get(1));
-
-    expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.STATIC);
-    expectedConfigFile.setSrcFile(getFilePath(localUrl, stagingDir));
-    expectedConfigFile.setDestFile(new Path(containerLocal3).getName());
-    assertConfigFile(expectedConfigFile, files.get(2));
-
-    // Ensure env value is correct
-    String env = serviceSpec.getConfiguration().getEnv()
-        .get("YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS");
-    String expectedMounts = new Path(containerLocal3).getName()
-        + ":" + containerLocal3 + ":rw";
-    assertTrue(env.contains(expectedMounts));
-  }
-
-  private void assertNumberOfServiceComponents(Service serviceSpec,
-      int expected) {
-    assertEquals(expected, serviceSpec.getComponents().size());
-  }
-
-  /**
-   * Non HDFS remote URI test.
-   * --localization https://a/b/1.patch:.
-   * --localization s3a://a/dir:/opt/mys3dir
-   */
-  @Test
-  public void testRunJobWithNonHDFSRemoteLocalization() throws Exception {
-    String remoteUri1 = "https://a/b/1.patch";
-    String containerLocal1 = ".";
-    String remoteUri2 = "s3a://a/s3dir";
-    String containerLocal2 = "/opt/mys3dir";
-
-    // create remote file in local staging dir to simulate HDFS
-    Path stagingDir = getStagingDir();
-    testCommons.getFileUtils().createFileInDir(stagingDir, remoteUri1);
-    File remoteDir1 =
-        testCommons.getFileUtils().createDirectory(stagingDir, remoteUri2);
-    testCommons.getFileUtils().createFileInDir(remoteDir1, "afile");
-
-    String suffix1 = "_" + remoteDir1.lastModified()
-        + "-" + mockClientContext.getRemoteDirectoryManager()
-        .getRemoteFileSize(remoteUri2);
-
-    String[] params = createCommonParamsBuilder()
-        .withLocalization(remoteUri1, containerLocal1)
-        .withLocalization(remoteUri2, containerLocal2)
-        .build();
-    RunJobCli runJobCli = createRunJobCli();
-    runJobCli.run(params);
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    assertNumberOfServiceComponents(serviceSpec, 3);
-
-    // Ensure download remote dir 2 times
-    verifyRdmCopyToRemoteLocalCalls(2);
-
-    // Ensure downloaded temp files are deleted
-    assertFilesAreDeleted(
-        testCommons.getFileUtils().getTempFileWithName(remoteUri1),
-        testCommons.getFileUtils().getTempFileWithName(remoteUri2));
-
-    // Ensure zip file are deleted
-    assertFilesAreDeleted(
-        testCommons.getFileUtils()
-            .getTempFileWithName(remoteUri2 + "_" + suffix1 + ZIP_EXTENSION));
-
-    List<ConfigFile> files = serviceSpec.getConfiguration().getFiles();
-    assertNumberOfLocalizations(files, 2);
-
-    ConfigFile expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.STATIC);
-    expectedConfigFile.setSrcFile(getFilePath(remoteUri1, stagingDir));
-    expectedConfigFile.setDestFile(new Path(remoteUri1).getName());
-    assertConfigFile(expectedConfigFile, files.get(0));
-
-    expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.ARCHIVE);
-    expectedConfigFile.setSrcFile(
-        getFilePathWithSuffix(stagingDir, remoteUri2, suffix1 + ZIP_EXTENSION));
-    expectedConfigFile.setDestFile(new Path(containerLocal2).getName());
-    assertConfigFile(expectedConfigFile, files.get(1));
-
-    // Ensure env value is correct
-    String env = serviceSpec.getConfiguration().getEnv()
-        .get("YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS");
-    String expectedMounts = new Path(remoteUri2).getName()
-        + ":" + containerLocal2 + ":rw";
-    assertTrue(env.contains(expectedMounts));
-  }
-
-  /**
-   * Test HDFS dir localization.
-   * --localization hdfs:///user/yarn/mydir:./mydir1
-   * --localization hdfs:///user/yarn/mydir2:/opt/dir2:rw
-   * --localization hdfs:///user/yarn/mydir:.
-   * --localization hdfs:///user/yarn/mydir2:./
-   */
-  @Test
-  public void testRunJobWithHdfsDirLocalization() throws Exception {
-    String remoteUrl = "hdfs:///user/yarn/mydir";
-    String containerPath = "./mydir1";
-    String remoteUrl2 = "hdfs:///user/yarn/mydir2";
-    String containerPath2 = "/opt/dir2";
-    String containerPath3 = ".";
-    String containerPath4 = "./";
-
-    // create remote file in local staging dir to simulate HDFS
-    Path stagingDir = getStagingDir();
-    File remoteDir1 =
-        testCommons.getFileUtils().createDirectory(stagingDir, remoteUrl);
-    testCommons.getFileUtils().createFileInDir(remoteDir1, "1.py");
-    testCommons.getFileUtils().createFileInDir(remoteDir1, "2.py");
-
-    File remoteDir2 =
-        testCommons.getFileUtils().createDirectory(stagingDir, remoteUrl2);
-    testCommons.getFileUtils().createFileInDir(remoteDir2, "3.py");
-    testCommons.getFileUtils().createFileInDir(remoteDir2, "4.py");
-
-    String suffix1 = "_" + remoteDir1.lastModified()
-        + "-" + mockClientContext.getRemoteDirectoryManager()
-        .getRemoteFileSize(remoteUrl);
-    String suffix2 = "_" + remoteDir2.lastModified()
-        + "-" + mockClientContext.getRemoteDirectoryManager()
-        .getRemoteFileSize(remoteUrl2);
-
-    String[] params = createCommonParamsBuilder()
-        .withLocalization(remoteUrl, containerPath)
-        .withLocalization(remoteUrl2, containerPath2)
-        .withLocalization(remoteUrl, containerPath3)
-        .withLocalization(remoteUrl2, containerPath4)
-        .build();
-    RunJobCli runJobCli = createRunJobCli();
-    runJobCli.run(params);
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    assertNumberOfServiceComponents(serviceSpec, 3);
-
-    // Ensure download remote dir 4 times
-    verifyRdmCopyToRemoteLocalCalls(4);
-
-    // Ensure downloaded temp files are deleted
-    assertFilesAreDeleted(
-        testCommons.getFileUtils().getTempFileWithName(remoteUrl),
-        testCommons.getFileUtils().getTempFileWithName(remoteUrl2));
-
-    // Ensure zip file are deleted
-    assertFilesAreDeleted(
-        testCommons.getFileUtils()
-            .getTempFileWithName(remoteUrl + suffix1 + ZIP_EXTENSION),
-        testCommons.getFileUtils()
-            .getTempFileWithName(remoteUrl2 + suffix2 + ZIP_EXTENSION));
-
-    // Ensure files will be localized
-    List<ConfigFile> files = serviceSpec.getConfiguration().getFiles();
-    assertNumberOfLocalizations(files, 4);
-
-    ConfigFile expectedConfigFile = new ConfigFile();
-    // The hdfs dir should be download and compress and let YARN to uncompress
-    expectedConfigFile.setType(ConfigFile.TypeEnum.ARCHIVE);
-    expectedConfigFile.setSrcFile(
-        getFilePathWithSuffix(stagingDir, remoteUrl, suffix1 + ZIP_EXTENSION));
-    // Relative path in container, but not "." or "./". Use its own name
-    expectedConfigFile.setDestFile(new Path(containerPath).getName());
-    assertConfigFile(expectedConfigFile, files.get(0));
-
-    expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.ARCHIVE);
-    expectedConfigFile.setSrcFile(
-        getFilePathWithSuffix(stagingDir, remoteUrl2, suffix2 + ZIP_EXTENSION));
-    expectedConfigFile.setDestFile(new Path(containerPath2).getName());
-    assertConfigFile(expectedConfigFile, files.get(1));
-
-    expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.ARCHIVE);
-    expectedConfigFile.setSrcFile(
-        getFilePathWithSuffix(stagingDir, remoteUrl, suffix1 + ZIP_EXTENSION));
-    // Relative path in container ".", use remote path name
-    expectedConfigFile.setDestFile(new Path(remoteUrl).getName());
-    assertConfigFile(expectedConfigFile, files.get(2));
-
-    expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.ARCHIVE);
-    expectedConfigFile.setSrcFile(
-        getFilePathWithSuffix(stagingDir, remoteUrl2, suffix2 + ZIP_EXTENSION));
-    // Relative path in container ".", use remote path name
-    expectedConfigFile.setDestFile(new Path(remoteUrl2).getName());
-    assertConfigFile(expectedConfigFile, files.get(3));
-
-    // Ensure mounts env value is correct. Add one mount string
-    String env = serviceSpec.getConfiguration().getEnv()
-        .get("YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS");
-
-    String expectedMounts =
-        new Path(containerPath2).getName() + ":" + containerPath2 + ":rw";
-    assertTrue(env.contains(expectedMounts));
-  }
-
-  /**
-   * Test if file/dir to be localized whose size exceeds limit.
-   * Max 10MB in configuration, mock remote will
-   * always return file size 100MB.
-   * This configuration will fail the job which has remoteUri
-   * But don't impact local dir/file
-   *
-   * --localization https://a/b/1.patch:.
-   * --localization s3a://a/dir:/opt/mys3dir
-   * --localization /temp/script2.py:./
-   */
-  @Test
-  public void testRunJobRemoteUriExceedLocalizationSize() throws Exception {
-    String remoteUri1 = "https://a/b/1.patch";
-    String containerLocal1 = ".";
-    String remoteUri2 = "s3a://a/s3dir";
-    String containerLocal2 = "/opt/mys3dir";
-    String localUri1 = "/temp/script2";
-    String containerLocal3 = "./";
-
-    SubmarineConfiguration submarineConf = new SubmarineConfiguration();
-
-    // Max 10MB, mock remote will always return file size 100MB.
-    submarineConf.set(
-        SubmarineConfiguration.LOCALIZATION_MAX_ALLOWED_FILE_SIZE_MB,
-        "10");
-    mockClientContext.setSubmarineConfig(submarineConf);
-
-    assertFalse(SubmarineLogs.isVerbose());
-
-    // create remote file in local staging dir to simulate
-    Path stagingDir = getStagingDir();
-    testCommons.getFileUtils().createFileInDir(stagingDir, remoteUri1);
-    File remoteDir1 =
-        testCommons.getFileUtils().createDirectory(stagingDir, remoteUri2);
-    testCommons.getFileUtils().createFileInDir(remoteDir1, "afile");
-
-    // create local file, we need to put it under local temp dir
-    File localFile1 = testCommons.getFileUtils().createFileInTempDir(localUri1);
-
-    try {
-      RunJobCli runJobCli = createRunJobCli();
-      String[] params = createCommonParamsBuilder()
-          .withLocalization(remoteUri1, containerLocal1)
-          .build();
-      runJobCli.run(params);
-    } catch (IOException e) {
-      // Shouldn't have exception because it's within file size limit
-      fail();
-    }
-    // we should download because fail fast
-    verifyRdmCopyToRemoteLocalCalls(1);
-    try {
-      String[] params = createCommonParamsBuilder()
-          .withLocalization(remoteUri1, containerLocal1)
-          .withLocalization(remoteUri2, containerLocal2)
-          .withLocalization(localFile1.getAbsolutePath(), containerLocal3)
-          .build();
-
-      reset(spyRdm);
-      RunJobCli runJobCli = createRunJobCliWithoutVerboseAssertion();
-      runJobCli.run(params);
-    } catch (IOException e) {
-      assertTrue(e.getMessage()
-          .contains("104857600 exceeds configured max size:10485760"));
-      // we shouldn't do any download because fail fast
-      verifyRdmCopyToRemoteLocalCalls(0);
-    }
-
-    try {
-      String[] params = createCommonParamsBuilder()
-          .withLocalization(localFile1.getAbsolutePath(), containerLocal3)
-          .build();
-      RunJobCli runJobCli = createRunJobCliWithoutVerboseAssertion();
-      runJobCli.run(params);
-    } catch (IOException e) {
-      assertTrue(e.getMessage()
-          .contains("104857600 exceeds configured max size:10485760"));
-      // we shouldn't do any download because fail fast
-      verifyRdmCopyToRemoteLocalCalls(0);
-    }
-  }
-
-  /**
-   * Test remote Uri doesn't exist.
-   * */
-  @Test
-  public void testRunJobWithNonExistRemoteUri() throws Exception {
-    String remoteUri1 = "hdfs:///a/b/1.patch";
-    String containerLocal1 = ".";
-    String localUri1 = "/a/b/c";
-    String containerLocal2 = "./";
-
-    try {
-      String[] params = createCommonParamsBuilder()
-          .withLocalization(remoteUri1, containerLocal1)
-          .build();
-      RunJobCli runJobCli = createRunJobCli();
-      runJobCli.run(params);
-    } catch (IOException e) {
-      assertTrue(e.getMessage().contains("doesn't exists"));
-    }
-
-    try {
-      String[] params = createCommonParamsBuilder()
-          .withLocalization(localUri1, containerLocal2)
-          .build();
-      RunJobCli runJobCli = createRunJobCliWithoutVerboseAssertion();
-      runJobCli.run(params);
-    } catch (IOException e) {
-      assertTrue(e.getMessage().contains("doesn't exists"));
-    }
-  }
-
-  /**
-   * Test local dir
-   * --localization /user/yarn/mydir:./mydir1
-   * --localization /user/yarn/mydir2:/opt/dir2:rw
-   * --localization /user/yarn/mydir2:.
-   */
-  @Test
-  public void testRunJobWithLocalDirLocalization() throws Exception {
-    String localUrl = "/user/yarn/mydir";
-    String containerPath = "./mydir1";
-    String localUrl2 = "/user/yarn/mydir2";
-    String containerPath2 = "/opt/dir2";
-    String containerPath3 = ".";
-
-    // create local file
-    File localDir1 = testCommons.getFileUtils().createDirInTempDir(localUrl);
-    testCommons.getFileUtils().createFileInDir(localDir1, "1.py");
-    testCommons.getFileUtils().createFileInDir(localDir1, "2.py");
-
-    File localDir2 = testCommons.getFileUtils().createDirInTempDir(localUrl2);
-    testCommons.getFileUtils().createFileInDir(localDir2, "3.py");
-    testCommons.getFileUtils().createFileInDir(localDir2, "4.py");
-
-    String suffix1 = "_" + localDir1.lastModified()
-        + "-" + localDir1.length();
-    String suffix2 = "_" + localDir2.lastModified()
-        + "-" + localDir2.length();
-
-    String[] params = createCommonParamsBuilder()
-        .withLocalization(localDir1.getAbsolutePath(), containerPath)
-        .withLocalization(localDir2.getAbsolutePath(), containerPath2)
-        .withLocalization(localDir2.getAbsolutePath(), containerPath3)
-        .build();
-    RunJobCli runJobCli = createRunJobCli();
-    runJobCli.run(params);
-
-    Service serviceSpec = testCommons.getServiceSpecFromJobSubmitter(
-        runJobCli.getJobSubmitter());
-    assertNumberOfServiceComponents(serviceSpec, 3);
-
-    // we shouldn't do any download
-    verifyRdmCopyToRemoteLocalCalls(0);
-
-    // Ensure local original files are not deleted
-    assertTrue(localDir1.exists());
-    assertTrue(localDir2.exists());
-
-    // Ensure zip file are deleted
-    assertFalse(
-        testCommons.getFileUtils()
-            .getTempFileWithName(localUrl + suffix1 + ZIP_EXTENSION)
-            .exists());
-    assertFalse(
-        testCommons.getFileUtils()
-            .getTempFileWithName(localUrl2 + suffix2 + ZIP_EXTENSION)
-            .exists());
-
-    // Ensure dirs will be zipped and localized
-    List<ConfigFile> files = serviceSpec.getConfiguration().getFiles();
-    assertNumberOfLocalizations(files, 3);
-
-    Path stagingDir = getStagingDir();
-    ConfigFile expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.ARCHIVE);
-    expectedConfigFile.setSrcFile(
-        getFilePathWithSuffix(stagingDir, localUrl, suffix1 + ZIP_EXTENSION));
-    expectedConfigFile.setDestFile(new Path(containerPath).getName());
-    assertConfigFile(expectedConfigFile, files.get(0));
-
-    expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.ARCHIVE);
-    expectedConfigFile.setSrcFile(
-        getFilePathWithSuffix(stagingDir, localUrl2, suffix2 + ZIP_EXTENSION));
-    expectedConfigFile.setDestFile(new Path(containerPath2).getName());
-    assertConfigFile(expectedConfigFile, files.get(1));
-
-    expectedConfigFile = new ConfigFile();
-    expectedConfigFile.setType(ConfigFile.TypeEnum.ARCHIVE);
-    expectedConfigFile.setSrcFile(
-        getFilePathWithSuffix(stagingDir, localUrl2, suffix2 + ZIP_EXTENSION));
-    expectedConfigFile.setDestFile(new Path(localUrl2).getName());
-    assertConfigFile(expectedConfigFile, files.get(2));
-
-    // Ensure mounts env value is correct
-    String env = serviceSpec.getConfiguration().getEnv()
-        .get("YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS");
-    String expectedMounts = new Path(containerPath2).getName()
-        + ":" + containerPath2 + ":rw";
-
-    assertTrue(env.contains(expectedMounts));
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/YarnServiceCliTestUtils.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/YarnServiceCliTestUtils.java
deleted file mode 100644
index e4825eaced5e..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/YarnServiceCliTestUtils.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.client.cli.yarnservice;
-
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.runtimes.RuntimeFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.common.MemorySubmarineStorage;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceRuntimeFactory;
-
-public class YarnServiceCliTestUtils {
-  public static MockClientContext getMockClientContext() {
-    MockClientContext mockClientContext = new MockClientContext();
-    RuntimeFactory runtimeFactory = new YarnServiceRuntimeFactory(
-        mockClientContext);
-    mockClientContext.setRuntimeFactory(runtimeFactory);
-    runtimeFactory.setSubmarineStorage(new MemorySubmarineStorage());
-    return mockClientContext;
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/TestServiceWrapper.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/TestServiceWrapper.java
deleted file mode 100644
index cd5c05c82b16..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/TestServiceWrapper.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.junit.Assert.*;
-import static org.mockito.ArgumentMatchers.eq;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.verify;
-import static org.mockito.Mockito.when;
-
-/**
- * Class to test the {@link ServiceWrapper}.
- */
-public class TestServiceWrapper {
-  private AbstractComponent createMockAbstractComponent(Component mockComponent,
-      String componentName, String localScriptFile) throws IOException {
-    when(mockComponent.getName()).thenReturn(componentName);
-
-    AbstractComponent mockAbstractComponent = mock(AbstractComponent.class);
-    when(mockAbstractComponent.createComponent()).thenReturn(mockComponent);
-    when(mockAbstractComponent.getLocalScriptFile())
-        .thenReturn(localScriptFile);
-    return mockAbstractComponent;
-  }
-
-  @Test
-  public void testWithSingleComponent() throws IOException {
-    Service mockService = mock(Service.class);
-    ServiceWrapper serviceWrapper = new ServiceWrapper(mockService);
-
-    Component mockComponent = mock(Component.class);
-    AbstractComponent mockAbstractComponent =
-        createMockAbstractComponent(mockComponent, "testComponent",
-            "testLocalScriptFile");
-    serviceWrapper.addComponent(mockAbstractComponent);
-
-    verify(mockService).addComponent(eq(mockComponent));
-
-    String launchCommand =
-        serviceWrapper.getLocalLaunchCommandPathForComponent("testComponent");
-    assertEquals("testLocalScriptFile", launchCommand);
-  }
-
-  @Test
-  public void testWithMultipleComponent() throws IOException {
-    Service mockService = mock(Service.class);
-    ServiceWrapper serviceWrapper = new ServiceWrapper(mockService);
-
-    Component mockComponent1 = mock(Component.class);
-    AbstractComponent mockAbstractComponent1 =
-        createMockAbstractComponent(mockComponent1, "testComponent1",
-            "testLocalScriptFile1");
-
-    Component mockComponent2 = mock(Component.class);
-    AbstractComponent mockAbstractComponent2 =
-        createMockAbstractComponent(mockComponent2, "testComponent2",
-            "testLocalScriptFile2");
-
-    serviceWrapper.addComponent(mockAbstractComponent1);
-    serviceWrapper.addComponent(mockAbstractComponent2);
-
-    verify(mockService).addComponent(eq(mockComponent1));
-    verify(mockService).addComponent(eq(mockComponent2));
-
-    String launchCommand1 =
-        serviceWrapper.getLocalLaunchCommandPathForComponent("testComponent1");
-    assertEquals("testLocalScriptFile1", launchCommand1);
-
-    String launchCommand2 =
-        serviceWrapper.getLocalLaunchCommandPathForComponent("testComponent2");
-    assertEquals("testLocalScriptFile2", launchCommand2);
-  }
-
-
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/TestTFConfigGenerator.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/TestTFConfigGenerator.java
deleted file mode 100644
index c8b2388814a3..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/TestTFConfigGenerator.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License. See accompanying LICENSE file.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice;
-
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.TensorFlowCommons;
-import org.codehaus.jettison.json.JSONException;
-import org.junit.Assert;
-import org.junit.Test;
-
-/**
- * Class to test some functionality of {@link TensorFlowCommons}.
- */
-public class TestTFConfigGenerator {
-  @Test
-  public void testSimpleDistributedTFConfigGenerator() throws JSONException {
-    String json = TensorFlowCommons.getTFConfigEnv("worker", 5, 3, "wtan",
-        "tf-job-001", "example.com");
-    String expected =
-        "{\\\"cluster\\\":{\\\"master\\\":[\\\"master-0.wtan.tf-job-001.example.com:8000\\\"],\\\"worker\\\":[\\\"worker-0.wtan.tf-job-001.example.com:8000\\\",\\\"worker-1.wtan.tf-job-001.example.com:8000\\\",\\\"worker-2.wtan.tf-job-001.example.com:8000\\\",\\\"worker-3.wtan.tf-job-001.example.com:8000\\\"],\\\"ps\\\":[\\\"ps-0.wtan.tf-job-001.example.com:8000\\\",\\\"ps-1.wtan.tf-job-001.example.com:8000\\\",\\\"ps-2.wtan.tf-job-001.example.com:8000\\\"]},\\\"task\\\":{ \\\"type\\\":\\\"worker\\\", \\\"index\\\":$_TASK_INDEX},\\\"environment\\\":\\\"cloud\\\"}";
-    Assert.assertEquals(expected, json);
-
-    json = TensorFlowCommons.getTFConfigEnv("ps", 5, 3, "wtan", "tf-job-001",
-        "example.com");
-    expected =
-        "{\\\"cluster\\\":{\\\"master\\\":[\\\"master-0.wtan.tf-job-001.example.com:8000\\\"],\\\"worker\\\":[\\\"worker-0.wtan.tf-job-001.example.com:8000\\\",\\\"worker-1.wtan.tf-job-001.example.com:8000\\\",\\\"worker-2.wtan.tf-job-001.example.com:8000\\\",\\\"worker-3.wtan.tf-job-001.example.com:8000\\\"],\\\"ps\\\":[\\\"ps-0.wtan.tf-job-001.example.com:8000\\\",\\\"ps-1.wtan.tf-job-001.example.com:8000\\\",\\\"ps-2.wtan.tf-job-001.example.com:8000\\\"]},\\\"task\\\":{ \\\"type\\\":\\\"ps\\\", \\\"index\\\":$_TASK_INDEX},\\\"environment\\\":\\\"cloud\\\"}";
-    Assert.assertEquals(expected, json);
-
-    json = TensorFlowCommons.getTFConfigEnv("master", 2, 1, "wtan", "tf-job-001",
-        "example.com");
-    expected =
-        "{\\\"cluster\\\":{\\\"master\\\":[\\\"master-0.wtan.tf-job-001.example.com:8000\\\"],\\\"worker\\\":[\\\"worker-0.wtan.tf-job-001.example.com:8000\\\"],\\\"ps\\\":[\\\"ps-0.wtan.tf-job-001.example.com:8000\\\"]},\\\"task\\\":{ \\\"type\\\":\\\"master\\\", \\\"index\\\":$_TASK_INDEX},\\\"environment\\\":\\\"cloud\\\"}";
-    Assert.assertEquals(expected, json);
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/AbstractTFLaunchCommandTestHelper.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/AbstractTFLaunchCommandTestHelper.java
deleted file mode 100644
index 34ee249d5d36..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/AbstractTFLaunchCommandTestHelper.java
+++ /dev/null
@@ -1,192 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command.TensorBoardLaunchCommand;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command.TensorFlowPsLaunchCommand;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command.TensorFlowWorkerLaunchCommand;
-import org.junit.Rule;
-import org.junit.rules.ExpectedException;
-
-import java.io.File;
-import java.io.IOException;
-import java.nio.charset.Charset;
-import java.nio.file.Files;
-import java.nio.file.Paths;
-import java.util.Arrays;
-import java.util.List;
-
-import static junit.framework.TestCase.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.fail;
-
-/**
- * This class is an abstract base class for testing Tensorboard and TensorFlow
- * launch commands.
- */
-public abstract class AbstractTFLaunchCommandTestHelper {
-  private TensorFlowRole taskType;
-  private boolean useTaskTypeOverride;
-
-  @Rule
-  public ExpectedException expectedException = ExpectedException.none();
-
-  private void assertScriptContainsExportedEnvVar(List<String> fileContents,
-      String varName) {
-    String expected = String.format("export %s=", varName);
-    assertScriptContainsLine(fileContents, expected);
-  }
-
-  protected static void assertScriptContainsExportedEnvVarWithValue(
-      List<String> fileContents, String varName, String value) {
-    String expected = String.format("export %s=%s", varName, value);
-    assertScriptContainsLine(fileContents, expected);
-  }
-
-  protected static void assertScriptContainsLine(List<String> fileContents,
-      String expected) {
-    String message = String.format(
-        "File does not contain expected line '%s'!" + " File contents: %s",
-        expected, Arrays.toString(fileContents.toArray()));
-    assertTrue(message, fileContents.contains(expected));
-  }
-
-  protected static void assertScriptContainsLineWithRegex(
-      List<String> fileContents,
-      String regex) {
-    String message = String.format(
-        "File does not contain expected line '%s'!" + " File contents: %s",
-        regex, Arrays.toString(fileContents.toArray()));
-
-    for (String line : fileContents) {
-      if (line.matches(regex)) {
-        return;
-      }
-    }
-    fail(message);
-  }
-
-  protected static void assertScriptDoesNotContainLine(
-      List<String> fileContents, String expected) {
-    String message = String.format(
-        "File contains unexpected line '%s'!" + " File contents: %s",
-        expected, Arrays.toString(fileContents.toArray()));
-    assertFalse(message, fileContents.contains(expected));
-  }
-
-
-  private AbstractLaunchCommand createLaunchCommandByTaskType(
-      TensorFlowRole taskType, TensorFlowRunJobParameters params)
-      throws IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-    HadoopEnvironmentSetup hadoopEnvSetup =
-        new HadoopEnvironmentSetup(mockClientContext, fsOperations);
-    Component component = new Component();
-    Configuration yarnConfig = new Configuration();
-
-    return createLaunchCommandByTaskTypeInternal(taskType, params,
-        hadoopEnvSetup, component, yarnConfig);
-  }
-
-  private AbstractLaunchCommand createLaunchCommandByTaskTypeInternal(
-      TensorFlowRole taskType, TensorFlowRunJobParameters params,
-      HadoopEnvironmentSetup hadoopEnvSetup, Component component,
-      Configuration yarnConfig)
-      throws IOException {
-    if (taskType == TensorFlowRole.TENSORBOARD) {
-      return new TensorBoardLaunchCommand(
-          hadoopEnvSetup, getTaskType(taskType), component, params);
-    } else if (taskType == TensorFlowRole.WORKER
-        || taskType == TensorFlowRole.PRIMARY_WORKER) {
-      return new TensorFlowWorkerLaunchCommand(
-          hadoopEnvSetup, getTaskType(taskType), component, params, yarnConfig);
-    } else if (taskType == TensorFlowRole.PS) {
-      return new TensorFlowPsLaunchCommand(
-          hadoopEnvSetup, getTaskType(taskType), component, params, yarnConfig);
-    }
-    throw new IllegalStateException("Unknown role!");
-  }
-
-  protected void overrideTaskType(TensorFlowRole taskType) {
-    this.taskType = taskType;
-    this.useTaskTypeOverride = true;
-  }
-
-  private TensorFlowRole getTaskType(TensorFlowRole taskType) {
-    if (useTaskTypeOverride) {
-      return this.taskType;
-    }
-    return taskType;
-  }
-
-  protected void testHdfsRelatedEnvironmentIsUndefined(TensorFlowRole taskType,
-      TensorFlowRunJobParameters params) throws IOException {
-    AbstractLaunchCommand launchCommand =
-        createLaunchCommandByTaskType(taskType, params);
-
-    expectedException.expect(IOException.class);
-    expectedException
-        .expectMessage("Failed to detect HDFS-related environments.");
-    launchCommand.generateLaunchScript();
-  }
-
-  protected List<String> testHdfsRelatedEnvironmentIsDefined(
-      TensorFlowRole taskType, TensorFlowRunJobParameters params)
-      throws IOException {
-    AbstractLaunchCommand launchCommand =
-        createLaunchCommandByTaskType(taskType, params);
-
-    String result = launchCommand.generateLaunchScript();
-    assertNotNull(result);
-    File resultFile = new File(result);
-    assertTrue(resultFile.exists());
-
-    List<String> fileContents = Files.readAllLines(
-        Paths.get(resultFile.toURI()),
-        Charset.forName("UTF-8"));
-
-    assertEquals("#!/bin/bash", fileContents.get(0));
-    assertScriptContainsExportedEnvVar(fileContents, "HADOOP_HOME");
-    assertScriptContainsExportedEnvVar(fileContents, "HADOOP_YARN_HOME");
-    assertScriptContainsExportedEnvVarWithValue(fileContents,
-        "HADOOP_HDFS_HOME", "testHdfsHome");
-    assertScriptContainsExportedEnvVarWithValue(fileContents,
-        "HADOOP_COMMON_HOME", "testHdfsHome");
-    assertScriptContainsExportedEnvVarWithValue(fileContents, "HADOOP_CONF_DIR",
-        "$WORK_DIR");
-    assertScriptContainsExportedEnvVarWithValue(fileContents, "JAVA_HOME",
-        "testJavaHome");
-    assertScriptContainsExportedEnvVarWithValue(fileContents, "LD_LIBRARY_PATH",
-        "$LD_LIBRARY_PATH:$JAVA_HOME/lib/amd64/server");
-    assertScriptContainsExportedEnvVarWithValue(fileContents, "CLASSPATH",
-        "`$HADOOP_HDFS_HOME/bin/hadoop classpath --glob`");
-
-    return fileContents;
-  }
-
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/TestLaunchCommandFactory.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/TestLaunchCommandFactory.java
deleted file mode 100644
index 5b40e0dd5419..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/TestLaunchCommandFactory.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command.TensorBoardLaunchCommand;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command.TensorFlowPsLaunchCommand;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command.TensorFlowWorkerLaunchCommand;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.junit.Assert.assertTrue;
-import static org.mockito.Mockito.mock;
-
-/**
- * This class is to test the {@link TensorFlowLaunchCommandFactory}.
- */
-public class TestLaunchCommandFactory {
-
-  private TensorFlowLaunchCommandFactory createLaunchCommandFactory(
-      TensorFlowRunJobParameters parameters) {
-    HadoopEnvironmentSetup hadoopEnvSetup = mock(HadoopEnvironmentSetup.class);
-    Configuration configuration = mock(Configuration.class);
-    return new TensorFlowLaunchCommandFactory(hadoopEnvSetup, parameters,
-        configuration);
-  }
-
-  @Test
-  public void createLaunchCommandWorkerAndPrimaryWorker() throws IOException {
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setWorkerLaunchCmd("testWorkerLaunchCommand");
-    TensorFlowLaunchCommandFactory launchCommandFactory =
-        createLaunchCommandFactory(parameters);
-    Component mockComponent = mock(Component.class);
-
-    AbstractLaunchCommand launchCommand =
-        launchCommandFactory.createLaunchCommand(TensorFlowRole.PRIMARY_WORKER,
-            mockComponent);
-
-    assertTrue(launchCommand instanceof TensorFlowWorkerLaunchCommand);
-
-    launchCommand =
-        launchCommandFactory.createLaunchCommand(TensorFlowRole.WORKER,
-            mockComponent);
-    assertTrue(launchCommand instanceof TensorFlowWorkerLaunchCommand);
-
-  }
-
-  @Test
-  public void createLaunchCommandPs() throws IOException {
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setPSLaunchCmd("testPSLaunchCommand");
-    TensorFlowLaunchCommandFactory launchCommandFactory =
-        createLaunchCommandFactory(parameters);
-    Component mockComponent = mock(Component.class);
-
-    AbstractLaunchCommand launchCommand =
-        launchCommandFactory.createLaunchCommand(TensorFlowRole.PS,
-            mockComponent);
-
-    assertTrue(launchCommand instanceof TensorFlowPsLaunchCommand);
-  }
-
-  @Test
-  public void createLaunchCommandTensorboard() throws IOException {
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setCheckpointPath("testCheckpointPath");
-    TensorFlowLaunchCommandFactory launchCommandFactory =
-        createLaunchCommandFactory(parameters);
-    Component mockComponent = mock(Component.class);
-
-    AbstractLaunchCommand launchCommand =
-        launchCommandFactory.createLaunchCommand(TensorFlowRole.TENSORBOARD,
-            mockComponent);
-
-    assertTrue(launchCommand instanceof TensorBoardLaunchCommand);
-  }
-
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/TestPyTorchServiceSpec.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/TestPyTorchServiceSpec.java
deleted file mode 100644
index b9512c523cb2..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/TestPyTorchServiceSpec.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.pytorch;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.PyTorchRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.PyTorchRole;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.ServiceWrapper;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.PyTorchLaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component.ComponentTestCommons;
-import org.apache.hadoop.yarn.submarine.utils.Localizer;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.List;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-
-public class TestPyTorchServiceSpec {
-
-  private ComponentTestCommons testCommons =
-      new ComponentTestCommons(PyTorchRole.PRIMARY_WORKER);
-
-  @Before
-  public void setUp() throws IOException {
-    testCommons.setupPyTorch();
-  }
-
-  @Test
-  public void testPytorchServiceSpec() throws IOException {
-    testCommons = new ComponentTestCommons(TensorFlowRole.PRIMARY_WORKER);
-    testCommons.setupTensorFlow();
-
-    PyTorchRunJobParameters parameters = new PyTorchRunJobParameters();
-    parameters.setWorkerResource(testCommons.resource);
-    parameters.setName("testJobName");
-    parameters.setNumWorkers(1);
-    parameters.setWorkerLaunchCmd("testWorkerLaunchCommand");
-
-    MockClientContext mockClientContext = new MockClientContext();
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-
-    HadoopEnvironmentSetup hadoopEnv =
-        new HadoopEnvironmentSetup(mockClientContext, fsOperations);
-
-    PyTorchLaunchCommandFactory launchCommandFactory =
-        new PyTorchLaunchCommandFactory(hadoopEnv, parameters,
-            new Configuration());
-
-    Localizer localizer = new Localizer(fsOperations,
-        mockClientContext.getRemoteDirectoryManager(), parameters);
-
-    PyTorchServiceSpec pyTorchServiceSpec = new PyTorchServiceSpec(parameters,
-        mockClientContext, fsOperations, launchCommandFactory, localizer);
-
-    ServiceWrapper serviceWrapper = pyTorchServiceSpec.create();
-    Service service = serviceWrapper.getService();
-
-    assertNotNull("Service must not be null!", service);
-    List<Component> components = service.getComponents();
-    assertEquals("Number of components is not correct!", 1, components.size());
-
-    Component component = components.get(0);
-    assertEquals(1L, (long) component.getNumberOfContainers());
-    assertEquals("./run-PRIMARY_WORKER.sh", component.getLaunchCommand());
-  }
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TestTensorBoardLaunchCommand.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TestTensorBoardLaunchCommand.java
deleted file mode 100644
index 402f321e5134..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TestTensorBoardLaunchCommand.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command;
-
-import com.google.common.collect.ImmutableList;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.AbstractTFLaunchCommandTestHelper;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.List;
-
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup.DOCKER_HADOOP_HDFS_HOME;
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup.DOCKER_JAVA_HOME;
-
-/**
- * This class is to test the {@link TensorBoardLaunchCommand}.
- */
-public class TestTensorBoardLaunchCommand extends
-    AbstractTFLaunchCommandTestHelper {
-
-  @Test
-  public void testHdfsRelatedEnvironmentIsUndefined() throws IOException {
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setInputPath("hdfs://bla");
-    params.setName("testJobname");
-    params.setCheckpointPath("something");
-
-    testHdfsRelatedEnvironmentIsUndefined(TensorFlowRole.TENSORBOARD,
-        params);
-  }
-
-  @Test
-  public void testHdfsRelatedEnvironmentIsDefined() throws IOException {
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setName("testName");
-    params.setCheckpointPath("testCheckpointPath");
-    params.setInputPath("hdfs://bla");
-    params.setEnvars(ImmutableList.of(
-        DOCKER_HADOOP_HDFS_HOME + "=" + "testHdfsHome",
-        DOCKER_JAVA_HOME + "=" + "testJavaHome"));
-
-    List<String> fileContents =
-        testHdfsRelatedEnvironmentIsDefined(TensorFlowRole.TENSORBOARD,
-            params);
-    assertScriptContainsExportedEnvVarWithValue(fileContents, "LC_ALL",
-        "C && tensorboard --logdir=testCheckpointPath");
-  }
-
-  @Test
-  public void testCheckpointPathUndefined() throws IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-    HadoopEnvironmentSetup hadoopEnvSetup =
-        new HadoopEnvironmentSetup(mockClientContext, fsOperations);
-
-    Component component = new Component();
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setCheckpointPath(null);
-
-    expectedException.expect(NullPointerException.class);
-    expectedException.expectMessage("CheckpointPath must not be null");
-    new TensorBoardLaunchCommand(hadoopEnvSetup, TensorFlowRole.TENSORBOARD,
-            component, params);
-  }
-
-  @Test
-  public void testCheckpointPathEmptyString() throws IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-    HadoopEnvironmentSetup hadoopEnvSetup =
-        new HadoopEnvironmentSetup(mockClientContext, fsOperations);
-
-    Component component = new Component();
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setCheckpointPath("");
-
-    expectedException.expect(IllegalArgumentException.class);
-    expectedException.expectMessage("CheckpointPath must not be empty");
-    new TensorBoardLaunchCommand(hadoopEnvSetup, TensorFlowRole.TENSORBOARD,
-        component, params);
-  }
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TestTensorFlowLaunchCommand.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TestTensorFlowLaunchCommand.java
deleted file mode 100644
index 25ce5f0ea3c0..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TestTensorFlowLaunchCommand.java
+++ /dev/null
@@ -1,253 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.command;
-
-import com.google.common.collect.ImmutableList;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.AbstractTFLaunchCommandTestHelper;
-import org.junit.Test;
-import org.junit.runner.RunWith;
-import org.junit.runners.Parameterized;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup.DOCKER_HADOOP_HDFS_HOME;
-import static org.apache.hadoop.yarn.submarine.runtimes.yarnservice.HadoopEnvironmentSetup.DOCKER_JAVA_HOME;
-
-/**
- * This class is to test the implementors of {@link TensorFlowLaunchCommand}.
- */
-@RunWith(Parameterized.class)
-public class TestTensorFlowLaunchCommand
-    extends AbstractTFLaunchCommandTestHelper {
-  private TensorFlowRole taskType;
-
-  @Parameterized.Parameters
-  public static Collection<Object[]> data() {
-    Collection<Object[]> params = new ArrayList<>();
-    params.add(new Object[]{TensorFlowRole.WORKER });
-    params.add(new Object[]{TensorFlowRole.PS });
-    return params;
-  }
-
-  public TestTensorFlowLaunchCommand(TensorFlowRole taskType) {
-    this.taskType = taskType;
-  }
-
-
-  private void assertScriptContainsLaunchCommand(List<String> fileContents,
-      TensorFlowRunJobParameters params) {
-    String launchCommand = null;
-    if (taskType == TensorFlowRole.WORKER) {
-      launchCommand = params.getWorkerLaunchCmd();
-    } else if (taskType == TensorFlowRole.PS) {
-      launchCommand = params.getPSLaunchCmd();
-    }
-    assertScriptContainsLine(fileContents, launchCommand);
-  }
-
-  private void setLaunchCommandToParams(TensorFlowRunJobParameters params) {
-    if (taskType == TensorFlowRole.WORKER) {
-      params.setWorkerLaunchCmd("testWorkerLaunchCommand");
-    } else if (taskType == TensorFlowRole.PS) {
-      params.setPSLaunchCmd("testPsLaunchCommand");
-    }
-  }
-
-  private void setLaunchCommandToParams(TensorFlowRunJobParameters params,
-      String value) {
-    if (taskType == TensorFlowRole.WORKER) {
-      params.setWorkerLaunchCmd(value);
-    } else if (taskType == TensorFlowRole.PS) {
-      params.setPSLaunchCmd(value);
-    }
-  }
-
-  private void assertTypeInJson(List<String> fileContents) {
-    String expectedType = null;
-    if (taskType == TensorFlowRole.WORKER) {
-      expectedType = "worker";
-    } else if (taskType == TensorFlowRole.PS) {
-      expectedType = "ps";
-    }
-    assertScriptContainsLineWithRegex(fileContents, String.format(".*type.*:" +
-        ".*%s.*", expectedType));
-  }
-
-  private TensorFlowLaunchCommand createTensorFlowLaunchCommandObject(
-      HadoopEnvironmentSetup hadoopEnvSetup, Configuration yarnConfig,
-      Component component, TensorFlowRunJobParameters params)
-      throws IOException {
-    if (taskType == TensorFlowRole.WORKER) {
-      return new TensorFlowWorkerLaunchCommand(hadoopEnvSetup, taskType,
-          component,
-          params, yarnConfig);
-    } else if (taskType == TensorFlowRole.PS) {
-      return new TensorFlowPsLaunchCommand(hadoopEnvSetup, taskType, component,
-          params, yarnConfig);
-    }
-    throw new IllegalStateException("Unknown tasktype!");
-  }
-
-  @Test
-  public void testHdfsRelatedEnvironmentIsUndefined() throws IOException {
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setInputPath("hdfs://bla");
-    params.setName("testJobname");
-    setLaunchCommandToParams(params);
-
-    testHdfsRelatedEnvironmentIsUndefined(taskType, params);
-  }
-
-  @Test
-  public void testHdfsRelatedEnvironmentIsDefined() throws IOException {
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setName("testName");
-    params.setInputPath("hdfs://bla");
-    params.setEnvars(ImmutableList.of(
-        DOCKER_HADOOP_HDFS_HOME + "=" + "testHdfsHome",
-        DOCKER_JAVA_HOME + "=" + "testJavaHome"));
-    setLaunchCommandToParams(params);
-
-    List<String> fileContents =
-        testHdfsRelatedEnvironmentIsDefined(taskType,
-            params);
-    assertScriptContainsLaunchCommand(fileContents, params);
-    assertScriptDoesNotContainLine(fileContents, "export TF_CONFIG=");
-  }
-
-  @Test
-  public void testLaunchCommandIsNull() throws IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-    HadoopEnvironmentSetup hadoopEnvSetup =
-        new HadoopEnvironmentSetup(mockClientContext, fsOperations);
-    Configuration yarnConfig = new Configuration();
-
-    Component component = new Component();
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setName("testName");
-    setLaunchCommandToParams(params, null);
-
-    expectedException.expect(IllegalArgumentException.class);
-    expectedException.expectMessage("LaunchCommand must not be null or empty");
-    TensorFlowLaunchCommand launchCommand =
-        createTensorFlowLaunchCommandObject(hadoopEnvSetup, yarnConfig,
-            component,
-        params);
-    launchCommand.generateLaunchScript();
-  }
-
-  @Test
-  public void testLaunchCommandIsEmpty() throws IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-    HadoopEnvironmentSetup hadoopEnvSetup =
-        new HadoopEnvironmentSetup(mockClientContext, fsOperations);
-    Configuration yarnConfig = new Configuration();
-
-    Component component = new Component();
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setName("testName");
-    setLaunchCommandToParams(params, "");
-
-    expectedException.expect(IllegalArgumentException.class);
-    expectedException.expectMessage("LaunchCommand must not be null or empty");
-    TensorFlowLaunchCommand launchCommand =
-        createTensorFlowLaunchCommandObject(hadoopEnvSetup, yarnConfig,
-            component, params);
-    launchCommand.generateLaunchScript();
-  }
-
-  @Test
-  public void testDistributedTrainingMissingTaskType() throws IOException {
-    overrideTaskType(null);
-
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setDistributed(true);
-    params.setName("testName");
-    params.setInputPath("hdfs://bla");
-    params.setEnvars(ImmutableList.of(
-        DOCKER_HADOOP_HDFS_HOME + "=" + "testHdfsHome",
-        DOCKER_JAVA_HOME + "=" + "testJavaHome"));
-    setLaunchCommandToParams(params);
-
-    expectedException.expect(NullPointerException.class);
-    expectedException.expectMessage("TensorFlowRole must not be null");
-    testHdfsRelatedEnvironmentIsDefined(taskType, params);
-  }
-
-  @Test
-  public void testDistributedTrainingNumberOfWorkersAndPsIsZero()
-      throws IOException {
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setDistributed(true);
-    params.setNumWorkers(0);
-    params.setNumPS(0);
-    params.setName("testName");
-    params.setInputPath("hdfs://bla");
-    params.setEnvars(ImmutableList.of(
-        DOCKER_HADOOP_HDFS_HOME + "=" + "testHdfsHome",
-        DOCKER_JAVA_HOME + "=" + "testJavaHome"));
-    setLaunchCommandToParams(params);
-
-    List<String> fileContents =
-        testHdfsRelatedEnvironmentIsDefined(taskType, params);
-
-    assertScriptDoesNotContainLine(fileContents, "export TF_CONFIG=");
-    assertScriptContainsLineWithRegex(fileContents, ".*worker.*:\\[\\].*");
-    assertScriptContainsLineWithRegex(fileContents, ".*ps.*:\\[\\].*");
-    assertTypeInJson(fileContents);
-  }
-
-  @Test
-  public void testDistributedTrainingNumberOfWorkersAndPsIsNonZero()
-      throws IOException {
-    TensorFlowRunJobParameters params = new TensorFlowRunJobParameters();
-    params.setDistributed(true);
-    params.setNumWorkers(3);
-    params.setNumPS(2);
-    params.setName("testName");
-    params.setInputPath("hdfs://bla");
-    params.setEnvars(ImmutableList.of(
-        DOCKER_HADOOP_HDFS_HOME + "=" + "testHdfsHome",
-        DOCKER_JAVA_HOME + "=" + "testJavaHome"));
-    setLaunchCommandToParams(params);
-
-    List<String> fileContents =
-        testHdfsRelatedEnvironmentIsDefined(taskType, params);
-
-    //assert we have multiple PS and workers
-    assertScriptDoesNotContainLine(fileContents, "export TF_CONFIG=");
-    assertScriptContainsLineWithRegex(fileContents, ".*worker.*:\\[.*,.*\\].*");
-    assertScriptContainsLineWithRegex(fileContents, ".*ps.*:\\[.*,.*\\].*");
-    assertTypeInJson(fileContents);
-  }
-
-
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/ComponentTestCommons.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/ComponentTestCommons.java
deleted file mode 100644
index 9d20a64234b7..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/ComponentTestCommons.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.service.api.ServiceApiConstants;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.submarine.client.cli.runjob.Framework;
-import org.apache.hadoop.yarn.submarine.common.Envs;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.common.api.Role;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.AbstractLaunchCommand;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.LaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.PyTorchLaunchCommandFactory;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.TensorFlowLaunchCommandFactory;
-
-import java.io.IOException;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.mockito.ArgumentMatchers.any;
-import static org.mockito.ArgumentMatchers.eq;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-/**
- * This class has some helper methods and fields
- * in order to test TensorFlow-related Components easier.
- */
-public class ComponentTestCommons {
-  String userName;
-  Role role;
-  LaunchCommandFactory mockLaunchCommandFactory;
-  FileSystemOperations fsOperations;
-  public MockClientContext mockClientContext;
-  Configuration yarnConfig;
-  public Resource resource;
-
-  public ComponentTestCommons(Role role) {
-    this.role = role;
-  }
-
-  public void setupTensorFlow() throws IOException {
-    setupInternal(Framework.TENSORFLOW);
-  }
-
-  public void setupPyTorch() throws IOException {
-    setupInternal(Framework.PYTORCH);
-  }
-
-  private void setupInternal(Framework framework) throws IOException {
-    this.userName = System.getProperty("user.name");
-    this.resource = Resource.newInstance(4000, 10);
-    setupDependencies(framework);
-  }
-
-  private void setupDependencies(Framework framework) throws IOException {
-    fsOperations = mock(FileSystemOperations.class);
-    mockClientContext = new MockClientContext();
-
-    if (framework == Framework.TENSORFLOW) {
-      mockLaunchCommandFactory = mock(TensorFlowLaunchCommandFactory.class);
-    } else if (framework == Framework.PYTORCH) {
-      mockLaunchCommandFactory = mock(PyTorchLaunchCommandFactory.class);
-    } else {
-      throw new UnsupportedOperationException(
-          "Unsupported framework type specified!");
-    }
-
-    AbstractLaunchCommand mockLaunchCommand = mock(AbstractLaunchCommand.class);
-    when(mockLaunchCommand.generateLaunchScript()).thenReturn("mockScript");
-    when(mockLaunchCommandFactory.createLaunchCommand(eq(role),
-        any(Component.class))).thenReturn(mockLaunchCommand);
-
-    yarnConfig = new Configuration();
-  }
-
-  void verifyCommonConfigEnvs(Component component) {
-    assertNotNull(component.getConfiguration().getEnv());
-    assertEquals(2, component.getConfiguration().getEnv().size());
-    assertEquals(ServiceApiConstants.COMPONENT_ID,
-        component.getConfiguration().getEnv().get(Envs.TASK_INDEX_ENV));
-    assertEquals(role.getName(),
-        component.getConfiguration().getEnv().get(Envs.TASK_TYPE_ENV));
-  }
-
-  void verifyResources(Component component) {
-    assertNotNull(component.getResource());
-    assertEquals(10, (int) component.getResource().getCpus());
-    assertEquals(4000,
-        (int) Integer.valueOf(component.getResource().getMemory()));
-  }
-}
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorBoardComponent.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorBoardComponent.java
deleted file mode 100644
index 9a933b775ba4..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorBoardComponent.java
+++ /dev/null
@@ -1,127 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.yarn.service.api.records.Artifact;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.Component.RestartPolicyEnum;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.TensorFlowLaunchCommandFactory;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-
-import java.io.IOException;
-
-import static org.junit.Assert.assertEquals;
-import static org.mockito.ArgumentMatchers.any;
-import static org.mockito.ArgumentMatchers.eq;
-import static org.mockito.Mockito.verify;
-
-/**
- * This class is to test {@link TensorBoardComponent}.
- */
-public class TestTensorBoardComponent {
-
-  @Rule
-  public ExpectedException expectedException = ExpectedException.none();
-  private ComponentTestCommons testCommons =
-      new ComponentTestCommons(TensorFlowRole.TENSORBOARD);
-
-  @Before
-  public void setUp() throws IOException {
-    testCommons.setupTensorFlow();
-  }
-
-  private TensorBoardComponent createTensorBoardComponent(
-      RunJobParameters parameters) {
-    return new TensorBoardComponent(
-        testCommons.fsOperations,
-        testCommons.mockClientContext.getRemoteDirectoryManager(),
-        parameters,
-        (TensorFlowLaunchCommandFactory) testCommons.mockLaunchCommandFactory,
-        testCommons.yarnConfig);
-  }
-
-  @Test
-  public void testTensorBoardComponentWithNullResource() throws IOException {
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setTensorboardResource(null);
-
-    TensorBoardComponent tensorBoardComponent =
-        createTensorBoardComponent(parameters);
-
-    expectedException.expect(NullPointerException.class);
-    expectedException.expectMessage("TensorBoard resource must not be null");
-    tensorBoardComponent.createComponent();
-  }
-
-  @Test
-  public void testTensorBoardComponentWithNullJobName() throws IOException {
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setTensorboardResource(testCommons.resource);
-    parameters.setName(null);
-
-    TensorBoardComponent tensorBoardComponent =
-        createTensorBoardComponent(parameters);
-
-    expectedException.expect(NullPointerException.class);
-    expectedException.expectMessage("Job name must not be null");
-    tensorBoardComponent.createComponent();
-  }
-
-  @Test
-  public void testTensorBoardComponent() throws IOException {
-    testCommons.yarnConfig.set("hadoop.registry.dns.domain-name", "testDomain");
-
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setTensorboardResource(testCommons.resource);
-    parameters.setName("testJobName");
-    parameters.setTensorboardDockerImage("testTBDockerImage");
-
-    TensorBoardComponent tensorBoardComponent =
-        createTensorBoardComponent(parameters);
-
-    Component component = tensorBoardComponent.createComponent();
-
-    assertEquals(testCommons.role.getComponentName(), component.getName());
-    testCommons.verifyCommonConfigEnvs(component);
-
-    assertEquals(1L, (long) component.getNumberOfContainers());
-    assertEquals(RestartPolicyEnum.NEVER, component.getRestartPolicy());
-    testCommons.verifyResources(component);
-    assertEquals(
-        new Artifact().type(Artifact.TypeEnum.DOCKER).id("testTBDockerImage"),
-        component.getArtifact());
-
-    assertEquals(String.format(
-        "http://tensorboard-0.testJobName.%s" + ".testDomain:6006",
-        testCommons.userName),
-        tensorBoardComponent.getTensorboardLink());
-
-    assertEquals("./run-TENSORBOARD.sh", component.getLaunchCommand());
-    verify(testCommons.fsOperations)
-        .uploadToRemoteFileAndLocalizeToContainerWorkDir(
-        any(Path.class), eq("mockScript"), eq("run-TENSORBOARD.sh"),
-        eq(component));
-  }
-
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorFlowPsComponent.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorFlowPsComponent.java
deleted file mode 100644
index cbf3dc2bf5b6..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorFlowPsComponent.java
+++ /dev/null
@@ -1,168 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component;
-
-import static junit.framework.TestCase.assertTrue;
-import static org.junit.Assert.assertEquals;
-import static org.mockito.ArgumentMatchers.any;
-import static org.mockito.ArgumentMatchers.eq;
-import static org.mockito.Mockito.verify;
-import java.io.IOException;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.yarn.service.api.records.Artifact;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.Component.RestartPolicyEnum;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.TensorFlowLaunchCommandFactory;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-
-/**
- * This class is to test {@link TensorFlowPsComponent}.
- */
-public class TestTensorFlowPsComponent {
-
-  @Rule
-  public ExpectedException expectedException = ExpectedException.none();
-  private ComponentTestCommons testCommons =
-      new ComponentTestCommons(TensorFlowRole.PS);
-
-  @Before
-  public void setUp() throws IOException {
-    testCommons.setupTensorFlow();
-  }
-
-  private TensorFlowPsComponent createPsComponent(RunJobParameters parameters) {
-    return new TensorFlowPsComponent(
-        testCommons.fsOperations,
-        testCommons.mockClientContext.getRemoteDirectoryManager(),
-        (TensorFlowLaunchCommandFactory) testCommons.mockLaunchCommandFactory,
-        parameters,
-        testCommons.yarnConfig);
-  }
-
-  private void verifyCommons(Component component) throws IOException {
-    assertEquals(testCommons.role.getComponentName(), component.getName());
-    testCommons.verifyCommonConfigEnvs(component);
-
-    assertTrue(component.getConfiguration().getProperties().isEmpty());
-
-    assertEquals(RestartPolicyEnum.NEVER, component.getRestartPolicy());
-    testCommons.verifyResources(component);
-    assertEquals(
-        new Artifact().type(Artifact.TypeEnum.DOCKER).id("testPSDockerImage"),
-        component.getArtifact());
-
-    String taskTypeUppercase = testCommons.role.getName().toUpperCase();
-    String expectedScriptName = String.format("run-%s.sh", taskTypeUppercase);
-    assertEquals(String.format("./%s", expectedScriptName),
-        component.getLaunchCommand());
-    verify(testCommons.fsOperations)
-        .uploadToRemoteFileAndLocalizeToContainerWorkDir(
-        any(Path.class), eq("mockScript"), eq(expectedScriptName),
-        eq(component));
-  }
-
-  @Test
-  public void testPSComponentWithNullResource() throws IOException {
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setPsResource(null);
-
-    TensorFlowPsComponent psComponent =
-        createPsComponent(parameters);
-
-    expectedException.expect(NullPointerException.class);
-    expectedException.expectMessage("PS resource must not be null");
-    psComponent.createComponent();
-  }
-
-  @Test
-  public void testPSComponentWithNullJobName() throws IOException {
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setPsResource(testCommons.resource);
-    parameters.setNumPS(1);
-    parameters.setName(null);
-
-    TensorFlowPsComponent psComponent =
-        createPsComponent(parameters);
-
-    expectedException.expect(NullPointerException.class);
-    expectedException.expectMessage("Job name must not be null");
-    psComponent.createComponent();
-  }
-
-  @Test
-  public void testPSComponentZeroNumberOfPS() throws IOException {
-    testCommons.yarnConfig.set("hadoop.registry.dns.domain-name", "testDomain");
-
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setPsResource(testCommons.resource);
-    parameters.setName("testJobName");
-    parameters.setPsDockerImage("testPSDockerImage");
-    parameters.setNumPS(0);
-
-    TensorFlowPsComponent psComponent =
-        createPsComponent(parameters);
-
-    expectedException.expect(IllegalArgumentException.class);
-    expectedException.expectMessage("Number of PS should be at least 1!");
-    psComponent.createComponent();
-  }
-
-  @Test
-  public void testPSComponentNumPSIsOne() throws IOException {
-    testCommons.yarnConfig.set("hadoop.registry.dns.domain-name", "testDomain");
-
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setPsResource(testCommons.resource);
-    parameters.setName("testJobName");
-    parameters.setNumPS(1);
-    parameters.setPsDockerImage("testPSDockerImage");
-
-    TensorFlowPsComponent psComponent =
-        createPsComponent(parameters);
-
-    Component component = psComponent.createComponent();
-
-    assertEquals(1L, (long) component.getNumberOfContainers());
-    verifyCommons(component);
-  }
-
-  @Test
-  public void testPSComponentNumPSIsTwo() throws IOException {
-    testCommons.yarnConfig.set("hadoop.registry.dns.domain-name", "testDomain");
-
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setPsResource(testCommons.resource);
-    parameters.setName("testJobName");
-    parameters.setNumPS(2);
-    parameters.setPsDockerImage("testPSDockerImage");
-
-    TensorFlowPsComponent psComponent =
-        createPsComponent(parameters);
-
-    Component component = psComponent.createComponent();
-
-    assertEquals(2L, (long) component.getNumberOfContainers());
-    verifyCommons(component);
-  }
-
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorFlowWorkerComponent.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorFlowWorkerComponent.java
deleted file mode 100644
index d75aff9057e8..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorFlowWorkerComponent.java
+++ /dev/null
@@ -1,218 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.runtimes.yarnservice.tensorflow.component;
-
-import com.google.common.collect.ImmutableMap;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.yarn.service.api.records.Artifact;
-import org.apache.hadoop.yarn.service.api.records.Component;
-import org.apache.hadoop.yarn.service.api.records.Component.RestartPolicyEnum;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.TensorFlowRunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.api.TensorFlowRole;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.command.TensorFlowLaunchCommandFactory;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-
-import java.io.IOException;
-import java.util.Map;
-
-import static junit.framework.TestCase.assertTrue;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.mockito.ArgumentMatchers.any;
-import static org.mockito.ArgumentMatchers.eq;
-import static org.mockito.Mockito.verify;
-
-/**
- * This class is to test {@link TensorFlowWorkerComponent}.
- */
-public class TestTensorFlowWorkerComponent {
-
-  @Rule
-  public ExpectedException expectedException = ExpectedException.none();
-  private ComponentTestCommons testCommons =
-      new ComponentTestCommons(TensorFlowRole.TENSORBOARD);
-
-  @Before
-  public void setUp() throws IOException {
-    testCommons.setupTensorFlow();
-  }
-
-  private TensorFlowWorkerComponent createWorkerComponent(
-      TensorFlowRunJobParameters parameters) {
-    return new TensorFlowWorkerComponent(
-        testCommons.fsOperations,
-        testCommons.mockClientContext.getRemoteDirectoryManager(),
-        parameters, testCommons.role,
-        (TensorFlowLaunchCommandFactory) testCommons.mockLaunchCommandFactory,
-        testCommons.yarnConfig);
-  }
-
-  private void verifyCommons(Component component) throws IOException {
-    verifyCommonsInternal(component, ImmutableMap.of());
-  }
-
-  private void verifyCommons(Component component,
-      Map<String, String> expectedProperties) throws IOException {
-    verifyCommonsInternal(component, expectedProperties);
-  }
-
-  private void verifyCommonsInternal(Component component,
-      Map<String, String> expectedProperties) throws IOException {
-    assertEquals(testCommons.role.getComponentName(), component.getName());
-    testCommons.verifyCommonConfigEnvs(component);
-
-    Map<String, String> actualProperties =
-        component.getConfiguration().getProperties();
-    if (!expectedProperties.isEmpty()) {
-      assertFalse(actualProperties.isEmpty());
-      expectedProperties.forEach(
-          (k, v) -> assertEquals(v, actualProperties.get(k)));
-    } else {
-      assertTrue(actualProperties.isEmpty());
-    }
-
-    assertEquals(RestartPolicyEnum.NEVER, component.getRestartPolicy());
-    testCommons.verifyResources(component);
-    assertEquals(
-        new Artifact().type(Artifact.TypeEnum.DOCKER)
-            .id("testWorkerDockerImage"),
-        component.getArtifact());
-
-    String taskTypeUppercase = testCommons.role.getName().toUpperCase();
-    String expectedScriptName = String.format("run-%s.sh", taskTypeUppercase);
-    assertEquals(String.format("./%s", expectedScriptName),
-        component.getLaunchCommand());
-    verify(testCommons.fsOperations)
-        .uploadToRemoteFileAndLocalizeToContainerWorkDir(
-        any(Path.class), eq("mockScript"), eq(expectedScriptName),
-        eq(component));
-  }
-
-  @Test
-  public void testWorkerComponentWithNullResource() throws IOException {
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setWorkerResource(null);
-
-    TensorFlowWorkerComponent workerComponent =
-        createWorkerComponent(parameters);
-
-    expectedException.expect(NullPointerException.class);
-    expectedException.expectMessage("Worker resource must not be null");
-    workerComponent.createComponent();
-  }
-
-  @Test
-  public void testWorkerComponentWithNullJobName() throws IOException {
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setWorkerResource(testCommons.resource);
-    parameters.setNumWorkers(1);
-    parameters.setName(null);
-
-    TensorFlowWorkerComponent workerComponent =
-        createWorkerComponent(parameters);
-
-    expectedException.expect(NullPointerException.class);
-    expectedException.expectMessage("Job name must not be null");
-    workerComponent.createComponent();
-  }
-
-  @Test
-  public void testNormalWorkerComponentZeroNumberOfWorkers()
-      throws IOException {
-    testCommons.yarnConfig.set("hadoop.registry.dns.domain-name", "testDomain");
-
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setWorkerResource(testCommons.resource);
-    parameters.setName("testJobName");
-    parameters.setWorkerDockerImage("testWorkerDockerImage");
-    parameters.setNumWorkers(0);
-
-    TensorFlowWorkerComponent workerComponent =
-        createWorkerComponent(parameters);
-
-    expectedException.expect(IllegalArgumentException.class);
-    expectedException.expectMessage("Number of workers should be at least 1!");
-    workerComponent.createComponent();
-  }
-
-  @Test
-  public void testNormalWorkerComponentNumWorkersIsOne() throws IOException {
-    testCommons.yarnConfig.set("hadoop.registry.dns.domain-name", "testDomain");
-
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setWorkerResource(testCommons.resource);
-    parameters.setName("testJobName");
-    parameters.setNumWorkers(1);
-    parameters.setWorkerDockerImage("testWorkerDockerImage");
-
-    TensorFlowWorkerComponent workerComponent =
-        createWorkerComponent(parameters);
-
-    Component component = workerComponent.createComponent();
-
-    assertEquals(0L, (long) component.getNumberOfContainers());
-    verifyCommons(component);
-  }
-
-  @Test
-  public void testNormalWorkerComponentNumWorkersIsTwo() throws IOException {
-    testCommons.yarnConfig.set("hadoop.registry.dns.domain-name", "testDomain");
-
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setWorkerResource(testCommons.resource);
-    parameters.setName("testJobName");
-    parameters.setNumWorkers(2);
-    parameters.setWorkerDockerImage("testWorkerDockerImage");
-
-    TensorFlowWorkerComponent workerComponent =
-        createWorkerComponent(parameters);
-
-    Component component = workerComponent.createComponent();
-
-    assertEquals(1L, (long) component.getNumberOfContainers());
-    verifyCommons(component);
-  }
-
-  @Test
-  public void testPrimaryWorkerComponentNumWorkersIsTwo() throws IOException {
-    testCommons.yarnConfig.set("hadoop.registry.dns.domain-name", "testDomain");
-    testCommons = new ComponentTestCommons(TensorFlowRole.PRIMARY_WORKER);
-    testCommons.setupTensorFlow();
-
-    TensorFlowRunJobParameters parameters = new TensorFlowRunJobParameters();
-    parameters.setWorkerResource(testCommons.resource);
-    parameters.setName("testJobName");
-    parameters.setNumWorkers(2);
-    parameters.setWorkerDockerImage("testWorkerDockerImage");
-
-    TensorFlowWorkerComponent workerComponent =
-        createWorkerComponent(parameters);
-
-    Component component = workerComponent.createComponent();
-
-    assertEquals(1L, (long) component.getNumberOfContainers());
-    // If the dependencies are upgraded to hadoop 3.3.0.
-    // yarn.service.container-state-report-as-service-state can be replaced
-    // with CONTAINER_STATE_REPORT_AS_SERVICE_STATE
-    verifyCommons(component, ImmutableMap.of(
-        "yarn.service.container-state-report-as-service-state", "true"));
-  }
-
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestClassPathUtilities.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestClassPathUtilities.java
deleted file mode 100644
index 8fdb4753e83f..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestClassPathUtilities.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import org.apache.hadoop.yarn.submarine.FileUtilitiesForTests;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.File;
-import java.io.IOException;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
-
-/**
- * This class is to test {@link ClassPathUtilities}.
- */
-public class TestClassPathUtilities {
-
-  private static final String CLASSPATH_KEY = "java.class.path";
-  private FileUtilitiesForTests fileUtils = new FileUtilitiesForTests();
-  private static String originalClasspath;
-
-  @BeforeClass
-  public static void setUpClass() {
-    originalClasspath = System.getProperty(CLASSPATH_KEY);
-  }
-
-  @Before
-  public void setUp() {
-    fileUtils.setup();
-  }
-
-  @After
-  public void teardown() throws IOException {
-    fileUtils.teardown();
-    System.setProperty(CLASSPATH_KEY, originalClasspath);
-  }
-
-  private static void addFileToClasspath(File file) {
-    String newClasspath = originalClasspath + ":" + file.getAbsolutePath();
-    System.setProperty(CLASSPATH_KEY, newClasspath);
-  }
-
-  @Test
-  public void findFileNotInClasspath() {
-    File resultFile = ClassPathUtilities.findFileOnClassPath("bla");
-    assertNull(resultFile);
-  }
-
-  @Test
-  public void findFileOnClasspath() throws Exception {
-    File testFile = fileUtils.createFileInTempDir("testFile");
-
-    addFileToClasspath(testFile);
-    File resultFile = ClassPathUtilities.findFileOnClassPath("testFile");
-
-    assertNotNull(resultFile);
-    assertEquals(testFile.getAbsolutePath(), resultFile.getAbsolutePath());
-  }
-
-  @Test
-  public void findDirectoryOnClasspath() throws Exception {
-    File testDir = fileUtils.createDirInTempDir("testDir");
-    File testFile = fileUtils.createFileInDir(testDir, "testFile");
-
-    addFileToClasspath(testDir);
-    File resultFile = ClassPathUtilities.findFileOnClassPath("testFile");
-
-    assertNotNull(resultFile);
-    assertEquals(testFile.getAbsolutePath(), resultFile.getAbsolutePath());
-  }
-
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestEnvironmentUtilities.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestEnvironmentUtilities.java
deleted file mode 100644
index a52c1cfe89e2..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestEnvironmentUtilities.java
+++ /dev/null
@@ -1,231 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import com.google.common.collect.ImmutableList;
-import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.Maps;
-import org.apache.hadoop.yarn.service.api.records.Configuration;
-import org.apache.hadoop.yarn.service.api.records.Service;
-import org.junit.Test;
-
-import java.util.Map;
-
-import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION;
-import static org.apache.hadoop.yarn.submarine.utils.EnvironmentUtilities.ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME;
-import static org.junit.Assert.*;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-/**
- * This class is to test {@link EnvironmentUtilities}.
- */
-public class TestEnvironmentUtilities {
-  private Service createServiceWithEmptyEnvVars() {
-    return createServiceWithEnvVars(Maps.newHashMap());
-  }
-
-  private Service createServiceWithEnvVars(Map<String, String> envVars) {
-    Service service = mock(Service.class);
-    Configuration config = mock(Configuration.class);
-    when(config.getEnv()).thenReturn(envVars);
-    when(service.getConfiguration()).thenReturn(config);
-
-    return service;
-  }
-
-  private void validateDefaultEnvVars(Map<String, String> resultEnvs) {
-    assertEquals("/etc/passwd:/etc/passwd:ro",
-        resultEnvs.get(ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME));
-  }
-
-  private org.apache.hadoop.conf.Configuration
-  createYarnConfigWithSecurityValue(String value) {
-    org.apache.hadoop.conf.Configuration mockConfig =
-        mock(org.apache.hadoop.conf.Configuration.class);
-    when(mockConfig.get(HADOOP_SECURITY_AUTHENTICATION)).thenReturn(value);
-    return mockConfig;
-  }
-
-  @Test
-  public void testGetValueOfNullEnvVar() {
-    assertEquals("", EnvironmentUtilities.getValueOfEnvironment(null));
-  }
-
-  @Test
-  public void testGetValueOfEmptyEnvVar() {
-    assertEquals("", EnvironmentUtilities.getValueOfEnvironment(""));
-  }
-
-  @Test
-  public void testGetValueOfEnvVarJustAnEqualsSign() {
-    assertEquals("", EnvironmentUtilities.getValueOfEnvironment("="));
-  }
-
-  @Test
-  public void testGetValueOfEnvVarWithoutValue() {
-    assertEquals("", EnvironmentUtilities.getValueOfEnvironment("a="));
-  }
-
-  @Test
-  public void testGetValueOfEnvVarValidFormat() {
-    assertEquals("bbb", EnvironmentUtilities.getValueOfEnvironment("a=bbb"));
-  }
-
-  @Test
-  public void testHandleServiceEnvWithNullMap() {
-    Service service = createServiceWithEmptyEnvVars();
-    org.apache.hadoop.conf.Configuration yarnConfig =
-        mock(org.apache.hadoop.conf.Configuration.class);
-    EnvironmentUtilities.handleServiceEnvs(service, yarnConfig, null);
-
-    Map<String, String> resultEnvs = service.getConfiguration().getEnv();
-    assertEquals(1, resultEnvs.size());
-    validateDefaultEnvVars(resultEnvs);
-  }
-
-  @Test
-  public void testHandleServiceEnvWithEmptyMap() {
-    Service service = createServiceWithEmptyEnvVars();
-    org.apache.hadoop.conf.Configuration yarnConfig =
-        mock(org.apache.hadoop.conf.Configuration.class);
-    EnvironmentUtilities.handleServiceEnvs(service, yarnConfig, null);
-
-    Map<String, String> resultEnvs = service.getConfiguration().getEnv();
-    assertEquals(1, resultEnvs.size());
-    validateDefaultEnvVars(resultEnvs);
-  }
-
-  @Test
-  public void testHandleServiceEnvWithYarnConfigSecurityValueNonKerberos() {
-    Service service = createServiceWithEmptyEnvVars();
-    org.apache.hadoop.conf.Configuration yarnConfig =
-        createYarnConfigWithSecurityValue("nonkerberos");
-    EnvironmentUtilities.handleServiceEnvs(service, yarnConfig, null);
-
-    Map<String, String> resultEnvs = service.getConfiguration().getEnv();
-    assertEquals(1, resultEnvs.size());
-    validateDefaultEnvVars(resultEnvs);
-  }
-
-  @Test
-  public void testHandleServiceEnvWithYarnConfigSecurityValueKerberos() {
-    Service service = createServiceWithEmptyEnvVars();
-    org.apache.hadoop.conf.Configuration yarnConfig =
-        createYarnConfigWithSecurityValue("kerberos");
-    EnvironmentUtilities.handleServiceEnvs(service, yarnConfig, null);
-
-    Map<String, String> resultEnvs = service.getConfiguration().getEnv();
-    assertEquals(1, resultEnvs.size());
-    assertEquals("/etc/passwd:/etc/passwd:ro,/etc/krb5.conf:/etc/krb5.conf:ro",
-        resultEnvs.get(ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME));
-  }
-
-  @Test
-  public void testHandleServiceEnvWithExistingEnvsAndValidNewEnvs() {
-    Map<String, String> existingEnvs = Maps.newHashMap(
-        ImmutableMap.<String, String>builder().
-            put("a", "1").
-            put("b", "2").
-            build());
-    ImmutableList<String> newEnvs = ImmutableList.of("c=3", "d=4");
-
-    Service service = createServiceWithEnvVars(existingEnvs);
-    org.apache.hadoop.conf.Configuration yarnConfig =
-        createYarnConfigWithSecurityValue("kerberos");
-    EnvironmentUtilities.handleServiceEnvs(service, yarnConfig, newEnvs);
-
-    Map<String, String> resultEnvs = service.getConfiguration().getEnv();
-    assertEquals(5, resultEnvs.size());
-    assertEquals("/etc/passwd:/etc/passwd:ro,/etc/krb5.conf:/etc/krb5.conf:ro",
-        resultEnvs.get(ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME));
-    assertEquals("1", resultEnvs.get("a"));
-    assertEquals("2", resultEnvs.get("b"));
-    assertEquals("3", resultEnvs.get("c"));
-    assertEquals("4", resultEnvs.get("d"));
-  }
-
-  @Test
-  public void testHandleServiceEnvWithExistingEnvsAndNewEnvsWithoutEquals() {
-    Map<String, String> existingEnvs = Maps.newHashMap(
-        ImmutableMap.<String, String>builder().
-            put("a", "1").
-            put("b", "2").
-            build());
-    ImmutableList<String> newEnvs = ImmutableList.of("c3", "d4");
-
-    Service service = createServiceWithEnvVars(existingEnvs);
-    org.apache.hadoop.conf.Configuration yarnConfig =
-        createYarnConfigWithSecurityValue("kerberos");
-    EnvironmentUtilities.handleServiceEnvs(service, yarnConfig, newEnvs);
-
-    Map<String, String> resultEnvs = service.getConfiguration().getEnv();
-    assertEquals(5, resultEnvs.size());
-    assertEquals("/etc/passwd:/etc/passwd:ro,/etc/krb5.conf:/etc/krb5.conf:ro",
-        resultEnvs.get(ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME));
-    assertEquals("1", resultEnvs.get("a"));
-    assertEquals("2", resultEnvs.get("b"));
-    assertEquals("", resultEnvs.get("c3"));
-    assertEquals("", resultEnvs.get("d4"));
-  }
-
-  @Test
-  public void testHandleServiceEnvWithExistingEnvVarKey() {
-    Map<String, String> existingEnvs = Maps.newHashMap(
-        ImmutableMap.<String, String>builder().
-            put("a", "1").
-            put("b", "2").
-            build());
-    ImmutableList<String> newEnvs = ImmutableList.of("a=33", "c=44");
-
-    Service service = createServiceWithEnvVars(existingEnvs);
-    org.apache.hadoop.conf.Configuration yarnConfig =
-        createYarnConfigWithSecurityValue("kerberos");
-    EnvironmentUtilities.handleServiceEnvs(service, yarnConfig, newEnvs);
-
-    Map<String, String> resultEnvs = service.getConfiguration().getEnv();
-    assertEquals(4, resultEnvs.size());
-    assertEquals("/etc/passwd:/etc/passwd:ro,/etc/krb5.conf:/etc/krb5.conf:ro",
-        resultEnvs.get(ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME));
-    assertEquals("1:33", resultEnvs.get("a"));
-    assertEquals("2", resultEnvs.get("b"));
-    assertEquals("44", resultEnvs.get("c"));
-  }
-
-  @Test
-  public void testHandleServiceEnvWithExistingEnvVarKeyMultipleTimes() {
-    Map<String, String> existingEnvs = Maps.newHashMap(
-        ImmutableMap.<String, String>builder().
-            put("a", "1").
-            put("b", "2").
-            build());
-    ImmutableList<String> newEnvs = ImmutableList.of("a=33", "a=44");
-
-    Service service = createServiceWithEnvVars(existingEnvs);
-    org.apache.hadoop.conf.Configuration yarnConfig =
-        createYarnConfigWithSecurityValue("kerberos");
-    EnvironmentUtilities.handleServiceEnvs(service, yarnConfig, newEnvs);
-
-    Map<String, String> resultEnvs = service.getConfiguration().getEnv();
-    assertEquals(3, resultEnvs.size());
-    assertEquals("/etc/passwd:/etc/passwd:ro,/etc/krb5.conf:/etc/krb5.conf:ro",
-        resultEnvs.get(ENV_DOCKER_MOUNTS_FOR_CONTAINER_RUNTIME));
-    assertEquals("1:33:44", resultEnvs.get("a"));
-    assertEquals("2", resultEnvs.get("b"));
-  }
-
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestKerberosPrincipalFactory.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestKerberosPrincipalFactory.java
deleted file mode 100644
index f08bce32584b..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestKerberosPrincipalFactory.java
+++ /dev/null
@@ -1,156 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;
-import org.apache.hadoop.yarn.submarine.FileUtilitiesForTests;
-import org.apache.hadoop.yarn.submarine.client.cli.param.runjob.RunJobParameters;
-import org.apache.hadoop.yarn.submarine.common.MockClientContext;
-import org.apache.hadoop.yarn.submarine.runtimes.yarnservice.FileSystemOperations;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.File;
-import java.io.IOException;
-
-import static org.junit.Assert.*;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-/**
- * This class is to test {@link KerberosPrincipalFactory}.
- */
-public class TestKerberosPrincipalFactory {
-  private FileUtilitiesForTests fileUtils = new FileUtilitiesForTests();
-
-  @Before
-  public void setUp() {
-    fileUtils.setup();
-  }
-
-  @After
-  public void teardown() throws IOException {
-    fileUtils.teardown();
-  }
-
-  private File createKeytabFile(String keytabFileName) throws IOException {
-    return fileUtils.createFileInTempDir(keytabFileName);
-  }
-
-  @Test
-  public void testCreatePrincipalEmptyPrincipalAndKeytab() throws IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-
-    RunJobParameters parameters = mock(RunJobParameters.class);
-    when(parameters.getPrincipal()).thenReturn("");
-    when(parameters.getKeytab()).thenReturn("");
-
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-    KerberosPrincipal result =
-        KerberosPrincipalFactory.create(fsOperations,
-            mockClientContext.getRemoteDirectoryManager(), parameters);
-
-    assertNull(result);
-  }
-  @Test
-  public void testCreatePrincipalEmptyPrincipalString() throws IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-
-    RunJobParameters parameters = mock(RunJobParameters.class);
-    when(parameters.getPrincipal()).thenReturn("");
-    when(parameters.getKeytab()).thenReturn("keytab");
-
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-    KerberosPrincipal result =
-        KerberosPrincipalFactory.create(fsOperations,
-            mockClientContext.getRemoteDirectoryManager(), parameters);
-
-    assertNull(result);
-  }
-
-  @Test
-  public void testCreatePrincipalEmptyKeyTabString() throws IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-
-    RunJobParameters parameters = mock(RunJobParameters.class);
-    when(parameters.getPrincipal()).thenReturn("principal");
-    when(parameters.getKeytab()).thenReturn("");
-
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-    KerberosPrincipal result =
-        KerberosPrincipalFactory.create(fsOperations,
-            mockClientContext.getRemoteDirectoryManager(), parameters);
-
-    assertNull(result);
-  }
-
-  @Test
-  public void testCreatePrincipalNonEmptyPrincipalAndKeytab()
-      throws IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-
-    RunJobParameters parameters = mock(RunJobParameters.class);
-    when(parameters.getPrincipal()).thenReturn("principal");
-    when(parameters.getKeytab()).thenReturn("keytab");
-
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-    KerberosPrincipal result =
-        KerberosPrincipalFactory.create(fsOperations,
-            mockClientContext.getRemoteDirectoryManager(), parameters);
-
-    assertNotNull(result);
-    assertEquals("file://keytab", result.getKeytab());
-    assertEquals("principal", result.getPrincipalName());
-  }
-
-  @Test
-  public void testCreatePrincipalDistributedKeytab() throws IOException {
-    MockClientContext mockClientContext = new MockClientContext();
-    String jobname = "testJobname";
-    String keytab = "testKeytab";
-    File keytabFile = createKeytabFile(keytab);
-
-    RunJobParameters parameters = mock(RunJobParameters.class);
-    when(parameters.getPrincipal()).thenReturn("principal");
-    when(parameters.getKeytab()).thenReturn(keytabFile.getAbsolutePath());
-    when(parameters.getName()).thenReturn(jobname);
-    when(parameters.isDistributeKeytab()).thenReturn(true);
-
-    FileSystemOperations fsOperations =
-        new FileSystemOperations(mockClientContext);
-
-    KerberosPrincipal result =
-        KerberosPrincipalFactory.create(fsOperations,
-            mockClientContext.getRemoteDirectoryManager(), parameters);
-
-    Path stagingDir = mockClientContext.getRemoteDirectoryManager()
-        .getJobStagingArea(parameters.getName(), true);
-    String expectedKeytabFilePath =
-        FileUtilitiesForTests.getFilename(stagingDir, keytab).getAbsolutePath();
-
-    assertNotNull(result);
-    assertEquals(expectedKeytabFilePath, result.getKeytab());
-    assertEquals("principal", result.getPrincipalName());
-  }
-
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestSubmarineResourceUtils.java b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestSubmarineResourceUtils.java
deleted file mode 100644
index 1c0a73b8dc5b..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestSubmarineResourceUtils.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *     http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.submarine.utils;
-
-import com.google.common.collect.ImmutableMap;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.LocalConfigurationProvider;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.service.api.records.ResourceInformation;
-import org.apache.hadoop.yarn.util.resource.ResourceUtils;
-import org.junit.After;
-import org.junit.Test;
-
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.Map;
-
-import static org.junit.Assert.*;
-
-/**
- * This class is to test {@link SubmarineResourceUtils}.
- */
-public class TestSubmarineResourceUtils {
-  /**
-   * With the dependencies of hadoop 3.2.0, Need to create a
-   * CustomResourceTypesConfigurationProvider implementations. If the
-   * dependencies are upgraded to hadoop 3.3.0. It can be replaced by
-   * org.apache.hadoop.yarn.util.resource.CustomResourceTypesConfigurationProvi-
-   * der
-   */
-  private static class CustomResourceTypesConfigurationProvider
-      extends LocalConfigurationProvider {
-
-    @Override
-    public InputStream getConfigurationInputStream(Configuration bootstrapConf,
-        String name) throws YarnException, IOException {
-      if (YarnConfiguration.RESOURCE_TYPES_CONFIGURATION_FILE.equals(name)) {
-        return new ByteArrayInputStream(
-            ("<configuration>\n" +
-                " <property>\n" +
-                "   <name>yarn.resource-types</name>\n" +
-                "   <value>" + CUSTOM_RESOURCE_NAME + "</value>\n" +
-                " </property>\n" +
-                " <property>\n" +
-                "   <name>yarn.resource-types.a-custom-resource.units</name>\n"
-                +
-                "   <value>G</value>\n" +
-                " </property>\n" +
-                "</configuration>\n").getBytes());
-      } else {
-        return super.getConfigurationInputStream(bootstrapConf, name);
-      }
-    }
-  }
-
-  private static final String CUSTOM_RESOURCE_NAME = "a-custom-resource";
-
-  private void initResourceTypes() {
-    // If the dependencies are upgraded to hadoop 3.3.0. It can be replaced by
-    // org.apache.hadoop.yarn.util.resource.CustomResourceTypesConfigurationPro-
-    // vider
-    Configuration configuration = new Configuration();
-    configuration.set(YarnConfiguration.RM_CONFIGURATION_PROVIDER_CLASS,
-        CustomResourceTypesConfigurationProvider.class.getName());
-    ResourceUtils.resetResourceTypes(configuration);
-  }
-
-  @After
-  public void cleanup() {
-    ResourceUtils.resetResourceTypes(new Configuration());
-  }
-
-  @Test
-  public void testConvertResourceWithCustomResource() {
-    initResourceTypes();
-    Resource res = Resource.newInstance(4096, 12,
-        ImmutableMap.of(CUSTOM_RESOURCE_NAME, 20L));
-
-    org.apache.hadoop.yarn.service.api.records.Resource serviceResource =
-        SubmarineResourceUtils.convertYarnResourceToServiceResource(res);
-
-    assertEquals(12, serviceResource.getCpus().intValue());
-    assertEquals(4096, (int) Integer.valueOf(serviceResource.getMemory()));
-    Map<String, ResourceInformation> additionalResources =
-        serviceResource.getAdditional();
-
-    // Additional resources also includes vcores and memory
-    assertEquals(3, additionalResources.size());
-    ResourceInformation customResourceRI =
-        additionalResources.get(CUSTOM_RESOURCE_NAME);
-    assertEquals("G", customResourceRI.getUnit());
-    assertEquals(20L, (long) customResourceRI.getValue());
-  }
-
-}
\ No newline at end of file
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/resources/core-site.xml b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/resources/core-site.xml
deleted file mode 100644
index 50ec1460bd60..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/resources/core-site.xml
+++ /dev/null
@@ -1,21 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-<!-- Put site-specific property overrides in this file. -->
-
-<configuration>
-
-</configuration>
diff --git a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/resources/hdfs-site.xml b/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/resources/hdfs-site.xml
deleted file mode 100644
index 50ec1460bd60..000000000000
--- a/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/resources/hdfs-site.xml
+++ /dev/null
@@ -1,21 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-<!-- Put site-specific property overrides in this file. -->
-
-<configuration>
-
-</configuration>
diff --git a/hadoop-submarine/pom.xml b/hadoop-submarine/pom.xml
deleted file mode 100644
index c3b13b37b358..000000000000
--- a/hadoop-submarine/pom.xml
+++ /dev/null
@@ -1,158 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
-                      https://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.hadoop</groupId>
-    <artifactId>hadoop-project</artifactId>
-    <version>3.2.0</version>
-    <relativePath/>
-  </parent>
-  <artifactId>hadoop-submarine</artifactId>
-  <version>0.3.0-SNAPSHOT</version>
-  <name>Hadoop Submarine</name>
-  <packaging>pom</packaging>
-
-  <properties>
-    <hadoop.common.build.dir>${basedir}/../hadoop-common-project/hadoop-common/target</hadoop.common.build.dir>
-  </properties>
-
-  <modules>
-    <module>hadoop-submarine-core</module>
-    <module>hadoop-submarine-all</module>
-    <module>hadoop-submarine-dist</module>
-  </modules>
-
-  <dependencyManagement>
-    <dependencies>
-      <dependency>
-        <groupId>org.mockito</groupId>
-        <artifactId>mockito-core</artifactId>
-        <version>2.23.4</version>
-        <scope>test</scope>
-      </dependency>
-
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-yarn-services-api</artifactId>
-        <version>${hadoop.version}</version>
-      </dependency>
-
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-common</artifactId>
-        <version>${hadoop.version}</version>
-      </dependency>
-
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-yarn-api</artifactId>
-        <version>${hadoop.version}</version>
-      </dependency>
-
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-yarn-common</artifactId>
-        <version>${hadoop.version}</version>
-      </dependency>
-
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-yarn-client</artifactId>
-        <version>${hadoop.version}</version>
-      </dependency>
-
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-hdfs</artifactId>
-        <version>${hadoop.version}</version>
-      </dependency>
-
-      <dependency>
-        <groupId>org.apache.commons</groupId>
-        <artifactId>commons-lang3</artifactId>
-        <version>3.7</version>
-      </dependency>
-    </dependencies>
-  </dependencyManagement>
-
-  <profiles>
-    <profile>
-      <id>hadoop-3.2</id>
-      <properties>
-        <hadoop.version>3.2.0</hadoop.version>
-      </properties>
-      <modules>
-        <module>hadoop-submarine-yarnservice-runtime</module>
-        <module>hadoop-submarine-tony-runtime</module>
-      </modules>
-    </profile>
-
-    <!-- Default profile-->
-    <profile>
-      <id>hadoop-3.1</id>
-      <activation>
-        <activeByDefault>true</activeByDefault>
-      </activation>
-      <properties>
-        <hadoop.version>3.1.2</hadoop.version>
-      </properties>
-      <modules>
-        <module>hadoop-submarine-yarnservice-runtime</module>
-        <module>hadoop-submarine-tony-runtime</module>
-      </modules>
-    </profile>
-
-    <profile>
-      <id>hadoop-2.9</id>
-      <properties>
-        <hadoop.version>2.9.2</hadoop.version>
-      </properties>
-      <modules>
-        <module>hadoop-submarine-tony-runtime</module>
-      </modules>
-    </profile>
-
-    <profile>
-      <id>hadoop-2.7</id>
-      <properties>
-        <hadoop.version>2.7.3</hadoop.version>
-      </properties>
-      <modules>
-        <module>hadoop-submarine-tony-runtime</module>
-      </modules>
-    </profile>
-
-    <profile>
-     <id>clover</id>
-     <activation>
-       <activeByDefault>false</activeByDefault>
-       <property>
-         <name>clover</name>
-       </property>
-     </activation>
-     <dependencies>
-       <dependency>
-         <groupId>com.cenqua.clover</groupId>
-         <artifactId>clover</artifactId>
-       </dependency>
-     </dependencies>
-    </profile>
-  </profiles>
-
-</project>
diff --git a/pom.xml b/pom.xml
index 779ca79ae2cc..c2505d7c3c38 100644
--- a/pom.xml
+++ b/pom.xml
@@ -404,7 +404,6 @@ xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x
             <exclude>**/build/**</exclude>
             <exclude>**/patchprocess/**</exclude>
             <exclude>**/*.js</exclude>
-            <exclude>hadoop-submarine/**/</exclude>
             <exclude>licenses/**</exclude>
             <exclude>licenses-binary/**</exclude>
          </excludes>
@@ -584,62 +583,6 @@ xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x
         </plugins>
       </build>
     </profile>
-    <profile>
-      <id>submarine-src</id>
-      <activation>
-        <activeByDefault>false</activeByDefault>
-      </activation>
-      <build>
-        <plugins>
-          <plugin>
-            <groupId>org.apache.maven.plugins</groupId>
-            <artifactId>maven-assembly-plugin</artifactId>
-            <inherited>false</inherited>
-            <executions>
-              <execution>
-                <id>src-dist</id>
-                <phase>package</phase>
-                <goals>
-                  <goal>single</goal>
-                </goals>
-                <configuration>
-                  <appendAssemblyId>false</appendAssemblyId>
-                  <attach>false</attach>
-                  <finalName>hadoop-submarine-${project.version}-src</finalName>
-                  <outputDirectory>hadoop-dist/target</outputDirectory>
-                  <!-- Not using descriptorRef and hadoop-assembly dependency -->
-                  <!-- to avoid making hadoop-main to depend on a module      -->
-                  <descriptors>
-                    <descriptor>hadoop-assemblies/src/main/resources/assemblies/hadoop-src-submarine.xml</descriptor>
-                  </descriptors>
-                </configuration>
-              </execution>
-            </executions>
-          </plugin>
-          <plugin>
-            <groupId>org.apache.maven.plugins</groupId>
-            <artifactId>maven-antrun-plugin</artifactId>
-            <inherited>false</inherited>
-            <executions>
-              <execution>
-                <id>src-dist-msg</id>
-                <phase>package</phase>
-                <goals>
-                  <goal>run</goal>
-                </goals>
-                <configuration>
-                  <target>
-                    <echo/>
-                    <echo>Hadoop source tar with Submarine source is available at: ${basedir}/hadoop-dist/target/hadoop-${project.version}-src-with-submarine.tar.gz</echo>
-                    <echo/>
-                  </target>
-                </configuration>
-              </execution>
-            </executions>
-          </plugin>
-        </plugins>
-      </build>
-    </profile>
 
 
     <profile>
@@ -737,15 +680,6 @@ xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/x
         </plugins>
       </build>
     </profile>
-    <profile>
-      <id>submarine</id>
-      <activation>
-        <activeByDefault>false</activeByDefault>
-      </activation>
-      <modules>
-        <module>hadoop-submarine</module>
-      </modules>
-    </profile>
     <profile>
       <id>aarch64</id>
       <properties>
